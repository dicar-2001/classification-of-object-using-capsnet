{"cells":[{"cell_type":"markdown","metadata":{"id":"sanKC5jwTygd"},"source":["### Importation des données"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLXoKAS_Tygj"},"outputs":[],"source":["import pickle\n","# Later, you can load it from the file without recalculating\n","with open('data.pkl', 'rb') as f:\n","    data = pickle.load(f)"]},{"cell_type":"markdown","metadata":{"id":"XR9P_DFrTygm"},"source":["### Division des données sous forme des batches"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hMjX5eopTygp"},"outputs":[],"source":["import random\n","import tensorflow as tf\n","batch_size = 32\n","batches = []\n","\n","random.shuffle(data)  # to shuffle the list in-place\n","\n","# Assuming train_data is a list of tuples (voxel_grid_tensor, class_label)\n","for i in range(0, len(data), batch_size):\n","    batch = data[i:i + batch_size]\n","\n","    # Separate voxel grids and class labels\n","    voxel_grids = [item[0] for item in batch]\n","    class_labels = [item[1] for item in batch]\n","\n","    # Append the batch as a tuple (list of tensors, list of class labels)\n","    batches.append((tf.stack(voxel_grids, axis=0), class_labels))"]},{"cell_type":"markdown","metadata":{"id":"Vnb2RobkTygs"},"source":["### Couches de convolution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GtLND4nuTygt"},"outputs":[],"source":["import tensorflow as tf\n","\n","class ConvLayer(tf.keras.layers.Layer):\n","\n","    def __init__(self, **kwargs):\n","        super(ConvLayer, self).__init__(**kwargs)\n","        # Batch Normalization after second conv layer\n","        self.batch_norm1 = tf.keras.layers.BatchNormalization()\n","        self.batch_norm2 = tf.keras.layers.BatchNormalization()\n","        self.dropout1 = tf.keras.layers.Dropout(rate=0.1)\n","        self.dropout2 = tf.keras.layers.Dropout(rate=0.1)\n","        self.conv_layer1 = tf.keras.layers.Conv3D(\n","        filters=32,              # Apply 32 filters (feature maps).\n","        kernel_size=(3, 3, 3),   # Use a 3x3x3 convolution kernel.\n","        strides=(1, 1, 1),       # Stride of 1 in each direction (no downsampling).\n","        padding='same',          # Use 'same' padding to preserve input dimensions.\n","        activation='relu',\n","        use_bias=True        # Use ReLU activation function.\n","    )\n","        self.conv_layer2= tf.keras.layers.Conv3D(\n","        filters=64,              # Apply 64 filters (feature maps).\n","        kernel_size=(3, 3, 3),   # Use a 3x3x3 convolution kernel.\n","        strides=(2, 2, 2),       # Stride of 2 in each direction (downsampling by 2x).\n","        padding='same',          # Use 'same' padding to preserve input dimensions.\n","        activation='relu'        # Use ReLU activation function.\n","    ) # Bias is included by default in tf.keras.layers.Conv2D\n","\n","\n","    def call(self, inputs):\n","        x = self.conv_layer1(inputs)\n","        x = self.batch_norm1(x)\n","        x = tf.nn.relu(x)\n","        x = self.dropout1(x)\n","\n","        x = self.conv_layer2(x)\n","        x = self.batch_norm2(x)\n","        x = tf.nn.relu(x)\n","        x = self.dropout2(x)\n","        return tf.nn.relu(x)\n"]},{"cell_type":"markdown","metadata":{"id":"2BI_LWcUTygv"},"source":["### Couche de capsules primaires"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xIUfXuaFTygx"},"outputs":[],"source":["import tensorflow as tf\n","\n","class PrimaryCaps(tf.keras.layers.Layer):\n","    def __init__(self, capsule_dim=16, filters=128, kernel_size=(3, 3, 3), strides=(2, 2, 2), padding='same', **kwargs):\n","        super(PrimaryCaps, self).__init__(**kwargs)\n","        # Define a single Conv3D layer\n","        self.conv_layer = tf.keras.layers.Conv3D(\n","            filters=filters,  # 128 filters\n","            kernel_size=kernel_size,\n","            strides=strides,\n","            padding=padding\n","        )\n","        self.capsule_dim = capsule_dim  # Capsule dimension (16)\n","\n","    def call(self, inputs):\n","        # Apply Conv3D layer\n","        conv_output = self.conv_layer(inputs)\n","\n","        # Reshape to form capsules, similar to the function logic\n","        reshaped_output = tf.keras.layers.Reshape(target_shape=[-1, self.capsule_dim])(conv_output)\n","\n","        # Apply squash function to the reshaped output\n","        capsules = self.squash(reshaped_output)\n","        return capsules\n","\n","    def squash(self, input_tensor):\n","        squared_norm = tf.reduce_sum(tf.square(input_tensor), axis=-1, keepdims=True)\n","        epsilon = 1e-7\n","        output_tensor = squared_norm * input_tensor / ((1.0 + squared_norm) * tf.sqrt(squared_norm + epsilon))\n","        return output_tensor\n"]},{"cell_type":"markdown","metadata":{"id":"YXHO1OfzTygz"},"source":["### Couche de capsules secondaire"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6u3KtMeKTyg1"},"outputs":[],"source":["import tensorflow as tf\n","\n","class DigitCaps(tf.keras.layers.Layer):\n","    def __init__(self, num_capsules=10, num_routes=4096,dim_capsules= 16, in_channels=1, **kwargs):\n","        super(DigitCaps, self).__init__(**kwargs)\n","        self.num_routes = num_routes\n","        self.num_capsules = num_capsules\n","        self.dim_capsules = dim_capsules\n","\n","        # Equivalent to nn.Parameter\n","        self.W = self.add_weight(shape=(num_routes, dim_capsules, num_capsules, dim_capsules),\n","                         initializer='glorot_uniform',\n","                         trainable=True)\n","\n","    def call(self, inputs):\n","        batch_size = tf.shape(inputs)[0]\n","\n","        # Perform matrix multiplication\n","        u_hat = tf.einsum('bij,ijkl->bikl', inputs, self.W)  # Matrix multiplication between primary capsules and weight\n","\n","        # Initialize b_ij with zeros\n","        b = tf.zeros(shape=(batch_size, self.num_routes, self.num_capsules), dtype=tf.float32)  # Shape: (batch_size, num_primary_capsules, num_classes)\n","        num_iterations = 8\n","        for _ in range(num_iterations):\n","            # Compute the coupling coefficients c_ij\n","            temperature = 1.0  # Tune this parameter\n","            c = tf.nn.softmax(b / temperature, axis=2)  # Shape: (batch_size, num_routes, num_capsules)\n","            c = tf.expand_dims(c, axis=-1)  # Shape: (batch_size, num_routes, num_capsules, 1)\n","\n","            # Compute the weighted sum of prediction vectors\n","            s = tf.reduce_sum(c * u_hat, axis=1)\n","            # Apply dropout before squash\n","            s = tf.keras.layers.Dropout(0.2)(s) # Shape: (batch_size, num_routes, capsule_dim)\n","            v = self.squash(s)\n","            v = tf.keras.layers.LayerNormalization()(v)\n","            v_expanded = tf.expand_dims(v, axis=1)  # Shape: (batch_size, 1, num_routes, capsule_dim)\n","\n","            b += tf.reduce_sum(u_hat * v_expanded, axis=-1)  # Update b_ij with agreement\n","\n","        return v  # Shape: (batch_size, num_routes, capsule_dim)\n","\n","    def squash(self, input_tensor):\n","        squared_norm = tf.reduce_sum(tf.square(input_tensor), axis=-1, keepdims=True)\n","        output_tensor = squared_norm * input_tensor / ((1.0 + squared_norm) * tf.sqrt(squared_norm + 1e-9))  # Epsilon to prevent division by zero\n","        return output_tensor"]},{"cell_type":"markdown","metadata":{"id":"gBEqxSI9Tyg2"},"source":["### Fonctions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yb9VQxXpTyg3"},"outputs":[],"source":["def convert_labels_to_tensor(y_true, num_classes=10, capsule_dim=16):\n","        \"\"\" Convert List of class codes into a tensor \"\"\"\n","        # Convert y_true to one-hot encoding\n","        y_true_one_hot = tf.one_hot(y_true, depth=num_classes)\n","\n","        # Create a default class vector tensor\n","        # Each class will be represented by a vector where the class index is a one-hot vector in a matrix of shape (num_classes, capsule_dim)\n","        class_vectors = tf.eye(num_classes, capsule_dim)\n","\n","        # Expand class_vectors to match the shape (batch_size, num_classes, capsule_dim)\n","        class_vectors_expanded = tf.expand_dims(class_vectors, axis=0)  # Shape: (1, num_classes, capsule_dim)\n","\n","        # Repeat class_vectors_expanded to match the batch size\n","        class_vectors_repeated = tf.tile(class_vectors_expanded, [tf.shape(y_true)[0], 1, 1])  # Shape: (batch_size, num_classes, capsule_dim)\n","\n","        # Compute the tensor where each class code is represented by its corresponding vector\n","        y_true_tensor = y_true_one_hot[:, :, tf.newaxis] * class_vectors_repeated  # Shape: (batch_size, num_classes, capsule_dim)\n","\n","        return y_true_tensor\n","\n","def get_class_codes_from_tensors(tensor_batch):\n","\n","    # Step 1: Compute the norm (length) of each capsule vector across the capsule dimension (axis=-1)\n","    capsule_norms = tf.norm(tensor_batch, axis=-1)  # Shape: (32, 10)\n","\n","    # Step 2: Find the class with the highest norm for each sample in the batch\n","    predicted_classes = tf.argmax(capsule_norms, axis=1)  # Shape: (batch_size,)\n","\n","    return predicted_classes.numpy().tolist()"]},{"cell_type":"markdown","metadata":{"id":"FcmMZ5jSTyg5"},"source":["### modèle de capsules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hqe0-woXTyg5"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","Losses = []\n","\n","class CapsNet(tf.keras.Model):\n","    def __init__(self, **kwargs):\n","        super(CapsNet, self).__init__(**kwargs)\n","        self.conv_layer = ConvLayer()\n","        self.primary_capsules = PrimaryCaps()\n","        self.digit_capsules = DigitCaps()\n","\n","\n","\n","        self.optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, clipnorm=1.0)\n","\n","\n","    def get_config(self):\n","        # Return a dictionary of the parameters to reconstruct the layer\n","        config = super(CapsNet, self).get_config()\n","        return config\n","\n","    @classmethod\n","    def from_config(cls, config):\n","        return cls(**config)  # Pass parameters to the constructor if needed\n","\n","    def call(self, data):\n","        conv_output = self.conv_layer(data)\n","        primary_caps_output = self.primary_capsules(conv_output)\n","        digit_caps_output = self.digit_capsules(primary_caps_output)\n","        return digit_caps_output\n","\n","    def margin_loss(self, x, labels):\n","        \"\"\"\n","    Computes the margin loss for Capsule Networks.\n","\n","    The margin loss encourages the network to output a vector norm close to 1 for the correct class and close to 0\n","    for the incorrect classes. This helps improve the network's classification accuracy and robustness.\n","\n","    The loss is computed as:\n","    - For correct classes, the loss increases if the norm of the output capsule vector is less than 0.9.\n","    - For incorrect classes, the loss increases if the norm of the output capsule vector is greater than 0.1.\n","\n","    Args:\n","        x: Tensor of shape (batch_size, num_classes, capsule_dim). The output from the digit capsules.\n","        labels: Tensor of shape (batch_size, num_classes). The one-hot encoded true labels.\n","\n","    Returns:\n","        A scalar tensor representing the margin loss, averaged across the batch.\n","    \"\"\"\n","        batch_size = tf.shape(x)[0]\n","        v_c = tf.sqrt(tf.reduce_sum(tf.square(x), axis=2, keepdims=True))\n","\n","        left = tf.nn.relu(0.9 - v_c)\n","        right = tf.nn.relu(v_c - 0.1)\n","\n","\n","        left = tf.reshape(left, [batch_size, -1])\n","        right = tf.reshape(right, [batch_size, -1])\n","\n","        labels_reduced = tf.reduce_mean(labels, axis=-1)  # Shape: (batch_size, num_classes)\n","\n","        # Compute margin loss\n","        loss = labels_reduced * left + 0.5 * (1.0 - labels_reduced) * right\n","\n","        loss = tf.reduce_mean(tf.reduce_sum(loss, axis=1))\n","\n","        return loss\n","\n","    def train_step(self, x_batch_train, y_batch_train):\n","        \"\"\"Perform a single training step.\"\"\"\n","        with tf.GradientTape() as tape:\n","            # Forward pass\n","            output = self.call(x_batch_train)\n","\n","            # Compute the loss\n","            loss = self.margin_loss(output, convert_labels_to_tensor(y_batch_train))\n","        # Backpropagation\n","        gradients = tape.gradient(loss, self.trainable_variables)\n","        if gradients:  # Ensure gradients are not None\n","            self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n","\n","            # Get the current accuracy result\n","        accuracy = self.accuracy(y_batch_train, get_class_codes_from_tensors(output))\n","\n","\n","        return f\"{loss.numpy()},{accuracy * 100}%\"\n","\n","    def train(self, dataset, epochs):\n","        \"\"\"Train the model for a number of epochs.\"\"\"\n","\n","        for epoch in range(epochs):\n","            print(f\"Epoch {epoch + 1}/{epochs}\")\n","\n","            # Iterate over batches of the dataset\n","            for step, (x_batch_train, y_batch_train) in enumerate(dataset): #217 steps\n","                print(f\"Step {step + 1}\")\n","                loss_Acc = self.train_step(x_batch_train, y_batch_train)\n","                Losses.append(loss_Acc)\n","                print(f\"Loss & Accuracy: {loss_Acc}\")\n","\n","\n","        print(\"Training completed.\")\n","\n","    def predict(self, objects_tensor) : # object tensor of shape (30,30,30,1)\n","        \"\"\"\n","        Predicts the class for the given input tensor.\n","\n","        Args:\n","            object_tensor (tf.Tensor): Input tensor of shape (batch_size, 30, 30, 30, 1).\n","\n","        Returns:\n","            np.ndarray: Predicted class probabilities or class labels.\n","        \"\"\"\n","        # Ensure the input is a tensor\n","        objects_tensor = tf.convert_to_tensor(objects_tensor, dtype=tf.float32)\n","\n","        #(32, 30, 30, 30, 1)\n","        # Perform a forward pass\n","        predictions = self.call(objects_tensor)\n","\n","        # Process the output to get class probabilities or labels\n","        # Assuming the output of digit capsules is (batch_size, num_classes, capsule_dim)\n","\n","        norm = tf.norm(predictions, axis=-1)  # Shape: (batch_size, num_classes)\n","\n","        # Convert to probabilities if needed (e.g., using softmax)\n","        probabilities = tf.nn.softmax(norm, axis=-1).numpy()\n","\n","\n","        # Optionally, get the class with the highest probability\n","        predicted_classes = np.argmax(probabilities, axis=-1)\n","\n","        return predicted_classes\n","\n","\n","    def accuracy(self, y_true, y_pred):\n","        # Ensure y_true and y_pred are lists of the same length\n","        assert len(y_true) == len(y_pred), \"The lengths of y_true and y_pred must match.\"\n","\n","    # Calculate the number of correct predictions\n","        correct_predictions = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)\n","\n","    # Calculate accuracy\n","        accuracy_value = correct_predictions / len(y_true)  # Divide by the total number of predictions\n","\n","        return accuracy_value\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dTXNP_S_Tyg8"},"outputs":[],"source":["capsule_net = CapsNet()\n","capsule_net.compile(optimizer= capsule_net.optimizer, loss=capsule_net.margin_loss, metrics=[capsule_net.accuracy])"]},{"cell_type":"markdown","metadata":{"id":"65CxbvUFTyg9"},"source":["### Entrainement du modèle :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZMMRs3hhTyg9","outputId":"882722fa-fc00-4b30-ec43-9f05c8a0bf92"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","Step 1\n","Loss & Accuracy: 0.054491881281137466,6.25%\n","Step 2\n","Loss & Accuracy: 0.05273398756980896,12.5%\n","Step 3\n","Loss & Accuracy: 0.05273396149277687,9.375%\n","Step 4\n","Loss & Accuracy: 0.05097605288028717,18.75%\n","Step 5\n","Loss & Accuracy: 0.05449172854423523,12.5%\n","Step 6\n","Loss & Accuracy: 0.04570271819829941,12.5%\n","Step 7\n","Loss & Accuracy: 0.04921828582882881,21.875%\n","Step 8\n","Loss & Accuracy: 0.0509759895503521,9.375%\n","Step 9\n","Loss & Accuracy: 0.04570259526371956,12.5%\n","Step 10\n","Loss & Accuracy: 0.04921809583902359,28.125%\n","Step 11\n","Loss & Accuracy: 0.04570233076810837,37.5%\n","Step 12\n","Loss & Accuracy: 0.04921797662973404,21.875%\n","Step 13\n","Loss & Accuracy: 0.047460153698921204,15.625%\n","Step 14\n","Loss & Accuracy: 0.05273344740271568,9.375%\n","Step 15\n","Loss & Accuracy: 0.049217339605093,18.75%\n","Step 16\n","Loss & Accuracy: 0.04745975881814957,21.875%\n","Step 17\n","Loss & Accuracy: 0.04921753704547882,15.625%\n","Step 18\n","Loss & Accuracy: 0.05448974668979645,18.75%\n","Step 19\n","Loss & Accuracy: 0.05273151397705078,18.75%\n","Step 20\n","Loss & Accuracy: 0.05448850244283676,12.5%\n","Step 21\n","Loss & Accuracy: 0.052730828523635864,12.5%\n","Step 22\n","Loss & Accuracy: 0.05448782444000244,6.25%\n","Step 23\n","Loss & Accuracy: 0.05096800625324249,18.75%\n","Step 24\n","Loss & Accuracy: 0.05096367746591568,15.625%\n","Step 25\n","Loss & Accuracy: 0.04744863510131836,12.5%\n","Step 26\n","Loss & Accuracy: 0.04743935540318489,15.625%\n","Step 27\n","Loss & Accuracy: 0.05096079409122467,6.25%\n","Step 28\n","Loss & Accuracy: 0.0526876300573349,18.75%\n","Step 29\n","Loss & Accuracy: 0.05090893805027008,15.625%\n","Step 30\n","Loss & Accuracy: 0.05093161016702652,3.125%\n","Step 31\n","Loss & Accuracy: 0.0437169186770916,15.625%\n","Step 32\n","Loss & Accuracy: 0.052671559154987335,3.125%\n","Step 33\n","Loss & Accuracy: 0.04710090160369873,15.625%\n","Step 34\n","Loss & Accuracy: 0.05064629390835762,12.5%\n","Step 35\n","Loss & Accuracy: 0.04726412519812584,3.125%\n","Step 36\n","Loss & Accuracy: 0.055647458881139755,18.75%\n","Step 37\n","Loss & Accuracy: 0.04555400833487511,0.0%\n","Step 38\n","Loss & Accuracy: 0.0506114661693573,15.625%\n","Step 39\n","Loss & Accuracy: 0.04867928475141525,15.625%\n","Step 40\n","Loss & Accuracy: 0.05208578705787659,18.75%\n","Step 41\n","Loss & Accuracy: 0.04821452498435974,34.375%\n","Step 42\n","Loss & Accuracy: 0.0495525598526001,28.125%\n","Step 43\n","Loss & Accuracy: 0.05461806431412697,18.75%\n","Step 44\n","Loss & Accuracy: 0.05200021713972092,31.25%\n","Step 45\n","Loss & Accuracy: 0.051853448152542114,25.0%\n","Step 46\n","Loss & Accuracy: 0.04843632131814957,28.125%\n","Step 47\n","Loss & Accuracy: 0.053425587713718414,37.5%\n","Step 48\n","Loss & Accuracy: 0.04488712549209595,28.125%\n","Step 49\n","Loss & Accuracy: 0.05166192352771759,40.625%\n","Step 50\n","Loss & Accuracy: 0.05322094261646271,53.125%\n","Step 51\n","Loss & Accuracy: 0.04818735644221306,43.75%\n","Step 52\n","Loss & Accuracy: 0.0460515134036541,46.875%\n","Step 53\n","Loss & Accuracy: 0.04627370089292526,50.0%\n","Step 54\n","Loss & Accuracy: 0.048517271876335144,37.5%\n","Step 55\n","Loss & Accuracy: 0.05141977220773697,50.0%\n","Step 56\n","Loss & Accuracy: 0.05138634890317917,50.0%\n","Step 57\n","Loss & Accuracy: 0.048160847276449203,37.5%\n","Step 58\n","Loss & Accuracy: 0.0530405193567276,59.375%\n","Step 59\n","Loss & Accuracy: 0.04313044995069504,40.625%\n","Step 60\n","Loss & Accuracy: 0.04614691436290741,43.75%\n","Step 61\n","Loss & Accuracy: 0.048586323857307434,40.625%\n","Step 62\n","Loss & Accuracy: 0.04762206971645355,40.625%\n","Step 63\n","Loss & Accuracy: 0.05148223042488098,50.0%\n","Step 64\n","Loss & Accuracy: 0.049600716680288315,56.25%\n","Step 65\n","Loss & Accuracy: 0.04750635102391243,62.5%\n","Step 66\n","Loss & Accuracy: 0.04742191359400749,53.125%\n","Step 67\n","Loss & Accuracy: 0.0479726642370224,40.625%\n","Step 68\n","Loss & Accuracy: 0.05819712579250336,43.75%\n","Step 69\n","Loss & Accuracy: 0.04921836033463478,50.0%\n","Step 70\n","Loss & Accuracy: 0.053245097398757935,43.75%\n","Step 71\n","Loss & Accuracy: 0.05159305781126022,43.75%\n","Step 72\n","Loss & Accuracy: 0.05004333704710007,37.5%\n","Step 73\n","Loss & Accuracy: 0.051826782524585724,43.75%\n","Step 74\n","Loss & Accuracy: 0.04990343004465103,53.125%\n","Step 75\n","Loss & Accuracy: 0.04993102699518204,46.875%\n","Step 76\n","Loss & Accuracy: 0.044723428785800934,43.75%\n","Step 77\n","Loss & Accuracy: 0.051726289093494415,43.75%\n","Step 78\n","Loss & Accuracy: 0.04994764178991318,37.5%\n","Step 79\n","Loss & Accuracy: 0.0482962504029274,46.875%\n","Step 80\n","Loss & Accuracy: 0.05146773159503937,56.25%\n","Step 81\n","Loss & Accuracy: 0.04653457924723625,34.375%\n","Step 82\n","Loss & Accuracy: 0.053682051599025726,40.625%\n","Step 83\n","Loss & Accuracy: 0.05345431715250015,40.625%\n","Step 84\n","Loss & Accuracy: 0.05113972723484039,53.125%\n","Step 85\n","Loss & Accuracy: 0.04977238550782204,40.625%\n","Step 86\n","Loss & Accuracy: 0.04947047308087349,56.25%\n","Step 87\n","Loss & Accuracy: 0.04953553527593613,46.875%\n","Step 88\n","Loss & Accuracy: 0.04918428882956505,53.125%\n","Step 89\n","Loss & Accuracy: 0.051125235855579376,50.0%\n","Step 90\n","Loss & Accuracy: 0.045532770454883575,46.875%\n","Step 91\n","Loss & Accuracy: 0.05424942076206207,62.5%\n","Step 92\n","Loss & Accuracy: 0.05081423372030258,34.375%\n","Step 93\n","Loss & Accuracy: 0.05232388898730278,43.75%\n","Step 94\n","Loss & Accuracy: 0.052629973739385605,40.625%\n","Step 95\n","Loss & Accuracy: 0.05075297877192497,56.25%\n","Step 96\n","Loss & Accuracy: 0.05093470960855484,43.75%\n","Step 97\n","Loss & Accuracy: 0.05062080919742584,56.25%\n","Step 98\n","Loss & Accuracy: 0.0457780547440052,68.75%\n","Step 99\n","Loss & Accuracy: 0.04937605559825897,34.375%\n","Step 100\n","Loss & Accuracy: 0.05228157341480255,65.625%\n","Step 101\n","Loss & Accuracy: 0.050504930317401886,53.125%\n","Step 102\n","Loss & Accuracy: 0.044115547090768814,53.125%\n","Step 103\n","Loss & Accuracy: 0.05221526697278023,62.5%\n","Step 104\n","Loss & Accuracy: 0.049044374376535416,56.25%\n","Step 105\n","Loss & Accuracy: 0.045767441391944885,56.25%\n","Step 106\n","Loss & Accuracy: 0.04568929225206375,59.375%\n","Step 107\n","Loss & Accuracy: 0.04922528937458992,56.25%\n","Step 108\n","Loss & Accuracy: 0.04691546410322189,68.75%\n","Step 109\n","Loss & Accuracy: 0.04565173387527466,62.5%\n","Step 110\n","Loss & Accuracy: 0.04740177094936371,68.75%\n","Step 111\n","Loss & Accuracy: 0.05377422273159027,71.875%\n","Step 112\n","Loss & Accuracy: 0.04577915370464325,68.75%\n","Step 113\n","Loss & Accuracy: 0.05013478174805641,68.75%\n","Step 114\n","Loss & Accuracy: 0.04885132238268852,65.625%\n","Step 115\n","Loss & Accuracy: 0.05129373073577881,78.125%\n","Step 116\n","Loss & Accuracy: 0.049367647618055344,68.75%\n","Step 117\n","Loss & Accuracy: 0.04510241374373436,56.25%\n","Step 118\n","Loss & Accuracy: 0.05408892780542374,75.0%\n","Step 119\n","Loss & Accuracy: 0.045795392245054245,56.25%\n","Step 120\n","Loss & Accuracy: 0.04699935391545296,65.625%\n","Step 121\n","Loss & Accuracy: 0.04889445751905441,62.5%\n","Step 122\n","Loss & Accuracy: 0.047437869012355804,59.375%\n","Step 123\n","Loss & Accuracy: 0.04916727915406227,59.375%\n","Step 124\n","Loss & Accuracy: 0.045485369861125946,43.75%\n","Step 125\n","Loss & Accuracy: 0.04562678933143616,50.0%\n","Step 126\n","Loss & Accuracy: 0.050710201263427734,65.625%\n","Step 127\n","Loss & Accuracy: 0.047255195677280426,59.375%\n","Step 128\n","Loss & Accuracy: 0.05241715908050537,65.625%\n","Step 129\n","Loss & Accuracy: 0.04867873713374138,65.625%\n","Step 130\n","Loss & Accuracy: 0.0492686927318573,59.375%\n","Step 131\n","Loss & Accuracy: 0.04639163613319397,65.625%\n","Step 132\n","Loss & Accuracy: 0.05000792443752289,68.75%\n","Step 133\n","Loss & Accuracy: 0.05055996775627136,50.0%\n","Step 134\n","Loss & Accuracy: 0.04468388110399246,40.625%\n","Step 135\n","Loss & Accuracy: 0.041792914271354675,56.25%\n","Step 136\n","Loss & Accuracy: 0.050026778131723404,62.5%\n","Step 137\n","Loss & Accuracy: 0.0488426573574543,59.375%\n","Step 138\n","Loss & Accuracy: 0.04286058992147446,50.0%\n","Step 139\n","Loss & Accuracy: 0.05051469802856445,50.0%\n","Step 140\n","Loss & Accuracy: 0.04739408195018768,62.5%\n","Step 141\n","Loss & Accuracy: 0.050760604441165924,56.25%\n","Step 142\n","Loss & Accuracy: 0.050450559705495834,71.875%\n","Step 143\n","Loss & Accuracy: 0.050850413739681244,68.75%\n","Step 144\n","Loss & Accuracy: 0.05244502052664757,62.5%\n","Step 145\n","Loss & Accuracy: 0.05243784189224243,59.375%\n","Step 146\n","Loss & Accuracy: 0.050639279186725616,56.25%\n","Step 147\n","Loss & Accuracy: 0.046958230435848236,56.25%\n","Step 148\n","Loss & Accuracy: 0.0515088215470314,53.125%\n","Step 149\n","Loss & Accuracy: 0.04550270736217499,71.875%\n","Step 150\n","Loss & Accuracy: 0.047383375465869904,46.875%\n","Step 151\n","Loss & Accuracy: 0.045706577599048615,53.125%\n","Step 152\n","Loss & Accuracy: 0.049208175390958786,71.875%\n","Step 153\n","Loss & Accuracy: 0.04715369641780853,65.625%\n","Step 154\n","Loss & Accuracy: 0.05200088769197464,71.875%\n","Step 155\n","Loss & Accuracy: 0.052219875156879425,65.625%\n","Step 156\n","Loss & Accuracy: 0.04902011901140213,62.5%\n","Step 157\n","Loss & Accuracy: 0.05264369398355484,71.875%\n","Step 158\n","Loss & Accuracy: 0.04731691628694534,62.5%\n","Step 159\n","Loss & Accuracy: 0.045857127755880356,65.625%\n","Step 160\n","Loss & Accuracy: 0.0440925732254982,56.25%\n","Step 161\n","Loss & Accuracy: 0.0523269921541214,65.625%\n","Step 162\n","Loss & Accuracy: 0.04697251319885254,65.625%\n","Step 163\n","Loss & Accuracy: 0.04767988622188568,50.0%\n","Step 164\n","Loss & Accuracy: 0.04854421317577362,71.875%\n","Step 165\n","Loss & Accuracy: 0.048448942601680756,65.625%\n","Step 166\n","Loss & Accuracy: 0.04862750694155693,71.875%\n","Step 167\n","Loss & Accuracy: 0.04551146179437637,53.125%\n","Step 168\n","Loss & Accuracy: 0.052074551582336426,75.0%\n","Step 169\n","Loss & Accuracy: 0.04791499674320221,71.875%\n","Step 170\n","Loss & Accuracy: 0.046559013426303864,62.5%\n","Step 171\n","Loss & Accuracy: 0.049372874200344086,62.5%\n","Step 172\n","Loss & Accuracy: 0.043193742632865906,59.375%\n","Step 173\n","Loss & Accuracy: 0.04635731503367424,68.75%\n","Step 174\n","Loss & Accuracy: 0.050981760025024414,71.875%\n","Step 175\n","Loss & Accuracy: 0.04878734424710274,53.125%\n","Step 176\n","Loss & Accuracy: 0.04547581821680069,59.375%\n","Step 177\n","Loss & Accuracy: 0.04553849250078201,68.75%\n","Step 178\n","Loss & Accuracy: 0.04875972121953964,81.25%\n","Step 179\n","Loss & Accuracy: 0.0469093844294548,78.125%\n","Step 180\n","Loss & Accuracy: 0.05076884478330612,62.5%\n","Step 181\n","Loss & Accuracy: 0.04455052688717842,62.5%\n","Step 182\n","Loss & Accuracy: 0.04878927394747734,53.125%\n","Step 183\n","Loss & Accuracy: 0.045253075659275055,59.375%\n","Step 184\n","Loss & Accuracy: 0.05018872767686844,53.125%\n","Step 185\n","Loss & Accuracy: 0.04642448201775551,62.5%\n","Step 186\n","Loss & Accuracy: 0.049846306443214417,65.625%\n","Step 187\n","Loss & Accuracy: 0.046852193772792816,56.25%\n","Step 188\n","Loss & Accuracy: 0.050657812505960464,75.0%\n","Step 189\n","Loss & Accuracy: 0.04983613267540932,75.0%\n","Step 190\n","Loss & Accuracy: 0.047221146523952484,50.0%\n","Step 191\n","Loss & Accuracy: 0.047304488718509674,53.125%\n","Step 192\n","Loss & Accuracy: 0.050618238747119904,59.375%\n","Step 193\n","Loss & Accuracy: 0.043770939111709595,56.25%\n","Step 194\n","Loss & Accuracy: 0.04708479717373848,75.0%\n","Step 195\n","Loss & Accuracy: 0.05361860990524292,71.875%\n","Step 196\n","Loss & Accuracy: 0.05050947517156601,68.75%\n","Step 197\n","Loss & Accuracy: 0.048778168857097626,75.0%\n","Step 198\n","Loss & Accuracy: 0.0438794270157814,65.625%\n","Step 199\n","Loss & Accuracy: 0.04707885533571243,71.875%\n","Step 200\n","Loss & Accuracy: 0.04186525195837021,56.25%\n","Step 201\n","Loss & Accuracy: 0.04854397475719452,81.25%\n","Step 202\n","Loss & Accuracy: 0.04506819695234299,68.75%\n","Step 203\n","Loss & Accuracy: 0.05084381252527237,65.625%\n","Step 204\n","Loss & Accuracy: 0.04521474242210388,68.75%\n","Step 205\n","Loss & Accuracy: 0.04889409989118576,71.875%\n","Step 206\n","Loss & Accuracy: 0.04382675141096115,43.75%\n","Step 207\n","Loss & Accuracy: 0.04993540793657303,71.875%\n","Step 208\n","Loss & Accuracy: 0.04492245614528656,46.875%\n","Step 209\n","Loss & Accuracy: 0.050347939133644104,56.25%\n","Step 210\n","Loss & Accuracy: 0.04751923307776451,50.0%\n","Step 211\n","Loss & Accuracy: 0.04953189194202423,75.0%\n","Step 212\n","Loss & Accuracy: 0.0513199046254158,75.0%\n","Step 213\n","Loss & Accuracy: 0.04147502779960632,59.375%\n","Step 214\n","Loss & Accuracy: 0.05004192516207695,59.375%\n","Step 215\n","Loss & Accuracy: 0.04976498335599899,75.0%\n","Step 216\n","Loss & Accuracy: 0.050973717123270035,71.875%\n","Step 217\n","Loss & Accuracy: 0.04575205594301224,28.57142857142857%\n","Epoch 2/10\n","Step 1\n","Loss & Accuracy: 0.052288707345724106,71.875%\n","Step 2\n","Loss & Accuracy: 0.05363483354449272,62.5%\n","Step 3\n","Loss & Accuracy: 0.05015052109956741,71.875%\n","Step 4\n","Loss & Accuracy: 0.048718854784965515,62.5%\n","Step 5\n","Loss & Accuracy: 0.05224672704935074,71.875%\n","Step 6\n","Loss & Accuracy: 0.04358324408531189,62.5%\n","Step 7\n","Loss & Accuracy: 0.04719574749469757,65.625%\n","Step 8\n","Loss & Accuracy: 0.0485321506857872,81.25%\n","Step 9\n","Loss & Accuracy: 0.043695807456970215,62.5%\n","Step 10\n","Loss & Accuracy: 0.04681944102048874,59.375%\n","Step 11\n","Loss & Accuracy: 0.043521709740161896,68.75%\n","Step 12\n","Loss & Accuracy: 0.04673243314027786,62.5%\n","Step 13\n","Loss & Accuracy: 0.045501258224248886,53.125%\n","Step 14\n","Loss & Accuracy: 0.0503121092915535,68.75%\n","Step 15\n","Loss & Accuracy: 0.04714572802186012,71.875%\n","Step 16\n","Loss & Accuracy: 0.044899336993694305,65.625%\n","Step 17\n","Loss & Accuracy: 0.04716441407799721,68.75%\n","Step 18\n","Loss & Accuracy: 0.05156707391142845,81.25%\n","Step 19\n","Loss & Accuracy: 0.04999271780252457,71.875%\n","Step 20\n","Loss & Accuracy: 0.051614679396152496,81.25%\n","Step 21\n","Loss & Accuracy: 0.04970855638384819,71.875%\n","Step 22\n","Loss & Accuracy: 0.051828619092702866,65.625%\n","Step 23\n","Loss & Accuracy: 0.04822403937578201,71.875%\n","Step 24\n","Loss & Accuracy: 0.047798193991184235,75.0%\n","Step 25\n","Loss & Accuracy: 0.044846415519714355,65.625%\n","Step 26\n","Loss & Accuracy: 0.04463427513837814,71.875%\n","Step 27\n","Loss & Accuracy: 0.04798981174826622,75.0%\n","Step 28\n","Loss & Accuracy: 0.04986090585589409,59.375%\n","Step 29\n","Loss & Accuracy: 0.04780550301074982,68.75%\n","Step 30\n","Loss & Accuracy: 0.04916207492351532,65.625%\n","Step 31\n","Loss & Accuracy: 0.04126933962106705,59.375%\n","Step 32\n","Loss & Accuracy: 0.05041144788265228,78.125%\n","Step 33\n","Loss & Accuracy: 0.044982925057411194,56.25%\n","Step 34\n","Loss & Accuracy: 0.049035511910915375,59.375%\n","Step 35\n","Loss & Accuracy: 0.04507738724350929,68.75%\n","Step 36\n","Loss & Accuracy: 0.05360109359025955,93.75%\n","Step 37\n","Loss & Accuracy: 0.043823543936014175,56.25%\n","Step 38\n","Loss & Accuracy: 0.04874502122402191,68.75%\n","Step 39\n","Loss & Accuracy: 0.04665558040142059,65.625%\n","Step 40\n","Loss & Accuracy: 0.050424039363861084,71.875%\n","Step 41\n","Loss & Accuracy: 0.046130403876304626,78.125%\n","Step 42\n","Loss & Accuracy: 0.04792625457048416,78.125%\n","Step 43\n","Loss & Accuracy: 0.049474798142910004,71.875%\n","Step 44\n","Loss & Accuracy: 0.04980091750621796,75.0%\n","Step 45\n","Loss & Accuracy: 0.050653569400310516,65.625%\n","Step 46\n","Loss & Accuracy: 0.04667878523468971,81.25%\n","Step 47\n","Loss & Accuracy: 0.052199091762304306,75.0%\n","Step 48\n","Loss & Accuracy: 0.04328899830579758,56.25%\n","Step 49\n","Loss & Accuracy: 0.0498117133975029,81.25%\n","Step 50\n","Loss & Accuracy: 0.052147362381219864,62.5%\n","Step 51\n","Loss & Accuracy: 0.04685549437999725,68.75%\n","Step 52\n","Loss & Accuracy: 0.04488105699419975,71.875%\n","Step 53\n","Loss & Accuracy: 0.04510647803544998,65.625%\n","Step 54\n","Loss & Accuracy: 0.047021325677633286,53.125%\n","Step 55\n","Loss & Accuracy: 0.050111882388591766,65.625%\n","Step 56\n","Loss & Accuracy: 0.04976804554462433,75.0%\n","Step 57\n","Loss & Accuracy: 0.04622606933116913,65.625%\n","Step 58\n","Loss & Accuracy: 0.051357004791498184,75.0%\n","Step 59\n","Loss & Accuracy: 0.04114746302366257,62.5%\n","Step 60\n","Loss & Accuracy: 0.045541271567344666,53.125%\n","Step 61\n","Loss & Accuracy: 0.04817122966051102,62.5%\n","Step 62\n","Loss & Accuracy: 0.04553266614675522,75.0%\n","Step 63\n","Loss & Accuracy: 0.0497119314968586,65.625%\n","Step 64\n","Loss & Accuracy: 0.04818592220544815,71.875%\n","Step 65\n","Loss & Accuracy: 0.04598039761185646,78.125%\n","Step 66\n","Loss & Accuracy: 0.04621727019548416,62.5%\n","Step 67\n","Loss & Accuracy: 0.04640273004770279,62.5%\n","Step 68\n","Loss & Accuracy: 0.053149543702602386,62.5%\n","Step 69\n","Loss & Accuracy: 0.047981783747673035,71.875%\n","Step 70\n","Loss & Accuracy: 0.051374319940805435,62.5%\n","Step 71\n","Loss & Accuracy: 0.05073361098766327,71.875%\n","Step 72\n","Loss & Accuracy: 0.048039574176073074,65.625%\n","Step 73\n","Loss & Accuracy: 0.05100126191973686,65.625%\n","Step 74\n","Loss & Accuracy: 0.04807310551404953,68.75%\n","Step 75\n","Loss & Accuracy: 0.04802373796701431,68.75%\n","Step 76\n","Loss & Accuracy: 0.04276479408144951,68.75%\n","Step 77\n","Loss & Accuracy: 0.05017104372382164,62.5%\n","Step 78\n","Loss & Accuracy: 0.04831496626138687,78.125%\n","Step 79\n","Loss & Accuracy: 0.04636373370885849,68.75%\n","Step 80\n","Loss & Accuracy: 0.04988802596926689,71.875%\n","Step 81\n","Loss & Accuracy: 0.04447842389345169,71.875%\n","Step 82\n","Loss & Accuracy: 0.051644664257764816,68.75%\n","Step 83\n","Loss & Accuracy: 0.05155082046985626,71.875%\n","Step 84\n","Loss & Accuracy: 0.05001819133758545,56.25%\n","Step 85\n","Loss & Accuracy: 0.04851662367582321,59.375%\n","Step 86\n","Loss & Accuracy: 0.04735475778579712,62.5%\n","Step 87\n","Loss & Accuracy: 0.04813067615032196,65.625%\n","Step 88\n","Loss & Accuracy: 0.04768161475658417,71.875%\n","Step 89\n","Loss & Accuracy: 0.04978077858686447,75.0%\n","Step 90\n","Loss & Accuracy: 0.04468773677945137,65.625%\n","Step 91\n","Loss & Accuracy: 0.052852485328912735,87.5%\n","Step 92\n","Loss & Accuracy: 0.04973972216248512,65.625%\n","Step 93\n","Loss & Accuracy: 0.051145654171705246,78.125%\n","Step 94\n","Loss & Accuracy: 0.051025450229644775,68.75%\n","Step 95\n","Loss & Accuracy: 0.048888303339481354,84.375%\n","Step 96\n","Loss & Accuracy: 0.05054660886526108,65.625%\n","Step 97\n","Loss & Accuracy: 0.0495147742331028,71.875%\n","Step 98\n","Loss & Accuracy: 0.04426424577832222,75.0%\n","Step 99\n","Loss & Accuracy: 0.04853764921426773,59.375%\n","Step 100\n","Loss & Accuracy: 0.051361456513404846,75.0%\n","Step 101\n","Loss & Accuracy: 0.050207458436489105,68.75%\n","Step 102\n","Loss & Accuracy: 0.04315171390771866,65.625%\n","Step 103\n","Loss & Accuracy: 0.05176495388150215,68.75%\n","Step 104\n","Loss & Accuracy: 0.04814370349049568,68.75%\n","Step 105\n","Loss & Accuracy: 0.045018091797828674,62.5%\n","Step 106\n","Loss & Accuracy: 0.04498932138085365,62.5%\n","Step 107\n","Loss & Accuracy: 0.04794692248106003,68.75%\n","Step 108\n","Loss & Accuracy: 0.04671814292669296,68.75%\n","Step 109\n","Loss & Accuracy: 0.04503248631954193,62.5%\n","Step 110\n","Loss & Accuracy: 0.046524688601493835,81.25%\n","Step 111\n","Loss & Accuracy: 0.05248323082923889,84.375%\n","Step 112\n","Loss & Accuracy: 0.04422592744231224,71.875%\n","Step 113\n","Loss & Accuracy: 0.04926571249961853,78.125%\n","Step 114\n","Loss & Accuracy: 0.04785119369626045,81.25%\n","Step 115\n","Loss & Accuracy: 0.05085660144686699,81.25%\n","Step 116\n","Loss & Accuracy: 0.04865868389606476,62.5%\n","Step 117\n","Loss & Accuracy: 0.044777072966098785,62.5%\n","Step 118\n","Loss & Accuracy: 0.049581415951251984,68.75%\n","Step 119\n","Loss & Accuracy: 0.04482739791274071,50.0%\n","Step 120\n","Loss & Accuracy: 0.045980773866176605,65.625%\n","Step 121\n","Loss & Accuracy: 0.049214959144592285,65.625%\n","Step 122\n","Loss & Accuracy: 0.046167079359292984,62.5%\n","Step 123\n","Loss & Accuracy: 0.04821997508406639,71.875%\n","Step 124\n","Loss & Accuracy: 0.044849734753370285,62.5%\n","Step 125\n","Loss & Accuracy: 0.04476006329059601,62.5%\n","Step 126\n","Loss & Accuracy: 0.05016232281923294,78.125%\n","Step 127\n","Loss & Accuracy: 0.046759843826293945,68.75%\n","Step 128\n","Loss & Accuracy: 0.05206681042909622,81.25%\n","Step 129\n","Loss & Accuracy: 0.04860154166817665,75.0%\n","Step 130\n","Loss & Accuracy: 0.048891764134168625,75.0%\n","Step 131\n","Loss & Accuracy: 0.04658947512507439,75.0%\n","Step 132\n","Loss & Accuracy: 0.04977220296859741,75.0%\n","Step 133\n","Loss & Accuracy: 0.050477348268032074,56.25%\n","Step 134\n","Loss & Accuracy: 0.04353345185518265,59.375%\n","Step 135\n","Loss & Accuracy: 0.041650231927633286,59.375%\n","Step 136\n","Loss & Accuracy: 0.04940754175186157,75.0%\n","Step 137\n","Loss & Accuracy: 0.04876808449625969,68.75%\n","Step 138\n","Loss & Accuracy: 0.04142178222537041,59.375%\n","Step 139\n","Loss & Accuracy: 0.050253815948963165,56.25%\n","Step 140\n","Loss & Accuracy: 0.046433087438344955,59.375%\n","Step 141\n","Loss & Accuracy: 0.04929730296134949,68.75%\n","Step 142\n","Loss & Accuracy: 0.04897376894950867,81.25%\n","Step 143\n","Loss & Accuracy: 0.04945051670074463,78.125%\n","Step 144\n","Loss & Accuracy: 0.051380954682826996,71.875%\n","Step 145\n","Loss & Accuracy: 0.05146774277091026,71.875%\n","Step 146\n","Loss & Accuracy: 0.04909335821866989,71.875%\n","Step 147\n","Loss & Accuracy: 0.04599399492144585,81.25%\n","Step 148\n","Loss & Accuracy: 0.050092972815036774,65.625%\n","Step 149\n","Loss & Accuracy: 0.04412628337740898,81.25%\n","Step 150\n","Loss & Accuracy: 0.0463358573615551,59.375%\n","Step 151\n","Loss & Accuracy: 0.0450008399784565,62.5%\n","Step 152\n","Loss & Accuracy: 0.04763541743159294,68.75%\n","Step 153\n","Loss & Accuracy: 0.04602965712547302,65.625%\n","Step 154\n","Loss & Accuracy: 0.050757087767124176,78.125%\n","Step 155\n","Loss & Accuracy: 0.051086146384477615,71.875%\n","Step 156\n","Loss & Accuracy: 0.04759035259485245,75.0%\n","Step 157\n","Loss & Accuracy: 0.05097343772649765,78.125%\n","Step 158\n","Loss & Accuracy: 0.04581523686647415,65.625%\n","Step 159\n","Loss & Accuracy: 0.04449787363409996,62.5%\n","Step 160\n","Loss & Accuracy: 0.04267922788858414,62.5%\n","Step 161\n","Loss & Accuracy: 0.051126159727573395,75.0%\n","Step 162\n","Loss & Accuracy: 0.04562584310770035,68.75%\n","Step 163\n","Loss & Accuracy: 0.04652221500873566,50.0%\n","Step 164\n","Loss & Accuracy: 0.048099786043167114,78.125%\n","Step 165\n","Loss & Accuracy: 0.04765188694000244,81.25%\n","Step 166\n","Loss & Accuracy: 0.04728205129504204,71.875%\n","Step 167\n","Loss & Accuracy: 0.04455765336751938,53.125%\n","Step 168\n","Loss & Accuracy: 0.050442375242710114,84.375%\n","Step 169\n","Loss & Accuracy: 0.046992041170597076,78.125%\n","Step 170\n","Loss & Accuracy: 0.046162404119968414,56.25%\n","Step 171\n","Loss & Accuracy: 0.04829709231853485,75.0%\n","Step 172\n","Loss & Accuracy: 0.042533278465270996,65.625%\n","Step 173\n","Loss & Accuracy: 0.045680053532123566,84.375%\n","Step 174\n","Loss & Accuracy: 0.048666104674339294,81.25%\n","Step 175\n","Loss & Accuracy: 0.047352202236652374,65.625%\n","Step 176\n","Loss & Accuracy: 0.04462404176592827,59.375%\n","Step 177\n","Loss & Accuracy: 0.04444848746061325,71.875%\n","Step 178\n","Loss & Accuracy: 0.047532446682453156,75.0%\n","Step 179\n","Loss & Accuracy: 0.04557257145643234,68.75%\n","Step 180\n","Loss & Accuracy: 0.04925830662250519,68.75%\n","Step 181\n","Loss & Accuracy: 0.04271954298019409,71.875%\n","Step 182\n","Loss & Accuracy: 0.047950007021427155,53.125%\n","Step 183\n","Loss & Accuracy: 0.044667989015579224,50.0%\n","Step 184\n","Loss & Accuracy: 0.04902587831020355,62.5%\n","Step 185\n","Loss & Accuracy: 0.04532184824347496,71.875%\n","Step 186\n","Loss & Accuracy: 0.049226757138967514,59.375%\n","Step 187\n","Loss & Accuracy: 0.04452881962060928,53.125%\n","Step 188\n","Loss & Accuracy: 0.04934549331665039,62.5%\n","Step 189\n","Loss & Accuracy: 0.04697142541408539,68.75%\n","Step 190\n","Loss & Accuracy: 0.046283017843961716,56.25%\n","Step 191\n","Loss & Accuracy: 0.045909710228443146,56.25%\n","Step 192\n","Loss & Accuracy: 0.04938843101263046,62.5%\n","Step 193\n","Loss & Accuracy: 0.04251588135957718,59.375%\n","Step 194\n","Loss & Accuracy: 0.04525730758905411,75.0%\n","Step 195\n","Loss & Accuracy: 0.05281146988272667,65.625%\n","Step 196\n","Loss & Accuracy: 0.04907861351966858,59.375%\n","Step 197\n","Loss & Accuracy: 0.0472402423620224,59.375%\n","Step 198\n","Loss & Accuracy: 0.0428331196308136,53.125%\n","Step 199\n","Loss & Accuracy: 0.045798033475875854,56.25%\n","Step 200\n","Loss & Accuracy: 0.04087848961353302,53.125%\n","Step 201\n","Loss & Accuracy: 0.046967677772045135,78.125%\n","Step 202\n","Loss & Accuracy: 0.044161081314086914,65.625%\n","Step 203\n","Loss & Accuracy: 0.04879004508256912,65.625%\n","Step 204\n","Loss & Accuracy: 0.04380994662642479,71.875%\n","Step 205\n","Loss & Accuracy: 0.04728225618600845,71.875%\n","Step 206\n","Loss & Accuracy: 0.04341023415327072,37.5%\n","Step 207\n","Loss & Accuracy: 0.046894218772649765,78.125%\n","Step 208\n","Loss & Accuracy: 0.04406047984957695,50.0%\n","Step 209\n","Loss & Accuracy: 0.04963069409132004,53.125%\n","Step 210\n","Loss & Accuracy: 0.04648343101143837,46.875%\n","Step 211\n","Loss & Accuracy: 0.04929511249065399,75.0%\n","Step 212\n","Loss & Accuracy: 0.051139913499355316,62.5%\n","Step 213\n","Loss & Accuracy: 0.041122522205114365,56.25%\n","Step 214\n","Loss & Accuracy: 0.04938129708170891,56.25%\n","Step 215\n","Loss & Accuracy: 0.04897520691156387,71.875%\n","Step 216\n","Loss & Accuracy: 0.04892921447753906,81.25%\n","Step 217\n","Loss & Accuracy: 0.045863572508096695,71.42857142857143%\n","Epoch 3/10\n","Step 1\n","Loss & Accuracy: 0.050807174295186996,59.375%\n","Step 2\n","Loss & Accuracy: 0.04933599755167961,75.0%\n","Step 3\n","Loss & Accuracy: 0.04895322769880295,68.75%\n","Step 4\n","Loss & Accuracy: 0.047181155532598495,71.875%\n","Step 5\n","Loss & Accuracy: 0.050616875290870667,71.875%\n","Step 6\n","Loss & Accuracy: 0.04205837473273277,65.625%\n","Step 7\n","Loss & Accuracy: 0.04556652158498764,56.25%\n","Step 8\n","Loss & Accuracy: 0.04702725261449814,78.125%\n","Step 9\n","Loss & Accuracy: 0.04219254106283188,65.625%\n","Step 10\n","Loss & Accuracy: 0.04515664279460907,59.375%\n","Step 11\n","Loss & Accuracy: 0.04203816503286362,59.375%\n","Step 12\n","Loss & Accuracy: 0.04545954614877701,59.375%\n","Step 13\n","Loss & Accuracy: 0.0438883937895298,56.25%\n","Step 14\n","Loss & Accuracy: 0.04940658435225487,75.0%\n","Step 15\n","Loss & Accuracy: 0.04566331207752228,62.5%\n","Step 16\n","Loss & Accuracy: 0.04393365979194641,53.125%\n","Step 17\n","Loss & Accuracy: 0.04560371860861778,65.625%\n","Step 18\n","Loss & Accuracy: 0.049928899854421616,81.25%\n","Step 19\n","Loss & Accuracy: 0.04827892780303955,71.875%\n","Step 20\n","Loss & Accuracy: 0.05007615685462952,84.375%\n","Step 21\n","Loss & Accuracy: 0.04836193472146988,84.375%\n","Step 22\n","Loss & Accuracy: 0.05024222284555435,78.125%\n","Step 23\n","Loss & Accuracy: 0.04708242788910866,65.625%\n","Step 24\n","Loss & Accuracy: 0.04656599089503288,78.125%\n","Step 25\n","Loss & Accuracy: 0.043539926409721375,68.75%\n","Step 26\n","Loss & Accuracy: 0.044508859515190125,68.75%\n","Step 27\n","Loss & Accuracy: 0.046974748373031616,59.375%\n","Step 28\n","Loss & Accuracy: 0.04902522638440132,65.625%\n","Step 29\n","Loss & Accuracy: 0.047178540378808975,84.375%\n","Step 30\n","Loss & Accuracy: 0.04750790446996689,71.875%\n","Step 31\n","Loss & Accuracy: 0.04081474989652634,68.75%\n","Step 32\n","Loss & Accuracy: 0.049058668315410614,84.375%\n","Step 33\n","Loss & Accuracy: 0.04379236698150635,56.25%\n","Step 34\n","Loss & Accuracy: 0.04744472727179527,59.375%\n","Step 35\n","Loss & Accuracy: 0.04382021725177765,65.625%\n","Step 36\n","Loss & Accuracy: 0.05201065167784691,93.75%\n","Step 37\n","Loss & Accuracy: 0.042159318923950195,50.0%\n","Step 38\n","Loss & Accuracy: 0.047020524740219116,71.875%\n","Step 39\n","Loss & Accuracy: 0.04569534584879875,59.375%\n","Step 40\n","Loss & Accuracy: 0.04929494112730026,62.5%\n","Step 41\n","Loss & Accuracy: 0.04549209773540497,59.375%\n","Step 42\n","Loss & Accuracy: 0.04680918529629707,62.5%\n","Step 43\n","Loss & Accuracy: 0.048536986112594604,75.0%\n","Step 44\n","Loss & Accuracy: 0.048483580350875854,68.75%\n","Step 45\n","Loss & Accuracy: 0.04861105978488922,62.5%\n","Step 46\n","Loss & Accuracy: 0.04549151659011841,53.125%\n","Step 47\n","Loss & Accuracy: 0.050068311393260956,56.25%\n","Step 48\n","Loss & Accuracy: 0.04216669499874115,50.0%\n","Step 49\n","Loss & Accuracy: 0.048358410596847534,59.375%\n","Step 50\n","Loss & Accuracy: 0.050331465899944305,65.625%\n","Step 51\n","Loss & Accuracy: 0.04534508287906647,50.0%\n","Step 52\n","Loss & Accuracy: 0.04381166771054268,53.125%\n","Step 53\n","Loss & Accuracy: 0.04437143728137016,50.0%\n","Step 54\n","Loss & Accuracy: 0.045473359525203705,65.625%\n","Step 55\n","Loss & Accuracy: 0.04909301549196243,59.375%\n","Step 56\n","Loss & Accuracy: 0.049117591232061386,59.375%\n","Step 57\n","Loss & Accuracy: 0.04538830369710922,78.125%\n","Step 58\n","Loss & Accuracy: 0.05030824989080429,65.625%\n","Step 59\n","Loss & Accuracy: 0.04062722250819206,50.0%\n","Step 60\n","Loss & Accuracy: 0.04373902082443237,62.5%\n","Step 61\n","Loss & Accuracy: 0.045536309480667114,56.25%\n","Step 62\n","Loss & Accuracy: 0.044963836669921875,71.875%\n","Step 63\n","Loss & Accuracy: 0.04841996729373932,75.0%\n","Step 64\n","Loss & Accuracy: 0.04726901650428772,65.625%\n","Step 65\n","Loss & Accuracy: 0.04548831656575203,65.625%\n","Step 66\n","Loss & Accuracy: 0.045131996273994446,71.875%\n","Step 67\n","Loss & Accuracy: 0.04511912539601326,59.375%\n","Step 68\n","Loss & Accuracy: 0.0519285574555397,56.25%\n","Step 69\n","Loss & Accuracy: 0.04643934220075607,53.125%\n","Step 70\n","Loss & Accuracy: 0.04959815740585327,59.375%\n","Step 71\n","Loss & Accuracy: 0.047933973371982574,56.25%\n","Step 72\n","Loss & Accuracy: 0.04669521003961563,53.125%\n","Step 73\n","Loss & Accuracy: 0.04892931133508682,59.375%\n","Step 74\n","Loss & Accuracy: 0.04664875566959381,62.5%\n","Step 75\n","Loss & Accuracy: 0.046756595373153687,68.75%\n","Step 76\n","Loss & Accuracy: 0.04193147271871567,59.375%\n","Step 77\n","Loss & Accuracy: 0.04874151945114136,59.375%\n","Step 78\n","Loss & Accuracy: 0.04689319059252739,56.25%\n","Step 79\n","Loss & Accuracy: 0.045149944722652435,65.625%\n","Step 80\n","Loss & Accuracy: 0.048050761222839355,68.75%\n","Step 81\n","Loss & Accuracy: 0.04350403696298599,59.375%\n","Step 82\n","Loss & Accuracy: 0.049767084419727325,71.875%\n","Step 83\n","Loss & Accuracy: 0.04964763671159744,78.125%\n","Step 84\n","Loss & Accuracy: 0.04799339920282364,71.875%\n","Step 85\n","Loss & Accuracy: 0.046283889561891556,56.25%\n","Step 86\n","Loss & Accuracy: 0.04646536707878113,68.75%\n","Step 87\n","Loss & Accuracy: 0.04633704945445061,68.75%\n","Step 88\n","Loss & Accuracy: 0.046524882316589355,59.375%\n","Step 89\n","Loss & Accuracy: 0.04825430363416672,53.125%\n","Step 90\n","Loss & Accuracy: 0.043018363416194916,68.75%\n","Step 91\n","Loss & Accuracy: 0.051083702594041824,65.625%\n","Step 92\n","Loss & Accuracy: 0.04815790057182312,34.375%\n","Step 93\n","Loss & Accuracy: 0.04938908666372299,56.25%\n","Step 94\n","Loss & Accuracy: 0.04941247031092644,59.375%\n","Step 95\n","Loss & Accuracy: 0.04776940494775772,56.25%\n","Step 96\n","Loss & Accuracy: 0.0486312061548233,37.5%\n","Step 97\n","Loss & Accuracy: 0.04810558632016182,53.125%\n","Step 98\n","Loss & Accuracy: 0.043557681143283844,71.875%\n","Step 99\n","Loss & Accuracy: 0.04720030725002289,53.125%\n","Step 100\n","Loss & Accuracy: 0.050412699580192566,75.0%\n","Step 101\n","Loss & Accuracy: 0.048663996160030365,68.75%\n","Step 102\n","Loss & Accuracy: 0.04211372882127762,50.0%\n","Step 103\n","Loss & Accuracy: 0.050066620111465454,56.25%\n","Step 104\n","Loss & Accuracy: 0.04689112678170204,50.0%\n","Step 105\n","Loss & Accuracy: 0.043621085584163666,53.125%\n","Step 106\n","Loss & Accuracy: 0.04366045817732811,50.0%\n","Step 107\n","Loss & Accuracy: 0.046601247042417526,50.0%\n","Step 108\n","Loss & Accuracy: 0.044755205512046814,65.625%\n","Step 109\n","Loss & Accuracy: 0.04345133900642395,50.0%\n","Step 110\n","Loss & Accuracy: 0.04478464275598526,56.25%\n","Step 111\n","Loss & Accuracy: 0.05114893615245819,59.375%\n","Step 112\n","Loss & Accuracy: 0.04330482706427574,65.625%\n","Step 113\n","Loss & Accuracy: 0.048448462039232254,62.5%\n","Step 114\n","Loss & Accuracy: 0.04656139016151428,62.5%\n","Step 115\n","Loss & Accuracy: 0.049951571971178055,81.25%\n","Step 116\n","Loss & Accuracy: 0.046861279755830765,68.75%\n","Step 117\n","Loss & Accuracy: 0.04349551349878311,53.125%\n","Step 118\n","Loss & Accuracy: 0.04831172898411751,68.75%\n","Step 119\n","Loss & Accuracy: 0.043395303189754486,50.0%\n","Step 120\n","Loss & Accuracy: 0.04472564533352852,50.0%\n","Step 121\n","Loss & Accuracy: 0.0462484210729599,71.875%\n","Step 122\n","Loss & Accuracy: 0.04479537159204483,53.125%\n","Step 123\n","Loss & Accuracy: 0.046281784772872925,56.25%\n","Step 124\n","Loss & Accuracy: 0.04304581135511398,53.125%\n","Step 125\n","Loss & Accuracy: 0.04319389909505844,56.25%\n","Step 126\n","Loss & Accuracy: 0.04786912351846695,71.875%\n","Step 127\n","Loss & Accuracy: 0.04486346244812012,62.5%\n","Step 128\n","Loss & Accuracy: 0.04983930289745331,59.375%\n","Step 129\n","Loss & Accuracy: 0.04637034982442856,56.25%\n","Step 130\n","Loss & Accuracy: 0.04612196981906891,68.75%\n","Step 131\n","Loss & Accuracy: 0.04509928822517395,62.5%\n","Step 132\n","Loss & Accuracy: 0.047901835292577744,71.875%\n","Step 133\n","Loss & Accuracy: 0.04805903136730194,65.625%\n","Step 134\n","Loss & Accuracy: 0.042071983218193054,43.75%\n","Step 135\n","Loss & Accuracy: 0.04015224799513817,46.875%\n","Step 136\n","Loss & Accuracy: 0.048360951244831085,56.25%\n","Step 137\n","Loss & Accuracy: 0.04643474519252777,56.25%\n","Step 138\n","Loss & Accuracy: 0.04017344117164612,37.5%\n","Step 139\n","Loss & Accuracy: 0.04789462313055992,43.75%\n","Step 140\n","Loss & Accuracy: 0.04528127238154411,31.25%\n","Step 141\n","Loss & Accuracy: 0.04846064746379852,43.75%\n","Step 142\n","Loss & Accuracy: 0.048083264380693436,62.5%\n","Step 143\n","Loss & Accuracy: 0.048171430826187134,43.75%\n","Step 144\n","Loss & Accuracy: 0.04982277750968933,59.375%\n","Step 145\n","Loss & Accuracy: 0.049654822796583176,62.5%\n","Step 146\n","Loss & Accuracy: 0.04800136014819145,65.625%\n","Step 147\n","Loss & Accuracy: 0.04461498558521271,59.375%\n","Step 148\n","Loss & Accuracy: 0.04794939607381821,75.0%\n","Step 149\n","Loss & Accuracy: 0.04345390200614929,65.625%\n","Step 150\n","Loss & Accuracy: 0.04476546496152878,59.375%\n","Step 151\n","Loss & Accuracy: 0.04337809234857559,62.5%\n","Step 152\n","Loss & Accuracy: 0.04653432220220566,59.375%\n","Step 153\n","Loss & Accuracy: 0.044729772955179214,53.125%\n","Step 154\n","Loss & Accuracy: 0.0498117096722126,50.0%\n","Step 155\n","Loss & Accuracy: 0.049527496099472046,62.5%\n","Step 156\n","Loss & Accuracy: 0.04634581133723259,68.75%\n","Step 157\n","Loss & Accuracy: 0.049596622586250305,62.5%\n","Step 158\n","Loss & Accuracy: 0.04487913101911545,59.375%\n","Step 159\n","Loss & Accuracy: 0.043440114706754684,62.5%\n","Step 160\n","Loss & Accuracy: 0.04170926287770271,62.5%\n","Step 161\n","Loss & Accuracy: 0.04957420751452446,68.75%\n","Step 162\n","Loss & Accuracy: 0.04480607807636261,59.375%\n","Step 163\n","Loss & Accuracy: 0.04480263590812683,62.5%\n","Step 164\n","Loss & Accuracy: 0.04651322215795517,62.5%\n","Step 165\n","Loss & Accuracy: 0.046201907098293304,59.375%\n","Step 166\n","Loss & Accuracy: 0.046613313257694244,56.25%\n","Step 167\n","Loss & Accuracy: 0.04355749487876892,53.125%\n","Step 168\n","Loss & Accuracy: 0.04964537173509598,71.875%\n","Step 169\n","Loss & Accuracy: 0.046405479311943054,59.375%\n","Step 170\n","Loss & Accuracy: 0.04482007399201393,43.75%\n","Step 171\n","Loss & Accuracy: 0.04686196893453598,68.75%\n","Step 172\n","Loss & Accuracy: 0.041411686688661575,50.0%\n","Step 173\n","Loss & Accuracy: 0.044754233211278915,59.375%\n","Step 174\n","Loss & Accuracy: 0.04791635274887085,62.5%\n","Step 175\n","Loss & Accuracy: 0.046289317309856415,65.625%\n","Step 176\n","Loss & Accuracy: 0.042967427521944046,65.625%\n","Step 177\n","Loss & Accuracy: 0.0429782010614872,43.75%\n","Step 178\n","Loss & Accuracy: 0.04677347093820572,53.125%\n","Step 179\n","Loss & Accuracy: 0.04453243315219879,62.5%\n","Step 180\n","Loss & Accuracy: 0.04816155880689621,62.5%\n","Step 181\n","Loss & Accuracy: 0.04198599234223366,59.375%\n","Step 182\n","Loss & Accuracy: 0.04706763103604317,71.875%\n","Step 183\n","Loss & Accuracy: 0.04385073110461235,59.375%\n","Step 184\n","Loss & Accuracy: 0.048517294228076935,59.375%\n","Step 185\n","Loss & Accuracy: 0.04483868181705475,71.875%\n","Step 186\n","Loss & Accuracy: 0.0479796901345253,62.5%\n","Step 187\n","Loss & Accuracy: 0.04365161061286926,53.125%\n","Step 188\n","Loss & Accuracy: 0.04913780838251114,50.0%\n","Step 189\n","Loss & Accuracy: 0.046169839799404144,68.75%\n","Step 190\n","Loss & Accuracy: 0.04500081390142441,43.75%\n","Step 191\n","Loss & Accuracy: 0.04488188773393631,59.375%\n","Step 192\n","Loss & Accuracy: 0.048132121562957764,62.5%\n","Step 193\n","Loss & Accuracy: 0.0417187437415123,37.5%\n","Step 194\n","Loss & Accuracy: 0.04475248605012894,25.0%\n","Step 195\n","Loss & Accuracy: 0.0520842969417572,46.875%\n","Step 196\n","Loss & Accuracy: 0.047677554190158844,62.5%\n","Step 197\n","Loss & Accuracy: 0.04634825140237808,68.75%\n","Step 198\n","Loss & Accuracy: 0.04194069653749466,46.875%\n","Step 199\n","Loss & Accuracy: 0.04499104619026184,62.5%\n","Step 200\n","Loss & Accuracy: 0.04031695798039436,43.75%\n","Step 201\n","Loss & Accuracy: 0.046725526452064514,43.75%\n","Step 202\n","Loss & Accuracy: 0.04331037402153015,50.0%\n","Step 203\n","Loss & Accuracy: 0.048381298780441284,46.875%\n","Step 204\n","Loss & Accuracy: 0.043150343000888824,50.0%\n","Step 205\n","Loss & Accuracy: 0.046679116785526276,43.75%\n","Step 206\n","Loss & Accuracy: 0.04187270253896713,43.75%\n","Step 207\n","Loss & Accuracy: 0.04646528884768486,53.125%\n","Step 208\n","Loss & Accuracy: 0.043516598641872406,50.0%\n","Step 209\n","Loss & Accuracy: 0.048635415732860565,40.625%\n","Step 210\n","Loss & Accuracy: 0.0451061986386776,43.75%\n","Step 211\n","Loss & Accuracy: 0.04793373495340347,68.75%\n","Step 212\n","Loss & Accuracy: 0.049562059342861176,65.625%\n","Step 213\n","Loss & Accuracy: 0.039769552648067474,56.25%\n","Step 214\n","Loss & Accuracy: 0.04796066880226135,56.25%\n","Step 215\n","Loss & Accuracy: 0.04828547686338425,68.75%\n","Step 216\n","Loss & Accuracy: 0.04842536151409149,53.125%\n","Step 217\n","Loss & Accuracy: 0.04420963302254677,57.14285714285714%\n","Epoch 4/10\n","Step 1\n","Loss & Accuracy: 0.049849435687065125,62.5%\n","Step 2\n","Loss & Accuracy: 0.04824042320251465,75.0%\n","Step 3\n","Loss & Accuracy: 0.047965146601200104,71.875%\n","Step 4\n","Loss & Accuracy: 0.046362243592739105,71.875%\n","Step 5\n","Loss & Accuracy: 0.04964858293533325,78.125%\n","Step 6\n","Loss & Accuracy: 0.04174792394042015,56.25%\n","Step 7\n","Loss & Accuracy: 0.044480904936790466,65.625%\n","Step 8\n","Loss & Accuracy: 0.046379949897527695,71.875%\n","Step 9\n","Loss & Accuracy: 0.041659124195575714,59.375%\n","Step 10\n","Loss & Accuracy: 0.044743914157152176,68.75%\n","Step 11\n","Loss & Accuracy: 0.04144533351063728,62.5%\n","Step 12\n","Loss & Accuracy: 0.04473347216844559,59.375%\n","Step 13\n","Loss & Accuracy: 0.04324305057525635,50.0%\n","Step 14\n","Loss & Accuracy: 0.047836970537900925,59.375%\n","Step 15\n","Loss & Accuracy: 0.04498296231031418,59.375%\n","Step 16\n","Loss & Accuracy: 0.04293659329414368,62.5%\n","Step 17\n","Loss & Accuracy: 0.044573210179805756,62.5%\n","Step 18\n","Loss & Accuracy: 0.04925491660833359,78.125%\n","Step 19\n","Loss & Accuracy: 0.04758739471435547,75.0%\n","Step 20\n","Loss & Accuracy: 0.04913461208343506,68.75%\n","Step 21\n","Loss & Accuracy: 0.047420114278793335,78.125%\n","Step 22\n","Loss & Accuracy: 0.04910014942288399,56.25%\n","Step 23\n","Loss & Accuracy: 0.04633746296167374,46.875%\n","Step 24\n","Loss & Accuracy: 0.04602246358990669,71.875%\n","Step 25\n","Loss & Accuracy: 0.04304710775613785,59.375%\n","Step 26\n","Loss & Accuracy: 0.043227650225162506,62.5%\n","Step 27\n","Loss & Accuracy: 0.04643046110868454,71.875%\n","Step 28\n","Loss & Accuracy: 0.04795518517494202,62.5%\n","Step 29\n","Loss & Accuracy: 0.04600144177675247,71.875%\n","Step 30\n","Loss & Accuracy: 0.04605875909328461,59.375%\n","Step 31\n","Loss & Accuracy: 0.039928168058395386,62.5%\n","Step 32\n","Loss & Accuracy: 0.047735072672367096,71.875%\n","Step 33\n","Loss & Accuracy: 0.04299943149089813,46.875%\n","Step 34\n","Loss & Accuracy: 0.04608738049864769,59.375%\n","Step 35\n","Loss & Accuracy: 0.04280181974172592,65.625%\n","Step 36\n","Loss & Accuracy: 0.05079425871372223,90.625%\n","Step 37\n","Loss & Accuracy: 0.041194453835487366,56.25%\n","Step 38\n","Loss & Accuracy: 0.046029746532440186,65.625%\n","Step 39\n","Loss & Accuracy: 0.04450572282075882,65.625%\n","Step 40\n","Loss & Accuracy: 0.04774146154522896,56.25%\n","Step 41\n","Loss & Accuracy: 0.0447196289896965,40.625%\n","Step 42\n","Loss & Accuracy: 0.046232640743255615,43.75%\n","Step 43\n","Loss & Accuracy: 0.04765553027391434,50.0%\n","Step 44\n","Loss & Accuracy: 0.04775713384151459,56.25%\n","Step 45\n","Loss & Accuracy: 0.047734495252370834,50.0%\n","Step 46\n","Loss & Accuracy: 0.04451069235801697,59.375%\n","Step 47\n","Loss & Accuracy: 0.049316924065351486,56.25%\n","Step 48\n","Loss & Accuracy: 0.04124533385038376,53.125%\n","Step 49\n","Loss & Accuracy: 0.04780342057347298,56.25%\n","Step 50\n","Loss & Accuracy: 0.04908709228038788,84.375%\n","Step 51\n","Loss & Accuracy: 0.04460086673498154,65.625%\n","Step 52\n","Loss & Accuracy: 0.04305616021156311,59.375%\n","Step 53\n","Loss & Accuracy: 0.04304104298353195,53.125%\n","Step 54\n","Loss & Accuracy: 0.044530969113111496,62.5%\n","Step 55\n","Loss & Accuracy: 0.04765426367521286,78.125%\n","Step 56\n","Loss & Accuracy: 0.047464437782764435,75.0%\n","Step 57\n","Loss & Accuracy: 0.044265858829021454,71.875%\n","Step 58\n","Loss & Accuracy: 0.04916941374540329,56.25%\n","Step 59\n","Loss & Accuracy: 0.03983002156019211,62.5%\n","Step 60\n","Loss & Accuracy: 0.0429065078496933,53.125%\n","Step 61\n","Loss & Accuracy: 0.04444045573472977,65.625%\n","Step 62\n","Loss & Accuracy: 0.044531941413879395,68.75%\n","Step 63\n","Loss & Accuracy: 0.0476839654147625,65.625%\n","Step 64\n","Loss & Accuracy: 0.04662329703569412,75.0%\n","Step 65\n","Loss & Accuracy: 0.04458733648061752,75.0%\n","Step 66\n","Loss & Accuracy: 0.04439461976289749,65.625%\n","Step 67\n","Loss & Accuracy: 0.044856958091259,56.25%\n","Step 68\n","Loss & Accuracy: 0.051263414323329926,65.625%\n","Step 69\n","Loss & Accuracy: 0.04656597971916199,37.5%\n","Step 70\n","Loss & Accuracy: 0.0496295802295208,37.5%\n","Step 71\n","Loss & Accuracy: 0.04826928675174713,50.0%\n","Step 72\n","Loss & Accuracy: 0.046333469450473785,43.75%\n","Step 73\n","Loss & Accuracy: 0.04823099821805954,40.625%\n","Step 74\n","Loss & Accuracy: 0.04627024754881859,65.625%\n","Step 75\n","Loss & Accuracy: 0.0462646521627903,71.875%\n","Step 76\n","Loss & Accuracy: 0.041382402181625366,56.25%\n","Step 77\n","Loss & Accuracy: 0.04789531230926514,71.875%\n","Step 78\n","Loss & Accuracy: 0.04615353047847748,65.625%\n","Step 79\n","Loss & Accuracy: 0.044375937432050705,75.0%\n","Step 80\n","Loss & Accuracy: 0.04765922576189041,53.125%\n","Step 81\n","Loss & Accuracy: 0.04281038045883179,59.375%\n","Step 82\n","Loss & Accuracy: 0.049394506961107254,65.625%\n","Step 83\n","Loss & Accuracy: 0.049130894243717194,68.75%\n","Step 84\n","Loss & Accuracy: 0.04768342524766922,62.5%\n","Step 85\n","Loss & Accuracy: 0.04620746895670891,65.625%\n","Step 86\n","Loss & Accuracy: 0.045974619686603546,81.25%\n","Step 87\n","Loss & Accuracy: 0.04609408974647522,75.0%\n","Step 88\n","Loss & Accuracy: 0.04624076932668686,68.75%\n","Step 89\n","Loss & Accuracy: 0.04764387384057045,75.0%\n","Step 90\n","Loss & Accuracy: 0.04268956929445267,71.875%\n","Step 91\n","Loss & Accuracy: 0.05052848532795906,87.5%\n","Step 92\n","Loss & Accuracy: 0.047694239765405655,50.0%\n","Step 93\n","Loss & Accuracy: 0.049163952469825745,71.875%\n","Step 94\n","Loss & Accuracy: 0.04934946820139885,50.0%\n","Step 95\n","Loss & Accuracy: 0.04764954745769501,53.125%\n","Step 96\n","Loss & Accuracy: 0.04768577963113785,40.625%\n","Step 97\n","Loss & Accuracy: 0.04778314381837845,59.375%\n","Step 98\n","Loss & Accuracy: 0.04288589209318161,56.25%\n","Step 99\n","Loss & Accuracy: 0.04627784341573715,40.625%\n","Step 100\n","Loss & Accuracy: 0.04918970167636871,78.125%\n","Step 101\n","Loss & Accuracy: 0.04762781411409378,75.0%\n","Step 102\n","Loss & Accuracy: 0.04119426757097244,53.125%\n","Step 103\n","Loss & Accuracy: 0.049040909856557846,78.125%\n","Step 104\n","Loss & Accuracy: 0.04585018754005432,78.125%\n","Step 105\n","Loss & Accuracy: 0.04267783463001251,59.375%\n","Step 106\n","Loss & Accuracy: 0.04272677004337311,50.0%\n","Step 107\n","Loss & Accuracy: 0.04598197713494301,62.5%\n","Step 108\n","Loss & Accuracy: 0.044526178389787674,56.25%\n","Step 109\n","Loss & Accuracy: 0.0427657812833786,62.5%\n","Step 110\n","Loss & Accuracy: 0.04435717314481735,68.75%\n","Step 111\n","Loss & Accuracy: 0.050761327147483826,71.875%\n","Step 112\n","Loss & Accuracy: 0.042793452739715576,59.375%\n","Step 113\n","Loss & Accuracy: 0.047572728246450424,71.875%\n","Step 114\n","Loss & Accuracy: 0.04578045383095741,75.0%\n","Step 115\n","Loss & Accuracy: 0.049095697700977325,81.25%\n","Step 116\n","Loss & Accuracy: 0.045978136360645294,62.5%\n","Step 117\n","Loss & Accuracy: 0.042671509087085724,68.75%\n","Step 118\n","Loss & Accuracy: 0.04748329520225525,78.125%\n","Step 119\n","Loss & Accuracy: 0.042790479958057404,53.125%\n","Step 120\n","Loss & Accuracy: 0.044572729617357254,71.875%\n","Step 121\n","Loss & Accuracy: 0.04596070200204849,87.5%\n","Step 122\n","Loss & Accuracy: 0.04441308230161667,71.875%\n","Step 123\n","Loss & Accuracy: 0.046215057373046875,53.125%\n","Step 124\n","Loss & Accuracy: 0.04280940815806389,65.625%\n","Step 125\n","Loss & Accuracy: 0.04281727224588394,53.125%\n","Step 126\n","Loss & Accuracy: 0.04752930626273155,56.25%\n","Step 127\n","Loss & Accuracy: 0.04443112760782242,56.25%\n","Step 128\n","Loss & Accuracy: 0.049091681838035583,62.5%\n","Step 129\n","Loss & Accuracy: 0.045910291373729706,43.75%\n","Step 130\n","Loss & Accuracy: 0.04605738818645477,40.625%\n","Step 131\n","Loss & Accuracy: 0.04430164769291878,53.125%\n","Step 132\n","Loss & Accuracy: 0.04740478843450546,84.375%\n","Step 133\n","Loss & Accuracy: 0.04772627726197243,75.0%\n","Step 134\n","Loss & Accuracy: 0.04128187149763107,71.875%\n","Step 135\n","Loss & Accuracy: 0.039642322808504105,62.5%\n","Step 136\n","Loss & Accuracy: 0.04750124737620354,78.125%\n","Step 137\n","Loss & Accuracy: 0.04613722860813141,75.0%\n","Step 138\n","Loss & Accuracy: 0.039626143872737885,56.25%\n","Step 139\n","Loss & Accuracy: 0.04762604087591171,84.375%\n","Step 140\n","Loss & Accuracy: 0.044617220759391785,65.625%\n","Step 141\n","Loss & Accuracy: 0.047775834798812866,68.75%\n","Step 142\n","Loss & Accuracy: 0.04749394580721855,71.875%\n","Step 143\n","Loss & Accuracy: 0.047726765275001526,59.375%\n","Step 144\n","Loss & Accuracy: 0.04919948801398277,71.875%\n","Step 145\n","Loss & Accuracy: 0.049147069454193115,62.5%\n","Step 146\n","Loss & Accuracy: 0.047598905861377716,75.0%\n","Step 147\n","Loss & Accuracy: 0.044559456408023834,50.0%\n","Step 148\n","Loss & Accuracy: 0.04759686067700386,53.125%\n","Step 149\n","Loss & Accuracy: 0.04284834861755371,53.125%\n","Step 150\n","Loss & Accuracy: 0.044303394854068756,56.25%\n","Step 151\n","Loss & Accuracy: 0.04273826256394386,59.375%\n","Step 152\n","Loss & Accuracy: 0.045800916850566864,53.125%\n","Step 153\n","Loss & Accuracy: 0.044129952788352966,75.0%\n","Step 154\n","Loss & Accuracy: 0.04896331578493118,75.0%\n","Step 155\n","Loss & Accuracy: 0.04905204474925995,78.125%\n","Step 156\n","Loss & Accuracy: 0.046156369149684906,71.875%\n","Step 157\n","Loss & Accuracy: 0.0489281564950943,84.375%\n","Step 158\n","Loss & Accuracy: 0.04426538944244385,75.0%\n","Step 159\n","Loss & Accuracy: 0.04286801815032959,71.875%\n","Step 160\n","Loss & Accuracy: 0.041320543736219406,59.375%\n","Step 161\n","Loss & Accuracy: 0.04914449155330658,68.75%\n","Step 162\n","Loss & Accuracy: 0.04435514658689499,56.25%\n","Step 163\n","Loss & Accuracy: 0.04454322159290314,43.75%\n","Step 164\n","Loss & Accuracy: 0.045989472419023514,68.75%\n","Step 165\n","Loss & Accuracy: 0.045876141637563705,84.375%\n","Step 166\n","Loss & Accuracy: 0.04614570364356041,78.125%\n","Step 167\n","Loss & Accuracy: 0.042748790234327316,71.875%\n","Step 168\n","Loss & Accuracy: 0.04890816658735275,93.75%\n","Step 169\n","Loss & Accuracy: 0.04575498774647713,84.375%\n","Step 170\n","Loss & Accuracy: 0.044198669493198395,75.0%\n","Step 171\n","Loss & Accuracy: 0.04578237608075142,68.75%\n","Step 172\n","Loss & Accuracy: 0.041170962154865265,56.25%\n","Step 173\n","Loss & Accuracy: 0.044308096170425415,65.625%\n","Step 174\n","Loss & Accuracy: 0.04754624515771866,81.25%\n","Step 175\n","Loss & Accuracy: 0.046123020350933075,62.5%\n","Step 176\n","Loss & Accuracy: 0.04288729652762413,62.5%\n","Step 177\n","Loss & Accuracy: 0.042846910655498505,53.125%\n","Step 178\n","Loss & Accuracy: 0.04610496014356613,50.0%\n","Step 179\n","Loss & Accuracy: 0.04465989023447037,62.5%\n","Step 180\n","Loss & Accuracy: 0.04760138690471649,50.0%\n","Step 181\n","Loss & Accuracy: 0.0413459911942482,53.125%\n","Step 182\n","Loss & Accuracy: 0.04615292698144913,56.25%\n","Step 183\n","Loss & Accuracy: 0.042777590453624725,65.625%\n","Step 184\n","Loss & Accuracy: 0.0477425679564476,59.375%\n","Step 185\n","Loss & Accuracy: 0.0443231463432312,65.625%\n","Step 186\n","Loss & Accuracy: 0.047727249562740326,53.125%\n","Step 187\n","Loss & Accuracy: 0.04294915497303009,62.5%\n","Step 188\n","Loss & Accuracy: 0.04754135012626648,71.875%\n","Step 189\n","Loss & Accuracy: 0.0463186576962471,75.0%\n","Step 190\n","Loss & Accuracy: 0.04446876794099808,59.375%\n","Step 191\n","Loss & Accuracy: 0.04425600916147232,78.125%\n","Step 192\n","Loss & Accuracy: 0.04752781242132187,68.75%\n","Step 193\n","Loss & Accuracy: 0.04109251871705055,50.0%\n","Step 194\n","Loss & Accuracy: 0.044625423848629,43.75%\n","Step 195\n","Loss & Accuracy: 0.050859007984399796,78.125%\n","Step 196\n","Loss & Accuracy: 0.04785941541194916,59.375%\n","Step 197\n","Loss & Accuracy: 0.04624062404036522,62.5%\n","Step 198\n","Loss & Accuracy: 0.04134581983089447,50.0%\n","Step 199\n","Loss & Accuracy: 0.044555701315402985,43.75%\n","Step 200\n","Loss & Accuracy: 0.039515621960163116,53.125%\n","Step 201\n","Loss & Accuracy: 0.04608885943889618,62.5%\n","Step 202\n","Loss & Accuracy: 0.042856842279434204,46.875%\n","Step 203\n","Loss & Accuracy: 0.04770825430750847,62.5%\n","Step 204\n","Loss & Accuracy: 0.042968615889549255,65.625%\n","Step 205\n","Loss & Accuracy: 0.04606413096189499,71.875%\n","Step 206\n","Loss & Accuracy: 0.04124952107667923,62.5%\n","Step 207\n","Loss & Accuracy: 0.046109043061733246,62.5%\n","Step 208\n","Loss & Accuracy: 0.04265797138214111,56.25%\n","Step 209\n","Loss & Accuracy: 0.04738393425941467,81.25%\n","Step 210\n","Loss & Accuracy: 0.044331423938274384,68.75%\n","Step 211\n","Loss & Accuracy: 0.04737246036529541,75.0%\n","Step 212\n","Loss & Accuracy: 0.04900078848004341,56.25%\n","Step 213\n","Loss & Accuracy: 0.039682887494564056,56.25%\n","Step 214\n","Loss & Accuracy: 0.04760249704122543,59.375%\n","Step 215\n","Loss & Accuracy: 0.047471802681684494,81.25%\n","Step 216\n","Loss & Accuracy: 0.04745061695575714,81.25%\n","Step 217\n","Loss & Accuracy: 0.043653540313243866,71.42857142857143%\n","Epoch 5/10\n","Step 1\n","Loss & Accuracy: 0.049029961228370667,71.875%\n","Step 2\n","Loss & Accuracy: 0.04745084047317505,78.125%\n","Step 3\n","Loss & Accuracy: 0.047487951815128326,75.0%\n","Step 4\n","Loss & Accuracy: 0.04589904844760895,75.0%\n","Step 5\n","Loss & Accuracy: 0.04903513193130493,78.125%\n","Step 6\n","Loss & Accuracy: 0.04112409055233002,56.25%\n","Step 7\n","Loss & Accuracy: 0.044307827949523926,68.75%\n","Step 8\n","Loss & Accuracy: 0.045940153300762177,53.125%\n","Step 9\n","Loss & Accuracy: 0.04115386679768562,56.25%\n","Step 10\n","Loss & Accuracy: 0.04463580250740051,53.125%\n","Step 11\n","Loss & Accuracy: 0.04105401411652565,53.125%\n","Step 12\n","Loss & Accuracy: 0.0442899689078331,62.5%\n","Step 13\n","Loss & Accuracy: 0.043021053075790405,59.375%\n","Step 14\n","Loss & Accuracy: 0.04762190952897072,65.625%\n","Step 15\n","Loss & Accuracy: 0.04436832666397095,59.375%\n","Step 16\n","Loss & Accuracy: 0.042858682572841644,68.75%\n","Step 17\n","Loss & Accuracy: 0.04428232088685036,75.0%\n","Step 18\n","Loss & Accuracy: 0.04909241572022438,75.0%\n","Step 19\n","Loss & Accuracy: 0.04758699610829353,65.625%\n","Step 20\n","Loss & Accuracy: 0.0490412637591362,81.25%\n","Step 21\n","Loss & Accuracy: 0.047640640288591385,71.875%\n","Step 22\n","Loss & Accuracy: 0.04894518479704857,87.5%\n","Step 23\n","Loss & Accuracy: 0.04587304964661598,78.125%\n","Step 24\n","Loss & Accuracy: 0.045796655118465424,68.75%\n","Step 25\n","Loss & Accuracy: 0.0427422970533371,65.625%\n","Step 26\n","Loss & Accuracy: 0.04262920096516609,78.125%\n","Step 27\n","Loss & Accuracy: 0.04615894705057144,62.5%\n","Step 28\n","Loss & Accuracy: 0.04739483818411827,75.0%\n","Step 29\n","Loss & Accuracy: 0.0458139032125473,84.375%\n","Step 30\n","Loss & Accuracy: 0.04594215750694275,78.125%\n","Step 31\n","Loss & Accuracy: 0.039545878767967224,68.75%\n","Step 32\n","Loss & Accuracy: 0.0473882332444191,68.75%\n","Step 33\n","Loss & Accuracy: 0.042657703161239624,71.875%\n","Step 34\n","Loss & Accuracy: 0.04585587978363037,71.875%\n","Step 35\n","Loss & Accuracy: 0.042602039873600006,68.75%\n","Step 36\n","Loss & Accuracy: 0.050498779863119125,84.375%\n","Step 37\n","Loss & Accuracy: 0.041011303663253784,65.625%\n","Step 38\n","Loss & Accuracy: 0.04564836621284485,90.625%\n","Step 39\n","Loss & Accuracy: 0.044267136603593826,84.375%\n","Step 40\n","Loss & Accuracy: 0.0473436638712883,75.0%\n","Step 41\n","Loss & Accuracy: 0.04409194737672806,84.375%\n","Step 42\n","Loss & Accuracy: 0.04577076435089111,68.75%\n","Step 43\n","Loss & Accuracy: 0.047415636479854584,78.125%\n","Step 44\n","Loss & Accuracy: 0.04741320759057999,84.375%\n","Step 45\n","Loss & Accuracy: 0.04743681102991104,81.25%\n","Step 46\n","Loss & Accuracy: 0.044157832860946655,71.875%\n","Step 47\n","Loss & Accuracy: 0.04912125691771507,68.75%\n","Step 48\n","Loss & Accuracy: 0.04105323925614357,65.625%\n","Step 49\n","Loss & Accuracy: 0.04729432240128517,87.5%\n","Step 50\n","Loss & Accuracy: 0.04899650067090988,81.25%\n","Step 51\n","Loss & Accuracy: 0.044181715697050095,68.75%\n","Step 52\n","Loss & Accuracy: 0.04267652705311775,59.375%\n","Step 53\n","Loss & Accuracy: 0.04266712814569473,50.0%\n","Step 54\n","Loss & Accuracy: 0.044176094233989716,62.5%\n","Step 55\n","Loss & Accuracy: 0.04750252887606621,53.125%\n","Step 56\n","Loss & Accuracy: 0.04744286090135574,68.75%\n","Step 57\n","Loss & Accuracy: 0.04439627379179001,65.625%\n","Step 58\n","Loss & Accuracy: 0.04901706799864769,68.75%\n","Step 59\n","Loss & Accuracy: 0.03970048576593399,65.625%\n","Step 60\n","Loss & Accuracy: 0.042629823088645935,65.625%\n","Step 61\n","Loss & Accuracy: 0.04437857121229172,75.0%\n","Step 62\n","Loss & Accuracy: 0.0442100465297699,81.25%\n","Step 63\n","Loss & Accuracy: 0.047316182404756546,81.25%\n","Step 64\n","Loss & Accuracy: 0.045813657343387604,81.25%\n","Step 65\n","Loss & Accuracy: 0.04421694576740265,68.75%\n","Step 66\n","Loss & Accuracy: 0.04432480037212372,65.625%\n","Step 67\n","Loss & Accuracy: 0.044332463294267654,68.75%\n","Step 68\n","Loss & Accuracy: 0.05070938915014267,71.875%\n","Step 69\n","Loss & Accuracy: 0.04583270475268364,68.75%\n","Step 70\n","Loss & Accuracy: 0.049162864685058594,62.5%\n","Step 71\n","Loss & Accuracy: 0.04742608219385147,78.125%\n","Step 72\n","Loss & Accuracy: 0.04590452089905739,59.375%\n","Step 73\n","Loss & Accuracy: 0.047468528151512146,56.25%\n","Step 74\n","Loss & Accuracy: 0.04579245299100876,68.75%\n","Step 75\n","Loss & Accuracy: 0.046138472855091095,65.625%\n","Step 76\n","Loss & Accuracy: 0.041124965995550156,59.375%\n","Step 77\n","Loss & Accuracy: 0.047541894018650055,68.75%\n","Step 78\n","Loss & Accuracy: 0.04587986320257187,62.5%\n","Step 79\n","Loss & Accuracy: 0.044442590326070786,71.875%\n","Step 80\n","Loss & Accuracy: 0.047474607825279236,65.625%\n","Step 81\n","Loss & Accuracy: 0.04253954440355301,75.0%\n","Step 82\n","Loss & Accuracy: 0.04925546050071716,87.5%\n","Step 83\n","Loss & Accuracy: 0.04883602261543274,84.375%\n","Step 84\n","Loss & Accuracy: 0.04739387333393097,78.125%\n","Step 85\n","Loss & Accuracy: 0.04590476676821709,68.75%\n","Step 86\n","Loss & Accuracy: 0.046057313680648804,75.0%\n","Step 87\n","Loss & Accuracy: 0.045921579003334045,68.75%\n","Step 88\n","Loss & Accuracy: 0.04584300145506859,62.5%\n","Step 89\n","Loss & Accuracy: 0.04740099981427193,62.5%\n","Step 90\n","Loss & Accuracy: 0.04300687462091446,53.125%\n","Step 91\n","Loss & Accuracy: 0.0505000501871109,90.625%\n","Step 92\n","Loss & Accuracy: 0.04786640778183937,59.375%\n","Step 93\n","Loss & Accuracy: 0.04968101531267166,65.625%\n","Step 94\n","Loss & Accuracy: 0.04962936043739319,62.5%\n","Step 95\n","Loss & Accuracy: 0.04793437570333481,43.75%\n","Step 96\n","Loss & Accuracy: 0.04779118299484253,53.125%\n","Step 97\n","Loss & Accuracy: 0.04764584079384804,62.5%\n","Step 98\n","Loss & Accuracy: 0.04360073804855347,56.25%\n","Step 99\n","Loss & Accuracy: 0.04618684574961662,56.25%\n","Step 100\n","Loss & Accuracy: 0.0497182160615921,56.25%\n","Step 101\n","Loss & Accuracy: 0.048235513269901276,62.5%\n","Step 102\n","Loss & Accuracy: 0.041743986308574677,46.875%\n","Step 103\n","Loss & Accuracy: 0.04975290969014168,46.875%\n","Step 104\n","Loss & Accuracy: 0.046359483152627945,50.0%\n","Step 105\n","Loss & Accuracy: 0.04296795278787613,46.875%\n","Step 106\n","Loss & Accuracy: 0.04291032999753952,40.625%\n","Step 107\n","Loss & Accuracy: 0.04645607993006706,37.5%\n","Step 108\n","Loss & Accuracy: 0.04435936361551285,43.75%\n","Step 109\n","Loss & Accuracy: 0.0427696593105793,56.25%\n","Step 110\n","Loss & Accuracy: 0.04436217620968819,65.625%\n","Step 111\n","Loss & Accuracy: 0.050686612725257874,62.5%\n","Step 112\n","Loss & Accuracy: 0.04280804470181465,56.25%\n","Step 113\n","Loss & Accuracy: 0.04806411638855934,68.75%\n","Step 114\n","Loss & Accuracy: 0.04597930610179901,59.375%\n","Step 115\n","Loss & Accuracy: 0.049089498817920685,75.0%\n","Step 116\n","Loss & Accuracy: 0.04604709893465042,68.75%\n","Step 117\n","Loss & Accuracy: 0.04271635040640831,53.125%\n","Step 118\n","Loss & Accuracy: 0.04787512123584747,56.25%\n","Step 119\n","Loss & Accuracy: 0.04267502948641777,56.25%\n","Step 120\n","Loss & Accuracy: 0.044235892593860626,56.25%\n","Step 121\n","Loss & Accuracy: 0.04598793387413025,71.875%\n","Step 122\n","Loss & Accuracy: 0.0444178581237793,53.125%\n","Step 123\n","Loss & Accuracy: 0.04595024138689041,71.875%\n","Step 124\n","Loss & Accuracy: 0.04290357604622841,46.875%\n","Step 125\n","Loss & Accuracy: 0.04276258498430252,50.0%\n","Step 126\n","Loss & Accuracy: 0.04748852550983429,62.5%\n","Step 127\n","Loss & Accuracy: 0.044682249426841736,59.375%\n","Step 128\n","Loss & Accuracy: 0.04940293729305267,68.75%\n","Step 129\n","Loss & Accuracy: 0.04591565206646919,81.25%\n","Step 130\n","Loss & Accuracy: 0.04611660912632942,81.25%\n","Step 131\n","Loss & Accuracy: 0.04470363259315491,56.25%\n","Step 132\n","Loss & Accuracy: 0.04743163287639618,81.25%\n","Step 133\n","Loss & Accuracy: 0.047625765204429626,59.375%\n","Step 134\n","Loss & Accuracy: 0.041639428585767746,43.75%\n","Step 135\n","Loss & Accuracy: 0.04014668986201286,50.0%\n","Step 136\n","Loss & Accuracy: 0.04784674942493439,56.25%\n","Step 137\n","Loss & Accuracy: 0.04653625935316086,59.375%\n","Step 138\n","Loss & Accuracy: 0.040304165333509445,43.75%\n","Step 139\n","Loss & Accuracy: 0.048277899622917175,56.25%\n","Step 140\n","Loss & Accuracy: 0.044956743717193604,43.75%\n","Step 141\n","Loss & Accuracy: 0.04797734320163727,56.25%\n","Step 142\n","Loss & Accuracy: 0.04812731593847275,43.75%\n","Step 143\n","Loss & Accuracy: 0.04786816984415054,43.75%\n","Step 144\n","Loss & Accuracy: 0.049719080328941345,53.125%\n","Step 145\n","Loss & Accuracy: 0.04944508522748947,53.125%\n","Step 146\n","Loss & Accuracy: 0.04803015664219856,40.625%\n","Step 147\n","Loss & Accuracy: 0.044598888605833054,56.25%\n","Step 148\n","Loss & Accuracy: 0.04814256727695465,21.875%\n","Step 149\n","Loss & Accuracy: 0.043363407254219055,43.75%\n","Step 150\n","Loss & Accuracy: 0.044823531061410904,46.875%\n","Step 151\n","Loss & Accuracy: 0.043057940900325775,46.875%\n","Step 152\n","Loss & Accuracy: 0.04628480225801468,37.5%\n","Step 153\n","Loss & Accuracy: 0.04447249695658684,59.375%\n","Step 154\n","Loss & Accuracy: 0.049299441277980804,59.375%\n","Step 155\n","Loss & Accuracy: 0.04934775084257126,50.0%\n","Step 156\n","Loss & Accuracy: 0.04604288190603256,40.625%\n","Step 157\n","Loss & Accuracy: 0.04934316873550415,56.25%\n","Step 158\n","Loss & Accuracy: 0.044331058859825134,71.875%\n","Step 159\n","Loss & Accuracy: 0.04281163215637207,65.625%\n","Step 160\n","Loss & Accuracy: 0.04148460179567337,59.375%\n","Step 161\n","Loss & Accuracy: 0.049408793449401855,62.5%\n","Step 162\n","Loss & Accuracy: 0.04447461664676666,59.375%\n","Step 163\n","Loss & Accuracy: 0.04465898126363754,43.75%\n","Step 164\n","Loss & Accuracy: 0.046322017908096313,65.625%\n","Step 165\n","Loss & Accuracy: 0.045997314155101776,75.0%\n","Step 166\n","Loss & Accuracy: 0.045964017510414124,65.625%\n","Step 167\n","Loss & Accuracy: 0.04310818761587143,37.5%\n","Step 168\n","Loss & Accuracy: 0.049197591841220856,62.5%\n","Step 169\n","Loss & Accuracy: 0.04589434340596199,56.25%\n","Step 170\n","Loss & Accuracy: 0.04437584429979324,46.875%\n","Step 171\n","Loss & Accuracy: 0.04599682241678238,37.5%\n","Step 172\n","Loss & Accuracy: 0.041043758392333984,62.5%\n","Step 173\n","Loss & Accuracy: 0.04416234791278839,81.25%\n","Step 174\n","Loss & Accuracy: 0.04727403447031975,93.75%\n","Step 175\n","Loss & Accuracy: 0.04590003192424774,87.5%\n","Step 176\n","Loss & Accuracy: 0.04260557144880295,68.75%\n","Step 177\n","Loss & Accuracy: 0.04266844689846039,71.875%\n","Step 178\n","Loss & Accuracy: 0.045760560780763626,78.125%\n","Step 179\n","Loss & Accuracy: 0.0440945103764534,65.625%\n","Step 180\n","Loss & Accuracy: 0.047480881214141846,68.75%\n","Step 181\n","Loss & Accuracy: 0.04108980670571327,59.375%\n","Step 182\n","Loss & Accuracy: 0.04603441059589386,65.625%\n","Step 183\n","Loss & Accuracy: 0.04281305521726608,62.5%\n","Step 184\n","Loss & Accuracy: 0.04745340347290039,68.75%\n","Step 185\n","Loss & Accuracy: 0.04416770860552788,71.875%\n","Step 186\n","Loss & Accuracy: 0.047588638961315155,65.625%\n","Step 187\n","Loss & Accuracy: 0.04275203123688698,59.375%\n","Step 188\n","Loss & Accuracy: 0.04737842455506325,78.125%\n","Step 189\n","Loss & Accuracy: 0.04588492959737778,71.875%\n","Step 190\n","Loss & Accuracy: 0.044367142021656036,59.375%\n","Step 191\n","Loss & Accuracy: 0.04421515017747879,62.5%\n","Step 192\n","Loss & Accuracy: 0.047339245676994324,65.625%\n","Step 193\n","Loss & Accuracy: 0.04098151624202728,62.5%\n","Step 194\n","Loss & Accuracy: 0.04416542127728462,65.625%\n","Step 195\n","Loss & Accuracy: 0.05049694702029228,75.0%\n","Step 196\n","Loss & Accuracy: 0.04777367413043976,84.375%\n","Step 197\n","Loss & Accuracy: 0.045850276947021484,68.75%\n","Step 198\n","Loss & Accuracy: 0.04139584302902222,46.875%\n","Step 199\n","Loss & Accuracy: 0.044632211327552795,59.375%\n","Step 200\n","Loss & Accuracy: 0.03980644419789314,62.5%\n","Step 201\n","Loss & Accuracy: 0.04615076631307602,43.75%\n","Step 202\n","Loss & Accuracy: 0.04270751774311066,43.75%\n","Step 203\n","Loss & Accuracy: 0.047904834151268005,46.875%\n","Step 204\n","Loss & Accuracy: 0.04269934073090553,59.375%\n","Step 205\n","Loss & Accuracy: 0.04590650647878647,68.75%\n","Step 206\n","Loss & Accuracy: 0.041259925812482834,65.625%\n","Step 207\n","Loss & Accuracy: 0.04604000970721245,78.125%\n","Step 208\n","Loss & Accuracy: 0.04281095415353775,68.75%\n","Step 209\n","Loss & Accuracy: 0.04732591658830643,75.0%\n","Step 210\n","Loss & Accuracy: 0.04482964426279068,65.625%\n","Step 211\n","Loss & Accuracy: 0.04764815419912338,71.875%\n","Step 212\n","Loss & Accuracy: 0.04895980656147003,81.25%\n","Step 213\n","Loss & Accuracy: 0.03977722302079201,68.75%\n","Step 214\n","Loss & Accuracy: 0.047801874577999115,81.25%\n","Step 215\n","Loss & Accuracy: 0.04786380007863045,71.875%\n","Step 216\n","Loss & Accuracy: 0.047903575003147125,56.25%\n","Step 217\n","Loss & Accuracy: 0.04393157735466957,42.857142857142854%\n","Epoch 6/10\n","Step 1\n","Loss & Accuracy: 0.04904772713780403,71.875%\n","Step 2\n","Loss & Accuracy: 0.04839731380343437,78.125%\n","Step 3\n","Loss & Accuracy: 0.04739367961883545,68.75%\n","Step 4\n","Loss & Accuracy: 0.0460161492228508,75.0%\n","Step 5\n","Loss & Accuracy: 0.049308910965919495,84.375%\n","Step 6\n","Loss & Accuracy: 0.041378453373909,62.5%\n","Step 7\n","Loss & Accuracy: 0.04450631141662598,59.375%\n","Step 8\n","Loss & Accuracy: 0.04595455154776573,78.125%\n","Step 9\n","Loss & Accuracy: 0.04128306359052658,40.625%\n","Step 10\n","Loss & Accuracy: 0.04457923769950867,53.125%\n","Step 11\n","Loss & Accuracy: 0.041175637394189835,59.375%\n","Step 12\n","Loss & Accuracy: 0.04444974660873413,62.5%\n","Step 13\n","Loss & Accuracy: 0.04306257143616676,37.5%\n","Step 14\n","Loss & Accuracy: 0.047826871275901794,43.75%\n","Step 15\n","Loss & Accuracy: 0.04438408464193344,62.5%\n","Step 16\n","Loss & Accuracy: 0.0428008995950222,53.125%\n","Step 17\n","Loss & Accuracy: 0.04427191615104675,68.75%\n","Step 18\n","Loss & Accuracy: 0.049089644104242325,65.625%\n","Step 19\n","Loss & Accuracy: 0.0475965179502964,53.125%\n","Step 20\n","Loss & Accuracy: 0.04924927279353142,68.75%\n","Step 21\n","Loss & Accuracy: 0.047729797661304474,65.625%\n","Step 22\n","Loss & Accuracy: 0.049089670181274414,81.25%\n","Step 23\n","Loss & Accuracy: 0.04591026157140732,75.0%\n","Step 24\n","Loss & Accuracy: 0.046488113701343536,59.375%\n","Step 25\n","Loss & Accuracy: 0.04284293204545975,65.625%\n","Step 26\n","Loss & Accuracy: 0.04292558878660202,56.25%\n","Step 27\n","Loss & Accuracy: 0.04635750502347946,56.25%\n","Step 28\n","Loss & Accuracy: 0.048086944967508316,62.5%\n","Step 29\n","Loss & Accuracy: 0.04629680514335632,75.0%\n","Step 30\n","Loss & Accuracy: 0.04625058174133301,62.5%\n","Step 31\n","Loss & Accuracy: 0.0396927148103714,59.375%\n","Step 32\n","Loss & Accuracy: 0.047614458948373795,56.25%\n","Step 33\n","Loss & Accuracy: 0.043157629668712616,34.375%\n","Step 34\n","Loss & Accuracy: 0.046353209763765335,40.625%\n","Step 35\n","Loss & Accuracy: 0.04273663088679314,53.125%\n","Step 36\n","Loss & Accuracy: 0.05075196921825409,78.125%\n","Step 37\n","Loss & Accuracy: 0.041260186582803726,59.375%\n","Step 38\n","Loss & Accuracy: 0.04609084129333496,50.0%\n","Step 39\n","Loss & Accuracy: 0.0444006621837616,53.125%\n","Step 40\n","Loss & Accuracy: 0.04754086956381798,65.625%\n","Step 41\n","Loss & Accuracy: 0.04439578205347061,53.125%\n","Step 42\n","Loss & Accuracy: 0.04601459205150604,50.0%\n","Step 43\n","Loss & Accuracy: 0.04752451926469803,62.5%\n","Step 44\n","Loss & Accuracy: 0.04758693277835846,59.375%\n","Step 45\n","Loss & Accuracy: 0.04746061563491821,50.0%\n","Step 46\n","Loss & Accuracy: 0.044385384768247604,46.875%\n","Step 47\n","Loss & Accuracy: 0.04895653575658798,43.75%\n","Step 48\n","Loss & Accuracy: 0.04126957803964615,56.25%\n","Step 49\n","Loss & Accuracy: 0.04746058210730553,40.625%\n","Step 50\n","Loss & Accuracy: 0.04908653721213341,59.375%\n","Step 51\n","Loss & Accuracy: 0.044359996914863586,50.0%\n","Step 52\n","Loss & Accuracy: 0.042748428881168365,53.125%\n","Step 53\n","Loss & Accuracy: 0.04279063642024994,56.25%\n","Step 54\n","Loss & Accuracy: 0.04434511810541153,59.375%\n","Step 55\n","Loss & Accuracy: 0.047488175332546234,78.125%\n","Step 56\n","Loss & Accuracy: 0.047339532524347305,84.375%\n","Step 57\n","Loss & Accuracy: 0.04427003860473633,65.625%\n","Step 58\n","Loss & Accuracy: 0.04888245463371277,84.375%\n","Step 59\n","Loss & Accuracy: 0.03945852071046829,56.25%\n","Step 60\n","Loss & Accuracy: 0.04276154935359955,75.0%\n","Step 61\n","Loss & Accuracy: 0.044120997190475464,71.875%\n","Step 62\n","Loss & Accuracy: 0.044125426560640335,75.0%\n","Step 63\n","Loss & Accuracy: 0.04741876199841499,84.375%\n","Step 64\n","Loss & Accuracy: 0.04580321162939072,53.125%\n","Step 65\n","Loss & Accuracy: 0.04416222870349884,56.25%\n","Step 66\n","Loss & Accuracy: 0.04437454044818878,59.375%\n","Step 67\n","Loss & Accuracy: 0.044256169348955154,65.625%\n","Step 68\n","Loss & Accuracy: 0.050633009523153305,75.0%\n","Step 69\n","Loss & Accuracy: 0.04585351049900055,75.0%\n","Step 70\n","Loss & Accuracy: 0.04902590811252594,65.625%\n","Step 71\n","Loss & Accuracy: 0.04744602367281914,65.625%\n","Step 72\n","Loss & Accuracy: 0.045750558376312256,65.625%\n","Step 73\n","Loss & Accuracy: 0.04736347496509552,75.0%\n","Step 74\n","Loss & Accuracy: 0.04574732854962349,78.125%\n","Step 75\n","Loss & Accuracy: 0.04585902392864227,62.5%\n","Step 76\n","Loss & Accuracy: 0.04104163125157356,59.375%\n","Step 77\n","Loss & Accuracy: 0.04741903766989708,65.625%\n","Step 78\n","Loss & Accuracy: 0.04588004946708679,65.625%\n","Step 79\n","Loss & Accuracy: 0.044215939939022064,78.125%\n","Step 80\n","Loss & Accuracy: 0.04755265265703201,53.125%\n","Step 81\n","Loss & Accuracy: 0.042624685913324356,62.5%\n","Step 82\n","Loss & Accuracy: 0.04884566366672516,87.5%\n","Step 83\n","Loss & Accuracy: 0.048968084156513214,75.0%\n","Step 84\n","Loss & Accuracy: 0.04733135551214218,68.75%\n","Step 85\n","Loss & Accuracy: 0.04585641622543335,78.125%\n","Step 86\n","Loss & Accuracy: 0.04582267999649048,81.25%\n","Step 87\n","Loss & Accuracy: 0.04562986642122269,75.0%\n","Step 88\n","Loss & Accuracy: 0.04578639566898346,68.75%\n","Step 89\n","Loss & Accuracy: 0.04730718582868576,65.625%\n","Step 90\n","Loss & Accuracy: 0.04250074177980423,78.125%\n","Step 91\n","Loss & Accuracy: 0.05046335607767105,75.0%\n","Step 92\n","Loss & Accuracy: 0.04743944853544235,62.5%\n","Step 93\n","Loss & Accuracy: 0.04897884279489517,81.25%\n","Step 94\n","Loss & Accuracy: 0.04897107928991318,75.0%\n","Step 95\n","Loss & Accuracy: 0.04740393906831741,65.625%\n","Step 96\n","Loss & Accuracy: 0.047333285212516785,50.0%\n","Step 97\n","Loss & Accuracy: 0.04752384126186371,56.25%\n","Step 98\n","Loss & Accuracy: 0.04256867989897728,68.75%\n","Step 99\n","Loss & Accuracy: 0.04597483575344086,62.5%\n","Step 100\n","Loss & Accuracy: 0.04902487248182297,75.0%\n","Step 101\n","Loss & Accuracy: 0.04752129688858986,75.0%\n","Step 102\n","Loss & Accuracy: 0.041089896112680435,50.0%\n","Step 103\n","Loss & Accuracy: 0.048906102776527405,78.125%\n","Step 104\n","Loss & Accuracy: 0.04575049877166748,78.125%\n","Step 105\n","Loss & Accuracy: 0.04263761639595032,62.5%\n","Step 106\n","Loss & Accuracy: 0.042741406708955765,59.375%\n","Step 107\n","Loss & Accuracy: 0.04580347239971161,59.375%\n","Step 108\n","Loss & Accuracy: 0.04417974874377251,71.875%\n","Step 109\n","Loss & Accuracy: 0.04276276379823685,50.0%\n","Step 110\n","Loss & Accuracy: 0.04416137561202049,75.0%\n","Step 111\n","Loss & Accuracy: 0.05061915144324303,84.375%\n","Step 112\n","Loss & Accuracy: 0.04267023503780365,65.625%\n","Step 113\n","Loss & Accuracy: 0.047184400260448456,78.125%\n","Step 114\n","Loss & Accuracy: 0.04588569700717926,75.0%\n","Step 115\n","Loss & Accuracy: 0.04881221428513527,81.25%\n","Step 116\n","Loss & Accuracy: 0.045983873307704926,56.25%\n","Step 117\n","Loss & Accuracy: 0.04276076704263687,65.625%\n","Step 118\n","Loss & Accuracy: 0.047674909234046936,53.125%\n","Step 119\n","Loss & Accuracy: 0.042706310749053955,40.625%\n","Step 120\n","Loss & Accuracy: 0.044295527040958405,43.75%\n","Step 121\n","Loss & Accuracy: 0.04604071378707886,46.875%\n","Step 122\n","Loss & Accuracy: 0.044258102774620056,40.625%\n","Step 123\n","Loss & Accuracy: 0.04572148993611336,65.625%\n","Step 124\n","Loss & Accuracy: 0.042647585272789,56.25%\n","Step 125\n","Loss & Accuracy: 0.042781174182891846,56.25%\n","Step 126\n","Loss & Accuracy: 0.04730077087879181,75.0%\n","Step 127\n","Loss & Accuracy: 0.04405851662158966,75.0%\n","Step 128\n","Loss & Accuracy: 0.04906696081161499,62.5%\n","Step 129\n","Loss & Accuracy: 0.045645732432603836,75.0%\n","Step 130\n","Loss & Accuracy: 0.04576379805803299,90.625%\n","Step 131\n","Loss & Accuracy: 0.04420539736747742,78.125%\n","Step 132\n","Loss & Accuracy: 0.04741618037223816,81.25%\n","Step 133\n","Loss & Accuracy: 0.047243960201740265,81.25%\n","Step 134\n","Loss & Accuracy: 0.04100455343723297,62.5%\n","Step 135\n","Loss & Accuracy: 0.03934159129858017,68.75%\n","Step 136\n","Loss & Accuracy: 0.04725881665945053,78.125%\n","Step 137\n","Loss & Accuracy: 0.045749008655548096,78.125%\n","Step 138\n","Loss & Accuracy: 0.03953740745782852,65.625%\n","Step 139\n","Loss & Accuracy: 0.04731731116771698,84.375%\n","Step 140\n","Loss & Accuracy: 0.0442424975335598,71.875%\n","Step 141\n","Loss & Accuracy: 0.04748973995447159,65.625%\n","Step 142\n","Loss & Accuracy: 0.047463469207286835,68.75%\n","Step 143\n","Loss & Accuracy: 0.0474928617477417,75.0%\n","Step 144\n","Loss & Accuracy: 0.04912298917770386,81.25%\n","Step 145\n","Loss & Accuracy: 0.04914288967847824,87.5%\n","Step 146\n","Loss & Accuracy: 0.04746543988585472,75.0%\n","Step 147\n","Loss & Accuracy: 0.044116489589214325,78.125%\n","Step 148\n","Loss & Accuracy: 0.047401063144207,62.5%\n","Step 149\n","Loss & Accuracy: 0.042897991836071014,71.875%\n","Step 150\n","Loss & Accuracy: 0.04420271888375282,68.75%\n","Step 151\n","Loss & Accuracy: 0.04271280765533447,65.625%\n","Step 152\n","Loss & Accuracy: 0.04585118964314461,78.125%\n","Step 153\n","Loss & Accuracy: 0.04410979896783829,81.25%\n","Step 154\n","Loss & Accuracy: 0.048984743654727936,62.5%\n","Step 155\n","Loss & Accuracy: 0.04922354221343994,71.875%\n","Step 156\n","Loss & Accuracy: 0.045717958360910416,68.75%\n","Step 157\n","Loss & Accuracy: 0.04905063658952713,68.75%\n","Step 158\n","Loss & Accuracy: 0.04438110068440437,65.625%\n","Step 159\n","Loss & Accuracy: 0.04270465299487114,62.5%\n","Step 160\n","Loss & Accuracy: 0.041181858628988266,56.25%\n","Step 161\n","Loss & Accuracy: 0.048986420035362244,62.5%\n","Step 162\n","Loss & Accuracy: 0.04421422630548477,59.375%\n","Step 163\n","Loss & Accuracy: 0.044079847633838654,68.75%\n","Step 164\n","Loss & Accuracy: 0.045774463564157486,65.625%\n","Step 165\n","Loss & Accuracy: 0.045749764889478683,59.375%\n","Step 166\n","Loss & Accuracy: 0.04584958031773567,59.375%\n","Step 167\n","Loss & Accuracy: 0.04263082146644592,71.875%\n","Step 168\n","Loss & Accuracy: 0.04885203391313553,84.375%\n","Step 169\n","Loss & Accuracy: 0.045754071325063705,68.75%\n","Step 170\n","Loss & Accuracy: 0.04413535073399544,78.125%\n","Step 171\n","Loss & Accuracy: 0.04568008705973625,68.75%\n","Step 172\n","Loss & Accuracy: 0.04090740531682968,65.625%\n","Step 173\n","Loss & Accuracy: 0.04405948892235756,81.25%\n","Step 174\n","Loss & Accuracy: 0.047261640429496765,84.375%\n","Step 175\n","Loss & Accuracy: 0.045845672488212585,75.0%\n","Step 176\n","Loss & Accuracy: 0.04263614863157272,78.125%\n","Step 177\n","Loss & Accuracy: 0.04259076714515686,68.75%\n","Step 178\n","Loss & Accuracy: 0.04574674740433693,65.625%\n","Step 179\n","Loss & Accuracy: 0.04474903270602226,43.75%\n","Step 180\n","Loss & Accuracy: 0.04747750237584114,59.375%\n","Step 181\n","Loss & Accuracy: 0.0413815975189209,56.25%\n","Step 182\n","Loss & Accuracy: 0.046212099492549896,81.25%\n","Step 183\n","Loss & Accuracy: 0.04298856854438782,59.375%\n","Step 184\n","Loss & Accuracy: 0.04765993729233742,71.875%\n","Step 185\n","Loss & Accuracy: 0.04438866674900055,50.0%\n","Step 186\n","Loss & Accuracy: 0.047653067857027054,46.875%\n","Step 187\n","Loss & Accuracy: 0.042697422206401825,53.125%\n","Step 188\n","Loss & Accuracy: 0.04734857380390167,78.125%\n","Step 189\n","Loss & Accuracy: 0.04581829905509949,62.5%\n","Step 190\n","Loss & Accuracy: 0.04439500719308853,46.875%\n","Step 191\n","Loss & Accuracy: 0.04426316171884537,53.125%\n","Step 192\n","Loss & Accuracy: 0.047474205493927,56.25%\n","Step 193\n","Loss & Accuracy: 0.0410538949072361,46.875%\n","Step 194\n","Loss & Accuracy: 0.04420504719018936,59.375%\n","Step 195\n","Loss & Accuracy: 0.05080825462937355,43.75%\n","Step 196\n","Loss & Accuracy: 0.04748094081878662,40.625%\n","Step 197\n","Loss & Accuracy: 0.04603736847639084,65.625%\n","Step 198\n","Loss & Accuracy: 0.04129946976900101,46.875%\n","Step 199\n","Loss & Accuracy: 0.04447152838110924,53.125%\n","Step 200\n","Loss & Accuracy: 0.03980754315853119,40.625%\n","Step 201\n","Loss & Accuracy: 0.04590526223182678,59.375%\n","Step 202\n","Loss & Accuracy: 0.04264775663614273,56.25%\n","Step 203\n","Loss & Accuracy: 0.04811646044254303,59.375%\n","Step 204\n","Loss & Accuracy: 0.04259483516216278,62.5%\n","Step 205\n","Loss & Accuracy: 0.04596439749002457,68.75%\n","Step 206\n","Loss & Accuracy: 0.04127439856529236,50.0%\n","Step 207\n","Loss & Accuracy: 0.045844417065382004,71.875%\n","Step 208\n","Loss & Accuracy: 0.04271012917160988,40.625%\n","Step 209\n","Loss & Accuracy: 0.04756097495555878,21.875%\n","Step 210\n","Loss & Accuracy: 0.04430796205997467,25.0%\n","Step 211\n","Loss & Accuracy: 0.04752358794212341,37.5%\n","Step 212\n","Loss & Accuracy: 0.049037981778383255,59.375%\n","Step 213\n","Loss & Accuracy: 0.039618395268917084,56.25%\n","Step 214\n","Loss & Accuracy: 0.04749772697687149,75.0%\n","Step 215\n","Loss & Accuracy: 0.04757445678114891,50.0%\n","Step 216\n","Loss & Accuracy: 0.047620609402656555,40.625%\n","Step 217\n","Loss & Accuracy: 0.04384205862879753,42.857142857142854%\n","Epoch 7/10\n","Step 1\n","Loss & Accuracy: 0.049053456634283066,62.5%\n","Step 2\n","Loss & Accuracy: 0.04771273210644722,50.0%\n","Step 3\n","Loss & Accuracy: 0.047595664858818054,59.375%\n","Step 4\n","Loss & Accuracy: 0.04606761783361435,68.75%\n","Step 5\n","Loss & Accuracy: 0.048989422619342804,68.75%\n","Step 6\n","Loss & Accuracy: 0.041121989488601685,59.375%\n","Step 7\n","Loss & Accuracy: 0.044506240636110306,56.25%\n","Step 8\n","Loss & Accuracy: 0.04577876627445221,71.875%\n","Step 9\n","Loss & Accuracy: 0.041211556643247604,46.875%\n","Step 10\n","Loss & Accuracy: 0.04432651400566101,62.5%\n","Step 11\n","Loss & Accuracy: 0.041195690631866455,53.125%\n","Step 12\n","Loss & Accuracy: 0.044323865324258804,40.625%\n","Step 13\n","Loss & Accuracy: 0.04286269098520279,46.875%\n","Step 14\n","Loss & Accuracy: 0.04760920628905296,53.125%\n","Step 15\n","Loss & Accuracy: 0.04431803524494171,43.75%\n","Step 16\n","Loss & Accuracy: 0.042893245816230774,31.25%\n","Step 17\n","Loss & Accuracy: 0.044219862669706345,59.375%\n","Step 18\n","Loss & Accuracy: 0.04896089807152748,68.75%\n","Step 19\n","Loss & Accuracy: 0.04732159525156021,75.0%\n","Step 20\n","Loss & Accuracy: 0.04885852336883545,75.0%\n","Step 21\n","Loss & Accuracy: 0.04749029874801636,75.0%\n","Step 22\n","Loss & Accuracy: 0.0490129254758358,68.75%\n","Step 23\n","Loss & Accuracy: 0.04596337303519249,59.375%\n","Step 24\n","Loss & Accuracy: 0.04588828980922699,65.625%\n","Step 25\n","Loss & Accuracy: 0.042662229388952255,59.375%\n","Step 26\n","Loss & Accuracy: 0.042669206857681274,68.75%\n","Step 27\n","Loss & Accuracy: 0.045759208500385284,59.375%\n","Step 28\n","Loss & Accuracy: 0.047474488615989685,59.375%\n","Step 29\n","Loss & Accuracy: 0.04578252509236336,71.875%\n","Step 30\n","Loss & Accuracy: 0.04574283957481384,71.875%\n","Step 31\n","Loss & Accuracy: 0.03935752063989639,59.375%\n","Step 32\n","Loss & Accuracy: 0.04726073145866394,81.25%\n","Step 33\n","Loss & Accuracy: 0.04272029176354408,53.125%\n","Step 34\n","Loss & Accuracy: 0.045655086636543274,78.125%\n","Step 35\n","Loss & Accuracy: 0.04262553155422211,62.5%\n","Step 36\n","Loss & Accuracy: 0.05063115060329437,75.0%\n","Step 37\n","Loss & Accuracy: 0.040961384773254395,53.125%\n","Step 38\n","Loss & Accuracy: 0.04568780958652496,81.25%\n","Step 39\n","Loss & Accuracy: 0.044183142483234406,62.5%\n","Step 40\n","Loss & Accuracy: 0.04744890332221985,56.25%\n","Step 41\n","Loss & Accuracy: 0.04413691163063049,56.25%\n","Step 42\n","Loss & Accuracy: 0.04587792977690697,56.25%\n","Step 43\n","Loss & Accuracy: 0.04743967205286026,65.625%\n","Step 44\n","Loss & Accuracy: 0.04721435159444809,87.5%\n","Step 45\n","Loss & Accuracy: 0.04730794578790665,78.125%\n","Step 46\n","Loss & Accuracy: 0.04423448443412781,71.875%\n","Step 47\n","Loss & Accuracy: 0.048829082399606705,56.25%\n","Step 48\n","Loss & Accuracy: 0.04092929884791374,53.125%\n","Step 49\n","Loss & Accuracy: 0.04812820255756378,71.875%\n","Step 50\n","Loss & Accuracy: 0.04895627498626709,71.875%\n","Step 51\n","Loss & Accuracy: 0.04448886215686798,68.75%\n","Step 52\n","Loss & Accuracy: 0.04307794198393822,65.625%\n","Step 53\n","Loss & Accuracy: 0.04310118407011032,53.125%\n","Step 54\n","Loss & Accuracy: 0.044670455157756805,50.0%\n","Step 55\n","Loss & Accuracy: 0.047703102231025696,53.125%\n","Step 56\n","Loss & Accuracy: 0.04744855314493179,53.125%\n","Step 57\n","Loss & Accuracy: 0.04446471482515335,59.375%\n","Step 58\n","Loss & Accuracy: 0.04929470270872116,56.25%\n","Step 59\n","Loss & Accuracy: 0.03946097940206528,59.375%\n","Step 60\n","Loss & Accuracy: 0.042831093072891235,50.0%\n","Step 61\n","Loss & Accuracy: 0.04432741925120354,59.375%\n","Step 62\n","Loss & Accuracy: 0.04413866996765137,71.875%\n","Step 63\n","Loss & Accuracy: 0.047539226710796356,50.0%\n","Step 64\n","Loss & Accuracy: 0.04588020592927933,46.875%\n","Step 65\n","Loss & Accuracy: 0.04422612860798836,43.75%\n","Step 66\n","Loss & Accuracy: 0.04434995725750923,53.125%\n","Step 67\n","Loss & Accuracy: 0.04431154578924179,62.5%\n","Step 68\n","Loss & Accuracy: 0.05058300495147705,68.75%\n","Step 69\n","Loss & Accuracy: 0.045842189341783524,62.5%\n","Step 70\n","Loss & Accuracy: 0.04895541071891785,56.25%\n","Step 71\n","Loss & Accuracy: 0.04741201177239418,65.625%\n","Step 72\n","Loss & Accuracy: 0.04567377641797066,65.625%\n","Step 73\n","Loss & Accuracy: 0.04725160449743271,68.75%\n","Step 74\n","Loss & Accuracy: 0.04574527591466904,78.125%\n","Step 75\n","Loss & Accuracy: 0.04569154605269432,84.375%\n","Step 76\n","Loss & Accuracy: 0.04106041043996811,68.75%\n","Step 77\n","Loss & Accuracy: 0.04724859073758125,78.125%\n","Step 78\n","Loss & Accuracy: 0.0457075834274292,87.5%\n","Step 79\n","Loss & Accuracy: 0.04399437457323074,81.25%\n","Step 80\n","Loss & Accuracy: 0.04729882627725601,65.625%\n","Step 81\n","Loss & Accuracy: 0.04250601679086685,71.875%\n","Step 82\n","Loss & Accuracy: 0.04877074062824249,84.375%\n","Step 83\n","Loss & Accuracy: 0.048882558941841125,75.0%\n","Step 84\n","Loss & Accuracy: 0.047290630638599396,65.625%\n","Step 85\n","Loss & Accuracy: 0.04578635096549988,68.75%\n","Step 86\n","Loss & Accuracy: 0.045687057077884674,78.125%\n","Step 87\n","Loss & Accuracy: 0.045707572251558304,78.125%\n","Step 88\n","Loss & Accuracy: 0.04577293246984482,65.625%\n","Step 89\n","Loss & Accuracy: 0.047372184693813324,56.25%\n","Step 90\n","Loss & Accuracy: 0.042561404407024384,68.75%\n","Step 91\n","Loss & Accuracy: 0.050445958971977234,75.0%\n","Step 92\n","Loss & Accuracy: 0.047371573746204376,65.625%\n","Step 93\n","Loss & Accuracy: 0.04897518455982208,75.0%\n","Step 94\n","Loss & Accuracy: 0.04897187277674675,56.25%\n","Step 95\n","Loss & Accuracy: 0.04732251167297363,43.75%\n","Step 96\n","Loss & Accuracy: 0.047866467386484146,56.25%\n","Step 97\n","Loss & Accuracy: 0.04724440351128578,68.75%\n","Step 98\n","Loss & Accuracy: 0.042534202337265015,62.5%\n","Step 99\n","Loss & Accuracy: 0.04568028450012207,56.25%\n","Step 100\n","Loss & Accuracy: 0.04904596880078316,68.75%\n","Step 101\n","Loss & Accuracy: 0.04734592139720917,78.125%\n","Step 102\n","Loss & Accuracy: 0.0409320592880249,68.75%\n","Step 103\n","Loss & Accuracy: 0.04890921339392662,75.0%\n","Step 104\n","Loss & Accuracy: 0.046012621372938156,56.25%\n","Step 105\n","Loss & Accuracy: 0.042665839195251465,40.625%\n","Step 106\n","Loss & Accuracy: 0.0428355410695076,40.625%\n","Step 107\n","Loss & Accuracy: 0.04588644206523895,53.125%\n","Step 108\n","Loss & Accuracy: 0.04442645609378815,43.75%\n","Step 109\n","Loss & Accuracy: 0.04274827986955643,50.0%\n","Step 110\n","Loss & Accuracy: 0.044367775321006775,62.5%\n","Step 111\n","Loss & Accuracy: 0.050563275814056396,62.5%\n","Step 112\n","Loss & Accuracy: 0.042699337005615234,50.0%\n","Step 113\n","Loss & Accuracy: 0.04729754850268364,71.875%\n","Step 114\n","Loss & Accuracy: 0.045669689774513245,65.625%\n","Step 115\n","Loss & Accuracy: 0.04972822219133377,71.875%\n","Step 116\n","Loss & Accuracy: 0.04583153873682022,65.625%\n","Step 117\n","Loss & Accuracy: 0.04280383884906769,75.0%\n","Step 118\n","Loss & Accuracy: 0.04782239347696304,75.0%\n","Step 119\n","Loss & Accuracy: 0.043020520359277725,56.25%\n","Step 120\n","Loss & Accuracy: 0.04453451186418533,62.5%\n","Step 121\n","Loss & Accuracy: 0.045964233577251434,59.375%\n","Step 122\n","Loss & Accuracy: 0.044231414794921875,53.125%\n","Step 123\n","Loss & Accuracy: 0.04614808037877083,56.25%\n","Step 124\n","Loss & Accuracy: 0.042569421231746674,65.625%\n","Step 125\n","Loss & Accuracy: 0.04270464926958084,56.25%\n","Step 126\n","Loss & Accuracy: 0.04754211753606796,65.625%\n","Step 127\n","Loss & Accuracy: 0.044273220002651215,59.375%\n","Step 128\n","Loss & Accuracy: 0.04886166751384735,68.75%\n","Step 129\n","Loss & Accuracy: 0.045985639095306396,68.75%\n","Step 130\n","Loss & Accuracy: 0.04580092057585716,50.0%\n","Step 131\n","Loss & Accuracy: 0.04420884698629379,53.125%\n","Step 132\n","Loss & Accuracy: 0.047244928777217865,81.25%\n","Step 133\n","Loss & Accuracy: 0.04738197103142738,62.5%\n","Step 134\n","Loss & Accuracy: 0.0410916842520237,46.875%\n","Step 135\n","Loss & Accuracy: 0.03943284600973129,50.0%\n","Step 136\n","Loss & Accuracy: 0.04745901748538017,56.25%\n","Step 137\n","Loss & Accuracy: 0.04593268036842346,65.625%\n","Step 138\n","Loss & Accuracy: 0.03944368660449982,68.75%\n","Step 139\n","Loss & Accuracy: 0.04742538928985596,71.875%\n","Step 140\n","Loss & Accuracy: 0.04432470351457596,62.5%\n","Step 141\n","Loss & Accuracy: 0.047427739948034286,68.75%\n","Step 142\n","Loss & Accuracy: 0.04729907959699631,59.375%\n","Step 143\n","Loss & Accuracy: 0.04743286594748497,62.5%\n","Step 144\n","Loss & Accuracy: 0.04913117736577988,78.125%\n","Step 145\n","Loss & Accuracy: 0.04897954314947128,68.75%\n","Step 146\n","Loss & Accuracy: 0.04727330803871155,78.125%\n","Step 147\n","Loss & Accuracy: 0.04428217560052872,56.25%\n","Step 148\n","Loss & Accuracy: 0.04728120192885399,65.625%\n","Step 149\n","Loss & Accuracy: 0.04264672100543976,65.625%\n","Step 150\n","Loss & Accuracy: 0.04423394054174423,59.375%\n","Step 151\n","Loss & Accuracy: 0.042728714644908905,59.375%\n","Step 152\n","Loss & Accuracy: 0.04599688574671745,56.25%\n","Step 153\n","Loss & Accuracy: 0.04422060400247574,68.75%\n","Step 154\n","Loss & Accuracy: 0.04899989813566208,65.625%\n","Step 155\n","Loss & Accuracy: 0.049232885241508484,56.25%\n","Step 156\n","Loss & Accuracy: 0.045884571969509125,65.625%\n","Step 157\n","Loss & Accuracy: 0.049144335091114044,65.625%\n","Step 158\n","Loss & Accuracy: 0.04459527134895325,43.75%\n","Step 159\n","Loss & Accuracy: 0.04274633154273033,65.625%\n","Step 160\n","Loss & Accuracy: 0.04127097129821777,59.375%\n","Step 161\n","Loss & Accuracy: 0.04918891191482544,68.75%\n","Step 162\n","Loss & Accuracy: 0.04426755756139755,59.375%\n","Step 163\n","Loss & Accuracy: 0.044074129313230515,53.125%\n","Step 164\n","Loss & Accuracy: 0.04680084064602852,50.0%\n","Step 165\n","Loss & Accuracy: 0.04596802219748497,65.625%\n","Step 166\n","Loss & Accuracy: 0.046403612941503525,65.625%\n","Step 167\n","Loss & Accuracy: 0.04325730353593826,59.375%\n","Step 168\n","Loss & Accuracy: 0.04973042011260986,65.625%\n","Step 169\n","Loss & Accuracy: 0.046566177159547806,56.25%\n","Step 170\n","Loss & Accuracy: 0.045048631727695465,28.125%\n","Step 171\n","Loss & Accuracy: 0.04634678363800049,40.625%\n","Step 172\n","Loss & Accuracy: 0.0414581261575222,37.5%\n","Step 173\n","Loss & Accuracy: 0.044501934200525284,46.875%\n","Step 174\n","Loss & Accuracy: 0.04834916442632675,50.0%\n","Step 175\n","Loss & Accuracy: 0.04592008888721466,65.625%\n","Step 176\n","Loss & Accuracy: 0.04300667345523834,56.25%\n","Step 177\n","Loss & Accuracy: 0.04330114275217056,50.0%\n","Step 178\n","Loss & Accuracy: 0.04664865881204605,62.5%\n","Step 179\n","Loss & Accuracy: 0.04505874216556549,53.125%\n","Step 180\n","Loss & Accuracy: 0.04834892228245735,53.125%\n","Step 181\n","Loss & Accuracy: 0.041711997240781784,46.875%\n","Step 182\n","Loss & Accuracy: 0.04651163890957832,40.625%\n","Step 183\n","Loss & Accuracy: 0.04325060546398163,15.625%\n","Step 184\n","Loss & Accuracy: 0.04788342863321304,37.5%\n","Step 185\n","Loss & Accuracy: 0.044964805245399475,46.875%\n","Step 186\n","Loss & Accuracy: 0.04788178205490112,40.625%\n","Step 187\n","Loss & Accuracy: 0.043131668120622635,37.5%\n","Step 188\n","Loss & Accuracy: 0.04800310730934143,40.625%\n","Step 189\n","Loss & Accuracy: 0.046198394149541855,56.25%\n","Step 190\n","Loss & Accuracy: 0.04458431154489517,37.5%\n","Step 191\n","Loss & Accuracy: 0.04436319321393967,40.625%\n","Step 192\n","Loss & Accuracy: 0.04751896858215332,56.25%\n","Step 193\n","Loss & Accuracy: 0.04106494411826134,59.375%\n","Step 194\n","Loss & Accuracy: 0.04405662789940834,71.875%\n","Step 195\n","Loss & Accuracy: 0.05053333193063736,87.5%\n","Step 196\n","Loss & Accuracy: 0.04743749648332596,59.375%\n","Step 197\n","Loss & Accuracy: 0.04570027440786362,71.875%\n","Step 198\n","Loss & Accuracy: 0.04103422164916992,62.5%\n","Step 199\n","Loss & Accuracy: 0.04443666338920593,62.5%\n","Step 200\n","Loss & Accuracy: 0.039493680000305176,50.0%\n","Step 201\n","Loss & Accuracy: 0.04567907750606537,78.125%\n","Step 202\n","Loss & Accuracy: 0.042626988142728806,65.625%\n","Step 203\n","Loss & Accuracy: 0.047454044222831726,53.125%\n","Step 204\n","Loss & Accuracy: 0.04290049523115158,59.375%\n","Step 205\n","Loss & Accuracy: 0.04577008634805679,56.25%\n","Step 206\n","Loss & Accuracy: 0.04118942469358444,46.875%\n","Step 207\n","Loss & Accuracy: 0.04576989263296127,75.0%\n","Step 208\n","Loss & Accuracy: 0.04264077916741371,50.0%\n","Step 209\n","Loss & Accuracy: 0.04742734134197235,50.0%\n","Step 210\n","Loss & Accuracy: 0.04419364780187607,62.5%\n","Step 211\n","Loss & Accuracy: 0.047120071947574615,87.5%\n","Step 212\n","Loss & Accuracy: 0.04878624528646469,87.5%\n","Step 213\n","Loss & Accuracy: 0.039376962929964066,65.625%\n","Step 214\n","Loss & Accuracy: 0.04724012687802315,81.25%\n","Step 215\n","Loss & Accuracy: 0.04730622470378876,75.0%\n","Step 216\n","Loss & Accuracy: 0.0471939817070961,65.625%\n","Step 217\n","Loss & Accuracy: 0.04352416470646858,57.14285714285714%\n","Epoch 8/10\n","Step 1\n","Loss & Accuracy: 0.04881475865840912,43.75%\n","Step 2\n","Loss & Accuracy: 0.04756050184369087,50.0%\n","Step 3\n","Loss & Accuracy: 0.04732833802700043,59.375%\n","Step 4\n","Loss & Accuracy: 0.045779719948768616,71.875%\n","Step 5\n","Loss & Accuracy: 0.04905752092599869,71.875%\n","Step 6\n","Loss & Accuracy: 0.04120193049311638,62.5%\n","Step 7\n","Loss & Accuracy: 0.04433583468198776,65.625%\n","Step 8\n","Loss & Accuracy: 0.04587353765964508,62.5%\n","Step 9\n","Loss & Accuracy: 0.04135986417531967,40.625%\n","Step 10\n","Loss & Accuracy: 0.04424365609884262,62.5%\n","Step 11\n","Loss & Accuracy: 0.04095524549484253,71.875%\n","Step 12\n","Loss & Accuracy: 0.044228360056877136,56.25%\n","Step 13\n","Loss & Accuracy: 0.04281487315893173,40.625%\n","Step 14\n","Loss & Accuracy: 0.04778784513473511,43.75%\n","Step 15\n","Loss & Accuracy: 0.04421888291835785,62.5%\n","Step 16\n","Loss & Accuracy: 0.04271387308835983,56.25%\n","Step 17\n","Loss & Accuracy: 0.04429380223155022,65.625%\n","Step 18\n","Loss & Accuracy: 0.04911542311310768,68.75%\n","Step 19\n","Loss & Accuracy: 0.04770395904779434,53.125%\n","Step 20\n","Loss & Accuracy: 0.049133412539958954,62.5%\n","Step 21\n","Loss & Accuracy: 0.04777853935956955,68.75%\n","Step 22\n","Loss & Accuracy: 0.049368567764759064,56.25%\n","Step 23\n","Loss & Accuracy: 0.0462065115571022,65.625%\n","Step 24\n","Loss & Accuracy: 0.045990560203790665,50.0%\n","Step 25\n","Loss & Accuracy: 0.04275588318705559,56.25%\n","Step 26\n","Loss & Accuracy: 0.04264768585562706,59.375%\n","Step 27\n","Loss & Accuracy: 0.04610205814242363,62.5%\n","Step 28\n","Loss & Accuracy: 0.047505758702754974,71.875%\n","Step 29\n","Loss & Accuracy: 0.0458434596657753,81.25%\n","Step 30\n","Loss & Accuracy: 0.04574938863515854,81.25%\n","Step 31\n","Loss & Accuracy: 0.03949765861034393,62.5%\n","Step 32\n","Loss & Accuracy: 0.04728236049413681,59.375%\n","Step 33\n","Loss & Accuracy: 0.04277094453573227,62.5%\n","Step 34\n","Loss & Accuracy: 0.04566022753715515,53.125%\n","Step 35\n","Loss & Accuracy: 0.04254007339477539,65.625%\n","Step 36\n","Loss & Accuracy: 0.050421975553035736,75.0%\n","Step 37\n","Loss & Accuracy: 0.041173867881298065,56.25%\n","Step 38\n","Loss & Accuracy: 0.04573749005794525,84.375%\n","Step 39\n","Loss & Accuracy: 0.04410652071237564,84.375%\n","Step 40\n","Loss & Accuracy: 0.04739899933338165,75.0%\n","Step 41\n","Loss & Accuracy: 0.04417901486158371,84.375%\n","Step 42\n","Loss & Accuracy: 0.04576384648680687,62.5%\n","Step 43\n","Loss & Accuracy: 0.04723577946424484,84.375%\n","Step 44\n","Loss & Accuracy: 0.04716382175683975,75.0%\n","Step 45\n","Loss & Accuracy: 0.04728551208972931,65.625%\n","Step 46\n","Loss & Accuracy: 0.0441456213593483,75.0%\n","Step 47\n","Loss & Accuracy: 0.04897334799170494,75.0%\n","Step 48\n","Loss & Accuracy: 0.04102778434753418,75.0%\n","Step 49\n","Loss & Accuracy: 0.047326523810625076,68.75%\n","Step 50\n","Loss & Accuracy: 0.048753056675195694,84.375%\n","Step 51\n","Loss & Accuracy: 0.04428568854928017,68.75%\n","Step 52\n","Loss & Accuracy: 0.042539048939943314,68.75%\n","Step 53\n","Loss & Accuracy: 0.04252643138170242,71.875%\n","Step 54\n","Loss & Accuracy: 0.04410736262798309,62.5%\n","Step 55\n","Loss & Accuracy: 0.047246649861335754,65.625%\n","Step 56\n","Loss & Accuracy: 0.047196876257658005,81.25%\n","Step 57\n","Loss & Accuracy: 0.04405174404382706,81.25%\n","Step 58\n","Loss & Accuracy: 0.048739220947027206,81.25%\n","Step 59\n","Loss & Accuracy: 0.03939849138259888,59.375%\n","Step 60\n","Loss & Accuracy: 0.04271451383829117,65.625%\n","Step 61\n","Loss & Accuracy: 0.044131308794021606,53.125%\n","Step 62\n","Loss & Accuracy: 0.044133514165878296,75.0%\n","Step 63\n","Loss & Accuracy: 0.047409940510988235,65.625%\n","Step 64\n","Loss & Accuracy: 0.04579859972000122,62.5%\n","Step 65\n","Loss & Accuracy: 0.04421410709619522,65.625%\n","Step 66\n","Loss & Accuracy: 0.04410040006041527,71.875%\n","Step 67\n","Loss & Accuracy: 0.04399443045258522,71.875%\n","Step 68\n","Loss & Accuracy: 0.05084706097841263,71.875%\n","Step 69\n","Loss & Accuracy: 0.04563803970813751,78.125%\n","Step 70\n","Loss & Accuracy: 0.048966072499752045,75.0%\n","Step 71\n","Loss & Accuracy: 0.047547683119773865,62.5%\n","Step 72\n","Loss & Accuracy: 0.045886389911174774,68.75%\n","Step 73\n","Loss & Accuracy: 0.04724626615643501,78.125%\n","Step 74\n","Loss & Accuracy: 0.04595457762479782,84.375%\n","Step 75\n","Loss & Accuracy: 0.04558346047997475,78.125%\n","Step 76\n","Loss & Accuracy: 0.040936194360256195,71.875%\n","Step 77\n","Loss & Accuracy: 0.04725062847137451,78.125%\n","Step 78\n","Loss & Accuracy: 0.04557812958955765,78.125%\n","Step 79\n","Loss & Accuracy: 0.04403204470872879,68.75%\n","Step 80\n","Loss & Accuracy: 0.04724736511707306,59.375%\n","Step 81\n","Loss & Accuracy: 0.042611971497535706,62.5%\n","Step 82\n","Loss & Accuracy: 0.04887305945158005,75.0%\n","Step 83\n","Loss & Accuracy: 0.048899561166763306,71.875%\n","Step 84\n","Loss & Accuracy: 0.04731591045856476,62.5%\n","Step 85\n","Loss & Accuracy: 0.04568048566579819,65.625%\n","Step 86\n","Loss & Accuracy: 0.04593881964683533,68.75%\n","Step 87\n","Loss & Accuracy: 0.045659974217414856,71.875%\n","Step 88\n","Loss & Accuracy: 0.045912351459264755,56.25%\n","Step 89\n","Loss & Accuracy: 0.047442443668842316,65.625%\n","Step 90\n","Loss & Accuracy: 0.04254912585020065,68.75%\n","Step 91\n","Loss & Accuracy: 0.05042713135480881,68.75%\n","Step 92\n","Loss & Accuracy: 0.04749751836061478,59.375%\n","Step 93\n","Loss & Accuracy: 0.048851173371076584,71.875%\n","Step 94\n","Loss & Accuracy: 0.04890873283147812,87.5%\n","Step 95\n","Loss & Accuracy: 0.047210678458213806,78.125%\n","Step 96\n","Loss & Accuracy: 0.047203510999679565,78.125%\n","Step 97\n","Loss & Accuracy: 0.04758653789758682,87.5%\n","Step 98\n","Loss & Accuracy: 0.04245138168334961,81.25%\n","Step 99\n","Loss & Accuracy: 0.04592733830213547,78.125%\n","Step 100\n","Loss & Accuracy: 0.049315497279167175,87.5%\n","Step 101\n","Loss & Accuracy: 0.047724947333335876,84.375%\n","Step 102\n","Loss & Accuracy: 0.04130667448043823,71.875%\n","Step 103\n","Loss & Accuracy: 0.04921405762434006,84.375%\n","Step 104\n","Loss & Accuracy: 0.045796457678079605,84.375%\n","Step 105\n","Loss & Accuracy: 0.042539067566394806,59.375%\n","Step 106\n","Loss & Accuracy: 0.04305429756641388,59.375%\n","Step 107\n","Loss & Accuracy: 0.0456172451376915,75.0%\n","Step 108\n","Loss & Accuracy: 0.04417329281568527,87.5%\n","Step 109\n","Loss & Accuracy: 0.04270707070827484,65.625%\n","Step 110\n","Loss & Accuracy: 0.04416859894990921,71.875%\n","Step 111\n","Loss & Accuracy: 0.05044355243444443,81.25%\n","Step 112\n","Loss & Accuracy: 0.04271508380770683,75.0%\n","Step 113\n","Loss & Accuracy: 0.04731607437133789,65.625%\n","Step 114\n","Loss & Accuracy: 0.04569704458117485,71.875%\n","Step 115\n","Loss & Accuracy: 0.04883146286010742,81.25%\n","Step 116\n","Loss & Accuracy: 0.045637913048267365,71.875%\n","Step 117\n","Loss & Accuracy: 0.042624332010746,71.875%\n","Step 118\n","Loss & Accuracy: 0.04730071872472763,81.25%\n","Step 119\n","Loss & Accuracy: 0.042596474289894104,62.5%\n","Step 120\n","Loss & Accuracy: 0.044087961316108704,71.875%\n","Step 121\n","Loss & Accuracy: 0.04562988877296448,78.125%\n","Step 122\n","Loss & Accuracy: 0.04450575262308121,68.75%\n","Step 123\n","Loss & Accuracy: 0.04572761803865433,71.875%\n","Step 124\n","Loss & Accuracy: 0.042732566595077515,62.5%\n","Step 125\n","Loss & Accuracy: 0.042803406715393066,59.375%\n","Step 126\n","Loss & Accuracy: 0.047547414898872375,65.625%\n","Step 127\n","Loss & Accuracy: 0.04420856386423111,65.625%\n","Step 128\n","Loss & Accuracy: 0.04886568337678909,65.625%\n","Step 129\n","Loss & Accuracy: 0.04577886313199997,81.25%\n","Step 130\n","Loss & Accuracy: 0.0455751046538353,81.25%\n","Step 131\n","Loss & Accuracy: 0.04423631727695465,62.5%\n","Step 132\n","Loss & Accuracy: 0.047175951302051544,81.25%\n","Step 133\n","Loss & Accuracy: 0.04723426327109337,81.25%\n","Step 134\n","Loss & Accuracy: 0.040938906371593475,65.625%\n","Step 135\n","Loss & Accuracy: 0.03929852694272995,56.25%\n","Step 136\n","Loss & Accuracy: 0.04757723584771156,68.75%\n","Step 137\n","Loss & Accuracy: 0.04568277671933174,71.875%\n","Step 138\n","Loss & Accuracy: 0.039587412029504776,65.625%\n","Step 139\n","Loss & Accuracy: 0.0474497452378273,90.625%\n","Step 140\n","Loss & Accuracy: 0.04436810314655304,68.75%\n","Step 141\n","Loss & Accuracy: 0.047532591968774796,71.875%\n","Step 142\n","Loss & Accuracy: 0.04747090861201286,56.25%\n","Step 143\n","Loss & Accuracy: 0.04760932922363281,56.25%\n","Step 144\n","Loss & Accuracy: 0.04933468997478485,68.75%\n","Step 145\n","Loss & Accuracy: 0.048852331936359406,78.125%\n","Step 146\n","Loss & Accuracy: 0.047399360686540604,68.75%\n","Step 147\n","Loss & Accuracy: 0.044178396463394165,71.875%\n","Step 148\n","Loss & Accuracy: 0.047393567860126495,53.125%\n","Step 149\n","Loss & Accuracy: 0.042651399970054626,50.0%\n","Step 150\n","Loss & Accuracy: 0.04464346915483475,43.75%\n","Step 151\n","Loss & Accuracy: 0.04269011318683624,50.0%\n","Step 152\n","Loss & Accuracy: 0.04577682912349701,68.75%\n","Step 153\n","Loss & Accuracy: 0.04416295886039734,65.625%\n","Step 154\n","Loss & Accuracy: 0.04913448914885521,56.25%\n","Step 155\n","Loss & Accuracy: 0.04912829026579857,50.0%\n","Step 156\n","Loss & Accuracy: 0.045902639627456665,50.0%\n","Step 157\n","Loss & Accuracy: 0.0493537001311779,40.625%\n","Step 158\n","Loss & Accuracy: 0.04436502978205681,62.5%\n","Step 159\n","Loss & Accuracy: 0.042834073305130005,53.125%\n","Step 160\n","Loss & Accuracy: 0.041307754814624786,56.25%\n","Step 161\n","Loss & Accuracy: 0.049351468682289124,65.625%\n","Step 162\n","Loss & Accuracy: 0.04450060799717903,62.5%\n","Step 163\n","Loss & Accuracy: 0.04443786293268204,59.375%\n","Step 164\n","Loss & Accuracy: 0.04602982476353645,28.125%\n","Step 165\n","Loss & Accuracy: 0.0460067093372345,34.375%\n","Step 166\n","Loss & Accuracy: 0.04593610018491745,46.875%\n","Step 167\n","Loss & Accuracy: 0.04256391525268555,59.375%\n","Step 168\n","Loss & Accuracy: 0.048878610134124756,75.0%\n","Step 169\n","Loss & Accuracy: 0.04565266892313957,78.125%\n","Step 170\n","Loss & Accuracy: 0.04406782239675522,75.0%\n","Step 171\n","Loss & Accuracy: 0.04580765217542648,43.75%\n","Step 172\n","Loss & Accuracy: 0.04095616191625595,50.0%\n","Step 173\n","Loss & Accuracy: 0.04408787190914154,65.625%\n","Step 174\n","Loss & Accuracy: 0.047166530042886734,75.0%\n","Step 175\n","Loss & Accuracy: 0.045753054320812225,65.625%\n","Step 176\n","Loss & Accuracy: 0.04259594529867172,56.25%\n","Step 177\n","Loss & Accuracy: 0.04259694740176201,62.5%\n","Step 178\n","Loss & Accuracy: 0.04571288824081421,68.75%\n","Step 179\n","Loss & Accuracy: 0.04407777637243271,71.875%\n","Step 180\n","Loss & Accuracy: 0.04734514653682709,65.625%\n","Step 181\n","Loss & Accuracy: 0.04089405760169029,71.875%\n","Step 182\n","Loss & Accuracy: 0.04564914479851723,84.375%\n","Step 183\n","Loss & Accuracy: 0.04243047907948494,84.375%\n","Step 184\n","Loss & Accuracy: 0.04724540188908577,78.125%\n","Step 185\n","Loss & Accuracy: 0.04402775317430496,68.75%\n","Step 186\n","Loss & Accuracy: 0.04719604551792145,81.25%\n","Step 187\n","Loss & Accuracy: 0.04246184974908829,81.25%\n","Step 188\n","Loss & Accuracy: 0.04709919914603233,87.5%\n","Step 189\n","Loss & Accuracy: 0.04554242640733719,87.5%\n","Step 190\n","Loss & Accuracy: 0.04402082785964012,71.875%\n","Step 191\n","Loss & Accuracy: 0.04405893012881279,81.25%\n","Step 192\n","Loss & Accuracy: 0.047281377017498016,87.5%\n","Step 193\n","Loss & Accuracy: 0.040893733501434326,78.125%\n","Step 194\n","Loss & Accuracy: 0.043975308537483215,81.25%\n","Step 195\n","Loss & Accuracy: 0.05028345435857773,96.875%\n","Step 196\n","Loss & Accuracy: 0.04713749885559082,90.625%\n","Step 197\n","Loss & Accuracy: 0.04560040682554245,81.25%\n","Step 198\n","Loss & Accuracy: 0.040853001177310944,75.0%\n","Step 199\n","Loss & Accuracy: 0.04408988356590271,71.875%\n","Step 200\n","Loss & Accuracy: 0.039318159222602844,68.75%\n","Step 201\n","Loss & Accuracy: 0.04556998610496521,75.0%\n","Step 202\n","Loss & Accuracy: 0.04239386320114136,75.0%\n","Step 203\n","Loss & Accuracy: 0.04733135923743248,87.5%\n","Step 204\n","Loss & Accuracy: 0.04257449135184288,71.875%\n","Step 205\n","Loss & Accuracy: 0.04566190391778946,84.375%\n","Step 206\n","Loss & Accuracy: 0.0410085991024971,75.0%\n","Step 207\n","Loss & Accuracy: 0.04589950665831566,65.625%\n","Step 208\n","Loss & Accuracy: 0.04266735538840294,59.375%\n","Step 209\n","Loss & Accuracy: 0.04721950739622116,71.875%\n","Step 210\n","Loss & Accuracy: 0.04421205446124077,53.125%\n","Step 211\n","Loss & Accuracy: 0.047305941581726074,62.5%\n","Step 212\n","Loss & Accuracy: 0.04893677681684494,65.625%\n","Step 213\n","Loss & Accuracy: 0.03949093818664551,59.375%\n","Step 214\n","Loss & Accuracy: 0.047353681176900864,68.75%\n","Step 215\n","Loss & Accuracy: 0.04728176072239876,65.625%\n","Step 216\n","Loss & Accuracy: 0.047272708266973495,78.125%\n","Step 217\n","Loss & Accuracy: 0.04356151819229126,57.14285714285714%\n","Epoch 9/10\n","Step 1\n","Loss & Accuracy: 0.04879295825958252,78.125%\n","Step 2\n","Loss & Accuracy: 0.04731554538011551,65.625%\n","Step 3\n","Loss & Accuracy: 0.04739271104335785,71.875%\n","Step 4\n","Loss & Accuracy: 0.04579326510429382,81.25%\n","Step 5\n","Loss & Accuracy: 0.04872237890958786,87.5%\n","Step 6\n","Loss & Accuracy: 0.04096497595310211,62.5%\n","Step 7\n","Loss & Accuracy: 0.04416458308696747,68.75%\n","Step 8\n","Loss & Accuracy: 0.04567187279462814,68.75%\n","Step 9\n","Loss & Accuracy: 0.040991853922605515,50.0%\n","Step 10\n","Loss & Accuracy: 0.04410115256905556,59.375%\n","Step 11\n","Loss & Accuracy: 0.04091769829392433,59.375%\n","Step 12\n","Loss & Accuracy: 0.04410509020090103,78.125%\n","Step 13\n","Loss & Accuracy: 0.04256867617368698,71.875%\n","Step 14\n","Loss & Accuracy: 0.047302231192588806,75.0%\n","Step 15\n","Loss & Accuracy: 0.04430558905005455,37.5%\n","Step 16\n","Loss & Accuracy: 0.042701758444309235,31.25%\n","Step 17\n","Loss & Accuracy: 0.044148098677396774,37.5%\n","Step 18\n","Loss & Accuracy: 0.04971891641616821,37.5%\n","Step 19\n","Loss & Accuracy: 0.0474911630153656,68.75%\n","Step 20\n","Loss & Accuracy: 0.049410395324230194,68.75%\n","Step 21\n","Loss & Accuracy: 0.04802446812391281,68.75%\n","Step 22\n","Loss & Accuracy: 0.04964561015367508,78.125%\n","Step 23\n","Loss & Accuracy: 0.04651457816362381,59.375%\n","Step 24\n","Loss & Accuracy: 0.046407803893089294,43.75%\n","Step 25\n","Loss & Accuracy: 0.04298652336001396,56.25%\n","Step 26\n","Loss & Accuracy: 0.042912889271974564,59.375%\n","Step 27\n","Loss & Accuracy: 0.0460253469645977,46.875%\n","Step 28\n","Loss & Accuracy: 0.04783313721418381,62.5%\n","Step 29\n","Loss & Accuracy: 0.046204667538404465,75.0%\n","Step 30\n","Loss & Accuracy: 0.04586978256702423,81.25%\n","Step 31\n","Loss & Accuracy: 0.039936065673828125,65.625%\n","Step 32\n","Loss & Accuracy: 0.04817024618387222,56.25%\n","Step 33\n","Loss & Accuracy: 0.04353709518909454,59.375%\n","Step 34\n","Loss & Accuracy: 0.04673200100660324,53.125%\n","Step 35\n","Loss & Accuracy: 0.043328724801540375,46.875%\n","Step 36\n","Loss & Accuracy: 0.051165513694286346,68.75%\n","Step 37\n","Loss & Accuracy: 0.04137401282787323,46.875%\n","Step 38\n","Loss & Accuracy: 0.04629191756248474,46.875%\n","Step 39\n","Loss & Accuracy: 0.0446116141974926,43.75%\n","Step 40\n","Loss & Accuracy: 0.04765520244836807,62.5%\n","Step 41\n","Loss & Accuracy: 0.044260215014219284,59.375%\n","Step 42\n","Loss & Accuracy: 0.046068452298641205,50.0%\n","Step 43\n","Loss & Accuracy: 0.04792681708931923,28.125%\n","Step 44\n","Loss & Accuracy: 0.04784566909074783,31.25%\n","Step 45\n","Loss & Accuracy: 0.04775027930736542,34.375%\n","Step 46\n","Loss & Accuracy: 0.04438868910074234,40.625%\n","Step 47\n","Loss & Accuracy: 0.04924960806965828,34.375%\n","Step 48\n","Loss & Accuracy: 0.0412369929254055,59.375%\n","Step 49\n","Loss & Accuracy: 0.04744131863117218,65.625%\n","Step 50\n","Loss & Accuracy: 0.049233902245759964,62.5%\n","Step 51\n","Loss & Accuracy: 0.044382400810718536,59.375%\n","Step 52\n","Loss & Accuracy: 0.04318419098854065,40.625%\n","Step 53\n","Loss & Accuracy: 0.042722318321466446,56.25%\n","Step 54\n","Loss & Accuracy: 0.044226810336112976,75.0%\n","Step 55\n","Loss & Accuracy: 0.04738898575305939,65.625%\n","Step 56\n","Loss & Accuracy: 0.04720938205718994,84.375%\n","Step 57\n","Loss & Accuracy: 0.04434258118271828,62.5%\n","Step 58\n","Loss & Accuracy: 0.04893377795815468,81.25%\n","Step 59\n","Loss & Accuracy: 0.03942376747727394,65.625%\n","Step 60\n","Loss & Accuracy: 0.04246661067008972,75.0%\n","Step 61\n","Loss & Accuracy: 0.044227395206689835,71.875%\n","Step 62\n","Loss & Accuracy: 0.0441308468580246,75.0%\n","Step 63\n","Loss & Accuracy: 0.04747680574655533,84.375%\n","Step 64\n","Loss & Accuracy: 0.046184856444597244,50.0%\n","Step 65\n","Loss & Accuracy: 0.044509999454021454,50.0%\n","Step 66\n","Loss & Accuracy: 0.04440153017640114,53.125%\n","Step 67\n","Loss & Accuracy: 0.044311024248600006,46.875%\n","Step 68\n","Loss & Accuracy: 0.050449490547180176,59.375%\n","Step 69\n","Loss & Accuracy: 0.04622696340084076,37.5%\n","Step 70\n","Loss & Accuracy: 0.04885553941130638,68.75%\n","Step 71\n","Loss & Accuracy: 0.04738409072160721,71.875%\n","Step 72\n","Loss & Accuracy: 0.04589901119470596,59.375%\n","Step 73\n","Loss & Accuracy: 0.04739483445882797,62.5%\n","Step 74\n","Loss & Accuracy: 0.04563406854867935,75.0%\n","Step 75\n","Loss & Accuracy: 0.04588831216096878,53.125%\n","Step 76\n","Loss & Accuracy: 0.040993690490722656,75.0%\n","Step 77\n","Loss & Accuracy: 0.04738221317529678,87.5%\n","Step 78\n","Loss & Accuracy: 0.04593407362699509,78.125%\n","Step 79\n","Loss & Accuracy: 0.04439156502485275,71.875%\n","Step 80\n","Loss & Accuracy: 0.04748349264264107,75.0%\n","Step 81\n","Loss & Accuracy: 0.04280967265367508,40.625%\n","Step 82\n","Loss & Accuracy: 0.04909239709377289,46.875%\n","Step 83\n","Loss & Accuracy: 0.04906687140464783,62.5%\n","Step 84\n","Loss & Accuracy: 0.04738941416144371,56.25%\n","Step 85\n","Loss & Accuracy: 0.04577759653329849,68.75%\n","Step 86\n","Loss & Accuracy: 0.045825161039829254,81.25%\n","Step 87\n","Loss & Accuracy: 0.045863717794418335,78.125%\n","Step 88\n","Loss & Accuracy: 0.045917611569166183,59.375%\n","Step 89\n","Loss & Accuracy: 0.047485023736953735,68.75%\n","Step 90\n","Loss & Accuracy: 0.042587898671627045,56.25%\n","Step 91\n","Loss & Accuracy: 0.05036681890487671,87.5%\n","Step 92\n","Loss & Accuracy: 0.04731325805187225,65.625%\n","Step 93\n","Loss & Accuracy: 0.049011535942554474,75.0%\n","Step 94\n","Loss & Accuracy: 0.049025122076272964,75.0%\n","Step 95\n","Loss & Accuracy: 0.047495074570178986,50.0%\n","Step 96\n","Loss & Accuracy: 0.04781562089920044,43.75%\n","Step 97\n","Loss & Accuracy: 0.04776197299361229,31.25%\n","Step 98\n","Loss & Accuracy: 0.0429072268307209,25.0%\n","Step 99\n","Loss & Accuracy: 0.04610222578048706,37.5%\n","Step 100\n","Loss & Accuracy: 0.0490955114364624,56.25%\n","Step 101\n","Loss & Accuracy: 0.04737582057714462,53.125%\n","Step 102\n","Loss & Accuracy: 0.0409533828496933,65.625%\n","Step 103\n","Loss & Accuracy: 0.04891807585954666,75.0%\n","Step 104\n","Loss & Accuracy: 0.04580476135015488,59.375%\n","Step 105\n","Loss & Accuracy: 0.042621515691280365,59.375%\n","Step 106\n","Loss & Accuracy: 0.04256005585193634,59.375%\n","Step 107\n","Loss & Accuracy: 0.04576455056667328,62.5%\n","Step 108\n","Loss & Accuracy: 0.04408666491508484,56.25%\n","Step 109\n","Loss & Accuracy: 0.04257682338356972,53.125%\n","Step 110\n","Loss & Accuracy: 0.044086895883083344,65.625%\n","Step 111\n","Loss & Accuracy: 0.050362832844257355,78.125%\n","Step 112\n","Loss & Accuracy: 0.04253026098012924,68.75%\n","Step 113\n","Loss & Accuracy: 0.047201968729496,78.125%\n","Step 114\n","Loss & Accuracy: 0.04563102871179581,81.25%\n","Step 115\n","Loss & Accuracy: 0.04896215349435806,87.5%\n","Step 116\n","Loss & Accuracy: 0.045846909284591675,75.0%\n","Step 117\n","Loss & Accuracy: 0.04275759309530258,53.125%\n","Step 118\n","Loss & Accuracy: 0.047439444810152054,75.0%\n","Step 119\n","Loss & Accuracy: 0.042715370655059814,34.375%\n","Step 120\n","Loss & Accuracy: 0.044378943741321564,50.0%\n","Step 121\n","Loss & Accuracy: 0.04573672264814377,62.5%\n","Step 122\n","Loss & Accuracy: 0.044117048382759094,65.625%\n","Step 123\n","Loss & Accuracy: 0.04575011879205704,46.875%\n","Step 124\n","Loss & Accuracy: 0.042611442506313324,59.375%\n","Step 125\n","Loss & Accuracy: 0.042564623057842255,56.25%\n","Step 126\n","Loss & Accuracy: 0.04727725312113762,68.75%\n","Step 127\n","Loss & Accuracy: 0.04417992755770683,59.375%\n","Step 128\n","Loss & Accuracy: 0.04890786111354828,78.125%\n","Step 129\n","Loss & Accuracy: 0.04570598900318146,65.625%\n","Step 130\n","Loss & Accuracy: 0.0455554723739624,78.125%\n","Step 131\n","Loss & Accuracy: 0.04406479001045227,75.0%\n","Step 132\n","Loss & Accuracy: 0.04718868434429169,81.25%\n","Step 133\n","Loss & Accuracy: 0.047202058136463165,84.375%\n","Step 134\n","Loss & Accuracy: 0.04107644036412239,62.5%\n","Step 135\n","Loss & Accuracy: 0.03947824239730835,71.875%\n","Step 136\n","Loss & Accuracy: 0.04738099128007889,81.25%\n","Step 137\n","Loss & Accuracy: 0.04558707773685455,87.5%\n","Step 138\n","Loss & Accuracy: 0.039689939469099045,68.75%\n","Step 139\n","Loss & Accuracy: 0.04715896397829056,93.75%\n","Step 140\n","Loss & Accuracy: 0.044209375977516174,75.0%\n","Step 141\n","Loss & Accuracy: 0.047507431358098984,84.375%\n","Step 142\n","Loss & Accuracy: 0.047614432871341705,68.75%\n","Step 143\n","Loss & Accuracy: 0.04765757545828819,68.75%\n","Step 144\n","Loss & Accuracy: 0.04907422512769699,68.75%\n","Step 145\n","Loss & Accuracy: 0.04904722049832344,62.5%\n","Step 146\n","Loss & Accuracy: 0.047482553869485855,56.25%\n","Step 147\n","Loss & Accuracy: 0.04424334317445755,56.25%\n","Step 148\n","Loss & Accuracy: 0.04716592654585838,75.0%\n","Step 149\n","Loss & Accuracy: 0.04250229895114899,53.125%\n","Step 150\n","Loss & Accuracy: 0.04419195279479027,53.125%\n","Step 151\n","Loss & Accuracy: 0.04259846359491348,56.25%\n","Step 152\n","Loss & Accuracy: 0.045787252485752106,59.375%\n","Step 153\n","Loss & Accuracy: 0.044146694242954254,62.5%\n","Step 154\n","Loss & Accuracy: 0.048881664872169495,65.625%\n","Step 155\n","Loss & Accuracy: 0.04898840934038162,40.625%\n","Step 156\n","Loss & Accuracy: 0.04578635096549988,50.0%\n","Step 157\n","Loss & Accuracy: 0.049128107726573944,50.0%\n","Step 158\n","Loss & Accuracy: 0.04411301016807556,50.0%\n","Step 159\n","Loss & Accuracy: 0.04268252104520798,43.75%\n","Step 160\n","Loss & Accuracy: 0.0411456897854805,37.5%\n","Step 161\n","Loss & Accuracy: 0.04904547706246376,46.875%\n","Step 162\n","Loss & Accuracy: 0.04428981989622116,40.625%\n","Step 163\n","Loss & Accuracy: 0.0443706288933754,31.25%\n","Step 164\n","Loss & Accuracy: 0.04595594108104706,21.875%\n","Step 165\n","Loss & Accuracy: 0.04587393254041672,40.625%\n","Step 166\n","Loss & Accuracy: 0.045728933066129684,59.375%\n","Step 167\n","Loss & Accuracy: 0.04267328977584839,40.625%\n","Step 168\n","Loss & Accuracy: 0.04884862154722214,62.5%\n","Step 169\n","Loss & Accuracy: 0.045736104249954224,46.875%\n","Step 170\n","Loss & Accuracy: 0.04422019422054291,43.75%\n","Step 171\n","Loss & Accuracy: 0.04578884690999985,62.5%\n","Step 172\n","Loss & Accuracy: 0.04097007215023041,53.125%\n","Step 173\n","Loss & Accuracy: 0.044084616005420685,84.375%\n","Step 174\n","Loss & Accuracy: 0.04714886099100113,87.5%\n","Step 175\n","Loss & Accuracy: 0.045598436146974564,81.25%\n","Step 176\n","Loss & Accuracy: 0.04264940321445465,81.25%\n","Step 177\n","Loss & Accuracy: 0.04254947602748871,78.125%\n","Step 178\n","Loss & Accuracy: 0.045970335602760315,68.75%\n","Step 179\n","Loss & Accuracy: 0.044477906078100204,43.75%\n","Step 180\n","Loss & Accuracy: 0.04766333848237991,37.5%\n","Step 181\n","Loss & Accuracy: 0.04114626348018646,34.375%\n","Step 182\n","Loss & Accuracy: 0.04577752575278282,56.25%\n","Step 183\n","Loss & Accuracy: 0.0426512137055397,50.0%\n","Step 184\n","Loss & Accuracy: 0.047406286001205444,56.25%\n","Step 185\n","Loss & Accuracy: 0.04421978443861008,46.875%\n","Step 186\n","Loss & Accuracy: 0.04728949815034866,68.75%\n","Step 187\n","Loss & Accuracy: 0.04264020174741745,50.0%\n","Step 188\n","Loss & Accuracy: 0.04729059338569641,59.375%\n","Step 189\n","Loss & Accuracy: 0.04571113362908363,68.75%\n","Step 190\n","Loss & Accuracy: 0.04405192285776138,65.625%\n","Step 191\n","Loss & Accuracy: 0.04413160681724548,68.75%\n","Step 192\n","Loss & Accuracy: 0.04735058546066284,75.0%\n","Step 193\n","Loss & Accuracy: 0.04097316786646843,68.75%\n","Step 194\n","Loss & Accuracy: 0.0440860316157341,65.625%\n","Step 195\n","Loss & Accuracy: 0.05051639676094055,71.875%\n","Step 196\n","Loss & Accuracy: 0.04732322692871094,56.25%\n","Step 197\n","Loss & Accuracy: 0.04568915814161301,62.5%\n","Step 198\n","Loss & Accuracy: 0.041033096611499786,40.625%\n","Step 199\n","Loss & Accuracy: 0.04420499503612518,53.125%\n","Step 200\n","Loss & Accuracy: 0.03940454125404358,65.625%\n","Step 201\n","Loss & Accuracy: 0.045766595751047134,59.375%\n","Step 202\n","Loss & Accuracy: 0.04245300590991974,71.875%\n","Step 203\n","Loss & Accuracy: 0.04725855588912964,71.875%\n","Step 204\n","Loss & Accuracy: 0.042850054800510406,62.5%\n","Step 205\n","Loss & Accuracy: 0.046046435832977295,71.875%\n","Step 206\n","Loss & Accuracy: 0.041119035333395004,65.625%\n","Step 207\n","Loss & Accuracy: 0.046382203698158264,50.0%\n","Step 208\n","Loss & Accuracy: 0.04323511943221092,68.75%\n","Step 209\n","Loss & Accuracy: 0.04822532460093498,46.875%\n","Step 210\n","Loss & Accuracy: 0.04507485404610634,43.75%\n","Step 211\n","Loss & Accuracy: 0.048245761543512344,50.0%\n","Step 212\n","Loss & Accuracy: 0.049735210835933685,37.5%\n","Step 213\n","Loss & Accuracy: 0.03988063335418701,37.5%\n","Step 214\n","Loss & Accuracy: 0.04761802405118942,40.625%\n","Step 215\n","Loss & Accuracy: 0.047600023448467255,59.375%\n","Step 216\n","Loss & Accuracy: 0.04746924340724945,40.625%\n","Step 217\n","Loss & Accuracy: 0.043329816311597824,71.42857142857143%\n","Epoch 10/10\n","Step 1\n","Loss & Accuracy: 0.048955660313367844,56.25%\n","Step 2\n","Loss & Accuracy: 0.047435127198696136,65.625%\n","Step 3\n","Loss & Accuracy: 0.04733201116323471,62.5%\n","Step 4\n","Loss & Accuracy: 0.04567540064454079,78.125%\n","Step 5\n","Loss & Accuracy: 0.04896289110183716,78.125%\n","Step 6\n","Loss & Accuracy: 0.0410294272005558,53.125%\n","Step 7\n","Loss & Accuracy: 0.044242240488529205,68.75%\n","Step 8\n","Loss & Accuracy: 0.045948415994644165,40.625%\n","Step 9\n","Loss & Accuracy: 0.04111673682928085,46.875%\n","Step 10\n","Loss & Accuracy: 0.04415026307106018,40.625%\n","Step 11\n","Loss & Accuracy: 0.0410211905837059,46.875%\n","Step 12\n","Loss & Accuracy: 0.04407689347863197,59.375%\n","Step 13\n","Loss & Accuracy: 0.04254388064146042,62.5%\n","Step 14\n","Loss & Accuracy: 0.04721461236476898,71.875%\n","Step 15\n","Loss & Accuracy: 0.0441344678401947,62.5%\n","Step 16\n","Loss & Accuracy: 0.042581528425216675,43.75%\n","Step 17\n","Loss & Accuracy: 0.044129468500614166,53.125%\n","Step 18\n","Loss & Accuracy: 0.04877353087067604,68.75%\n","Step 19\n","Loss & Accuracy: 0.047218579798936844,78.125%\n","Step 20\n","Loss & Accuracy: 0.04881429672241211,78.125%\n","Step 21\n","Loss & Accuracy: 0.047284193336963654,93.75%\n","Step 22\n","Loss & Accuracy: 0.048954177647829056,90.625%\n","Step 23\n","Loss & Accuracy: 0.045810408890247345,78.125%\n","Step 24\n","Loss & Accuracy: 0.04564817249774933,81.25%\n","Step 25\n","Loss & Accuracy: 0.0425693541765213,46.875%\n","Step 26\n","Loss & Accuracy: 0.04249431937932968,65.625%\n","Step 27\n","Loss & Accuracy: 0.04569578170776367,71.875%\n","Step 28\n","Loss & Accuracy: 0.04717125743627548,75.0%\n","Step 29\n","Loss & Accuracy: 0.045643821358680725,78.125%\n","Step 30\n","Loss & Accuracy: 0.045692093670368195,71.875%\n","Step 31\n","Loss & Accuracy: 0.03942190855741501,68.75%\n","Step 32\n","Loss & Accuracy: 0.04732169210910797,68.75%\n","Step 33\n","Loss & Accuracy: 0.042644090950489044,59.375%\n","Step 34\n","Loss & Accuracy: 0.04559340700507164,62.5%\n","Step 35\n","Loss & Accuracy: 0.04274074733257294,62.5%\n","Step 36\n","Loss & Accuracy: 0.05056518688797951,65.625%\n","Step 37\n","Loss & Accuracy: 0.041063204407691956,59.375%\n","Step 38\n","Loss & Accuracy: 0.04579035937786102,81.25%\n","Step 39\n","Loss & Accuracy: 0.04417039453983307,65.625%\n","Step 40\n","Loss & Accuracy: 0.047303758561611176,59.375%\n","Step 41\n","Loss & Accuracy: 0.04468286782503128,65.625%\n","Step 42\n","Loss & Accuracy: 0.045745253562927246,68.75%\n","Step 43\n","Loss & Accuracy: 0.04762691259384155,68.75%\n","Step 44\n","Loss & Accuracy: 0.04776415228843689,65.625%\n","Step 45\n","Loss & Accuracy: 0.04775071144104004,53.125%\n","Step 46\n","Loss & Accuracy: 0.0444977693259716,50.0%\n","Step 47\n","Loss & Accuracy: 0.04918794333934784,43.75%\n","Step 48\n","Loss & Accuracy: 0.04124617204070091,43.75%\n","Step 49\n","Loss & Accuracy: 0.047342926263809204,50.0%\n","Step 50\n","Loss & Accuracy: 0.04883842170238495,62.5%\n","Step 51\n","Loss & Accuracy: 0.044135525822639465,75.0%\n","Step 52\n","Loss & Accuracy: 0.04264502972364426,65.625%\n","Step 53\n","Loss & Accuracy: 0.042441535741090775,62.5%\n","Step 54\n","Loss & Accuracy: 0.044047676026821136,68.75%\n","Step 55\n","Loss & Accuracy: 0.047202304005622864,62.5%\n","Step 56\n","Loss & Accuracy: 0.04732038080692291,81.25%\n","Step 57\n","Loss & Accuracy: 0.044156864285469055,65.625%\n","Step 58\n","Loss & Accuracy: 0.04889214411377907,65.625%\n","Step 59\n","Loss & Accuracy: 0.03949406370520592,40.625%\n","Step 60\n","Loss & Accuracy: 0.0425826795399189,59.375%\n","Step 61\n","Loss & Accuracy: 0.04413514584302902,43.75%\n","Step 62\n","Loss & Accuracy: 0.04452907666563988,59.375%\n","Step 63\n","Loss & Accuracy: 0.04727910831570625,75.0%\n","Step 64\n","Loss & Accuracy: 0.04582009091973305,65.625%\n","Step 65\n","Loss & Accuracy: 0.044307589530944824,65.625%\n","Step 66\n","Loss & Accuracy: 0.04421449080109596,81.25%\n","Step 67\n","Loss & Accuracy: 0.04420200735330582,40.625%\n","Step 68\n","Loss & Accuracy: 0.05033645033836365,53.125%\n","Step 69\n","Loss & Accuracy: 0.04582902044057846,78.125%\n","Step 70\n","Loss & Accuracy: 0.04876850172877312,90.625%\n","Step 71\n","Loss & Accuracy: 0.04738514497876167,87.5%\n","Step 72\n","Loss & Accuracy: 0.04589461535215378,90.625%\n","Step 73\n","Loss & Accuracy: 0.04742304980754852,87.5%\n","Step 74\n","Loss & Accuracy: 0.04580366611480713,84.375%\n","Step 75\n","Loss & Accuracy: 0.045737236738204956,68.75%\n","Step 76\n","Loss & Accuracy: 0.04101294279098511,71.875%\n","Step 77\n","Loss & Accuracy: 0.04735623300075531,81.25%\n","Step 78\n","Loss & Accuracy: 0.0456211157143116,84.375%\n","Step 79\n","Loss & Accuracy: 0.044155433773994446,87.5%\n","Step 80\n","Loss & Accuracy: 0.047366198152303696,81.25%\n","Step 81\n","Loss & Accuracy: 0.04255528748035431,62.5%\n","Step 82\n","Loss & Accuracy: 0.048810213804244995,68.75%\n","Step 83\n","Loss & Accuracy: 0.04884149134159088,75.0%\n","Step 84\n","Loss & Accuracy: 0.04714105650782585,78.125%\n","Step 85\n","Loss & Accuracy: 0.04560698941349983,84.375%\n","Step 86\n","Loss & Accuracy: 0.04567025601863861,84.375%\n","Step 87\n","Loss & Accuracy: 0.04559691250324249,68.75%\n","Step 88\n","Loss & Accuracy: 0.04556305706501007,68.75%\n","Step 89\n","Loss & Accuracy: 0.04712386429309845,71.875%\n","Step 90\n","Loss & Accuracy: 0.04248231649398804,62.5%\n","Step 91\n","Loss & Accuracy: 0.05034849792718887,90.625%\n","Step 92\n","Loss & Accuracy: 0.04727546125650406,68.75%\n","Step 93\n","Loss & Accuracy: 0.048769690096378326,78.125%\n","Step 94\n","Loss & Accuracy: 0.048798397183418274,81.25%\n","Step 95\n","Loss & Accuracy: 0.04736680909991264,62.5%\n","Step 96\n","Loss & Accuracy: 0.04724593088030815,71.875%\n","Step 97\n","Loss & Accuracy: 0.047281019389629364,65.625%\n","Step 98\n","Loss & Accuracy: 0.042698562145233154,59.375%\n","Step 99\n","Loss & Accuracy: 0.045787643641233444,40.625%\n","Step 100\n","Loss & Accuracy: 0.048838648945093155,75.0%\n","Step 101\n","Loss & Accuracy: 0.04725432023406029,65.625%\n","Step 102\n","Loss & Accuracy: 0.0408884659409523,59.375%\n","Step 103\n","Loss & Accuracy: 0.048820607364177704,68.75%\n","Step 104\n","Loss & Accuracy: 0.04561209678649902,78.125%\n","Step 105\n","Loss & Accuracy: 0.042572420090436935,59.375%\n","Step 106\n","Loss & Accuracy: 0.0426415354013443,50.0%\n","Step 107\n","Loss & Accuracy: 0.045764800161123276,37.5%\n","Step 108\n","Loss & Accuracy: 0.04421498253941536,31.25%\n","Step 109\n","Loss & Accuracy: 0.04287322610616684,37.5%\n","Step 110\n","Loss & Accuracy: 0.04423462599515915,53.125%\n","Step 111\n","Loss & Accuracy: 0.050526898354291916,68.75%\n","Step 112\n","Loss & Accuracy: 0.04276087135076523,46.875%\n","Step 113\n","Loss & Accuracy: 0.04735390469431877,59.375%\n","Step 114\n","Loss & Accuracy: 0.045791689306497574,34.375%\n","Step 115\n","Loss & Accuracy: 0.04909009486436844,46.875%\n","Step 116\n","Loss & Accuracy: 0.04587671905755997,43.75%\n","Step 117\n","Loss & Accuracy: 0.04258148372173309,62.5%\n","Step 118\n","Loss & Accuracy: 0.047369860112667084,75.0%\n","Step 119\n","Loss & Accuracy: 0.042502082884311676,75.0%\n","Step 120\n","Loss & Accuracy: 0.04415866360068321,68.75%\n","Step 121\n","Loss & Accuracy: 0.04575531184673309,71.875%\n","Step 122\n","Loss & Accuracy: 0.04413454234600067,56.25%\n","Step 123\n","Loss & Accuracy: 0.045688919723033905,56.25%\n","Step 124\n","Loss & Accuracy: 0.04253203794360161,56.25%\n","Step 125\n","Loss & Accuracy: 0.04265111684799194,46.875%\n","Step 126\n","Loss & Accuracy: 0.04742582142353058,53.125%\n","Step 127\n","Loss & Accuracy: 0.04424729570746422,65.625%\n","Step 128\n","Loss & Accuracy: 0.04907381907105446,50.0%\n","Step 129\n","Loss & Accuracy: 0.04575752839446068,71.875%\n","Step 130\n","Loss & Accuracy: 0.04557696729898453,78.125%\n","Step 131\n","Loss & Accuracy: 0.04416622593998909,43.75%\n","Step 132\n","Loss & Accuracy: 0.047404129058122635,65.625%\n","Step 133\n","Loss & Accuracy: 0.04724844917654991,78.125%\n","Step 134\n","Loss & Accuracy: 0.041105788201093674,50.0%\n","Step 135\n","Loss & Accuracy: 0.039614442735910416,50.0%\n","Step 136\n","Loss & Accuracy: 0.047561876475811005,59.375%\n","Step 137\n","Loss & Accuracy: 0.04590248316526413,53.125%\n","Step 138\n","Loss & Accuracy: 0.039364464581012726,65.625%\n","Step 139\n","Loss & Accuracy: 0.04769602417945862,62.5%\n","Step 140\n","Loss & Accuracy: 0.044142015278339386,59.375%\n","Step 141\n","Loss & Accuracy: 0.047293663024902344,59.375%\n","Step 142\n","Loss & Accuracy: 0.047224149107933044,59.375%\n","Step 143\n","Loss & Accuracy: 0.04722893238067627,65.625%\n","Step 144\n","Loss & Accuracy: 0.048780135810375214,65.625%\n","Step 145\n","Loss & Accuracy: 0.04909374564886093,71.875%\n","Step 146\n","Loss & Accuracy: 0.04727838933467865,71.875%\n","Step 147\n","Loss & Accuracy: 0.04424542933702469,68.75%\n","Step 148\n","Loss & Accuracy: 0.04742174223065376,78.125%\n","Step 149\n","Loss & Accuracy: 0.042603421956300735,53.125%\n","Step 150\n","Loss & Accuracy: 0.04411446675658226,46.875%\n","Step 151\n","Loss & Accuracy: 0.04302598536014557,50.0%\n","Step 152\n","Loss & Accuracy: 0.045680657029151917,59.375%\n","Step 153\n","Loss & Accuracy: 0.044083207845687866,71.875%\n","Step 154\n","Loss & Accuracy: 0.048921629786491394,56.25%\n","Step 155\n","Loss & Accuracy: 0.048909805715084076,59.375%\n","Step 156\n","Loss & Accuracy: 0.04580070823431015,53.125%\n","Step 157\n","Loss & Accuracy: 0.04893815517425537,46.875%\n","Step 158\n","Loss & Accuracy: 0.04420213773846626,46.875%\n","Step 159\n","Loss & Accuracy: 0.042627692222595215,65.625%\n","Step 160\n","Loss & Accuracy: 0.04107297956943512,59.375%\n","Step 161\n","Loss & Accuracy: 0.04897965490818024,62.5%\n","Step 162\n","Loss & Accuracy: 0.04411715269088745,62.5%\n","Step 163\n","Loss & Accuracy: 0.044029876589775085,59.375%\n","Step 164\n","Loss & Accuracy: 0.04564277455210686,40.625%\n","Step 165\n","Loss & Accuracy: 0.045647718012332916,68.75%\n","Step 166\n","Loss & Accuracy: 0.04564468562602997,62.5%\n","Step 167\n","Loss & Accuracy: 0.04262060299515724,50.0%\n","Step 168\n","Loss & Accuracy: 0.04883187264204025,81.25%\n","Step 169\n","Loss & Accuracy: 0.04565074294805527,68.75%\n","Step 170\n","Loss & Accuracy: 0.04415127635002136,50.0%\n","Step 171\n","Loss & Accuracy: 0.045639343559741974,68.75%\n","Step 172\n","Loss & Accuracy: 0.04100850597023964,46.875%\n","Step 173\n","Loss & Accuracy: 0.04399767518043518,68.75%\n","Step 174\n","Loss & Accuracy: 0.047250546514987946,75.0%\n","Step 175\n","Loss & Accuracy: 0.04559517651796341,81.25%\n","Step 176\n","Loss & Accuracy: 0.042538031935691833,56.25%\n","Step 177\n","Loss & Accuracy: 0.04252571612596512,71.875%\n","Step 178\n","Loss & Accuracy: 0.04571130871772766,71.875%\n","Step 179\n","Loss & Accuracy: 0.04414815455675125,68.75%\n","Step 180\n","Loss & Accuracy: 0.04723188653588295,59.375%\n","Step 181\n","Loss & Accuracy: 0.04124888777732849,62.5%\n","Step 182\n","Loss & Accuracy: 0.04562065377831459,84.375%\n","Step 183\n","Loss & Accuracy: 0.042611729353666306,71.875%\n","Step 184\n","Loss & Accuracy: 0.04744167625904083,68.75%\n","Step 185\n","Loss & Accuracy: 0.04418204724788666,68.75%\n","Step 186\n","Loss & Accuracy: 0.04735058918595314,71.875%\n","Step 187\n","Loss & Accuracy: 0.042567942291498184,59.375%\n","Step 188\n","Loss & Accuracy: 0.047281257808208466,68.75%\n","Step 189\n","Loss & Accuracy: 0.04574550688266754,65.625%\n","Step 190\n","Loss & Accuracy: 0.044330205768346786,50.0%\n","Step 191\n","Loss & Accuracy: 0.04435063526034355,40.625%\n","Step 192\n","Loss & Accuracy: 0.04750891029834747,56.25%\n","Step 193\n","Loss & Accuracy: 0.04118233919143677,43.75%\n","Step 194\n","Loss & Accuracy: 0.04422953724861145,53.125%\n","Step 195\n","Loss & Accuracy: 0.050651874393224716,50.0%\n","Step 196\n","Loss & Accuracy: 0.047493040561676025,62.5%\n","Step 197\n","Loss & Accuracy: 0.045770347118377686,53.125%\n","Step 198\n","Loss & Accuracy: 0.041208721697330475,34.375%\n","Step 199\n","Loss & Accuracy: 0.04430684074759483,50.0%\n","Step 200\n","Loss & Accuracy: 0.039368391036987305,59.375%\n","Step 201\n","Loss & Accuracy: 0.045876555144786835,50.0%\n","Step 202\n","Loss & Accuracy: 0.04255829006433487,50.0%\n","Step 203\n","Loss & Accuracy: 0.04743608087301254,53.125%\n","Step 204\n","Loss & Accuracy: 0.04270164668560028,31.25%\n","Step 205\n","Loss & Accuracy: 0.04635225236415863,46.875%\n","Step 206\n","Loss & Accuracy: 0.04112601280212402,46.875%\n","Step 207\n","Loss & Accuracy: 0.04633096605539322,25.0%\n","Step 208\n","Loss & Accuracy: 0.04310574755072594,59.375%\n","Step 209\n","Loss & Accuracy: 0.04809720069169998,53.125%\n","Step 210\n","Loss & Accuracy: 0.044906847178936005,56.25%\n","Step 211\n","Loss & Accuracy: 0.048070136457681656,56.25%\n","Step 212\n","Loss & Accuracy: 0.0494399219751358,59.375%\n","Step 213\n","Loss & Accuracy: 0.03973565995693207,50.0%\n","Step 214\n","Loss & Accuracy: 0.04743761941790581,71.875%\n","Step 215\n","Loss & Accuracy: 0.047524597495794296,65.625%\n","Step 216\n","Loss & Accuracy: 0.047355055809020996,75.0%\n","Step 217\n","Loss & Accuracy: 0.04331391677260399,71.42857142857143%\n","Training completed.\n"]}],"source":["import numpy as np\n","\n","n_epochs = 10\n","capsule_net.train(batches, n_epochs)"]},{"cell_type":"markdown","metadata":{"id":"QxaKCj3hTyg_"},"source":["### Sauvegarde du modèle pour une utilisation ultérieure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D1WMKOm7Tyg_","outputId":"16293a0f-1429-41a8-d53b-ec251bee6376"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=keras\n","c:\\Users\\Nwara\\Documents\\ModelNet40 - Princeton 3D Object Dataset\\.venv\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:107: UserWarning: You are saving a model that has not yet been built. It might not contain any weights yet. Consider building the model first by calling it on some data.\n","  return saving_lib.save_model(model, filepath)\n"]}],"source":["# Save the model in the new Keras format\n","capsule_net.save('model.keras', save_format='keras')"]},{"cell_type":"markdown","metadata":{"id":"0j94p6S1TyhA"},"source":["### Sauvegarde de la liste des pertes en cas de panne du noyau Jupyter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ST109eSDTyhB"},"outputs":[],"source":["import pickle\n","\n","# Save the variable to a file\n","with open('LossesAndAccuracies.pkl', 'wb') as f:\n","    pickle.dump(Losses, f)"]},{"cell_type":"markdown","metadata":{"id":"4B48kvJ9TyhB"},"source":["### Visualisation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5eyC_FggTyhC","outputId":"9caff2d5-e086-42f8-b184-112047a11011"},"outputs":[{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAA90AAAHqCAYAAAAZLi26AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACYIUlEQVR4nOzdd3hT5fvH8Xe6KZsyyt57b0GWysbBFFBZIogKgvDlqzgY4lcUQVHBgQKCskEQFZEKgoO9QZSNyAYZZZaO8/vj+SVQ2kKhaU+afF7XlasnJycnd56mTe4843ZYlmUhIiIiIiIiIm7nZ3cAIiIiIiIiIt5KSbeIiIiIiIhIKlHSLSIiIiIiIpJKlHSLiIiIiIiIpBIl3SIiIiIiIiKpREm3iIiIiIiISCpR0i0iIiIiIiKSSpR0i4iIiIiIiKQSJd0iIiIiIiIiqURJt4iImx08eBCHw8GYMWPsDkVERERSqEiRIjz44IN2hyHpmJJukTTwxRdf4HA42LBhg92heAVnUpvU5a233rI7RBERcZOPPvoIh8NB7dq17Q5FUkmRIkWSfE9v3ry53eGJpFiA3QGIiNytzp0707JlywT7q1atakM0IiKSGqZPn06RIkVYt24de/fupUSJEnaHJKmgSpUqDBo0KMH+fPny2RCNiHsp6RYRj3Tp0iUyZsx4y2OqVavGE088kUYRiYhIWjtw4ACrVq3i66+/5umnn2b69OkMGzbM7rASlZz3LV8VExNDXFwcQUFBSR6TP39+vaeL19LwchEPsnnzZlq0aEGWLFnIlCkTDzzwAGvWrIl3THR0NCNGjKBkyZKEhIQQFhZGvXr1iIiIcB1z/PhxevToQYECBQgODiZv3rw88sgjHDx48LYxLF++nPr165MxY0ayZcvGI488wp9//um6fd68eTgcDlauXJngvp9++ikOh4MdO3a49v3111+0b9+eHDlyEBISQo0aNVi0aFG8+zmH369cuZJnn32W3LlzU6BAgeQ22y0552EtXbqUKlWqEBISQrly5fj6668THLt//346dOhAjhw5CA0N5Z577uH7779PcNzVq1cZPnw4pUqVIiQkhLx589K2bVv27duX4NiJEydSvHhxgoODqVmzJuvXr493e0p+VyIi3m769Olkz56dVq1a0b59e6ZPn57ocefOneOFF16gSJEiBAcHU6BAAbp27crp06ddx9zuf/eKFStwOBysWLEi3rmdU5q++OIL177u3buTKVMm9u3bR8uWLcmcOTOPP/44AL/++isdOnSgUKFCBAcHU7BgQV544QWuXLmSIO6//vqLRx99lFy5cpEhQwZKly7NK6+8AsDPP/+Mw+FgwYIFCe43Y8YMHA4Hq1evvmX73e597cSJEwQEBDBixIgE9921axcOh4Px48fHa+cBAwZQsGBBgoODKVGiBG+//TZxcXEJ2mvMmDGMGzfO9R64c+fOW8aaHM52379/P82aNSNjxozky5eP119/Hcuy4h176dIlBg0a5Iq1dOnSjBkzJsFxAF999RW1atUiNDSU7Nmz06BBA5YuXZrguN9++41atWoREhJCsWLFmDZtWrzbk/MZTXyTerpFPMQff/xB/fr1yZIlC//9738JDAzk008/pVGjRqxcudI1l2348OGMGjWKp556ilq1ahEZGcmGDRvYtGkTTZo0AaBdu3b88ccf9OvXjyJFinDy5EkiIiI4dOgQRYoUSTKGn376iRYtWlCsWDGGDx/OlStX+PDDD7n33nvZtGkTRYoUoVWrVmTKlIk5c+bQsGHDePefPXs25cuXp0KFCq7ndO+995I/f35eeuklMmbMyJw5c2jdujXz58+nTZs28e7/7LPPkitXLoYOHcqlS5du22aXL1+O94HKKVu2bAQEXP/3tmfPHjp27EifPn3o1q0bU6ZMoUOHDixZssTVZidOnKBu3bpcvnyZ559/nrCwMKZOncrDDz/MvHnzXLHGxsby4IMPsmzZMjp16kT//v25cOECERER7Nixg+LFi7sed8aMGVy4cIGnn34ah8PB6NGjadu2Lfv37ycwMDBFvysREV8wffp02rZtS1BQEJ07d+bjjz9m/fr11KxZ03XMxYsXqV+/Pn/++SdPPvkk1apV4/Tp0yxatIjDhw+TM2fOO/rfnVwxMTE0a9aMevXqMWbMGEJDQwGYO3culy9f5plnniEsLIx169bx4YcfcvjwYebOneu6/7Zt26hfvz6BgYH07t2bIkWKsG/fPr799lv+97//0ahRIwoWLMj06dMTvF9Onz6d4sWLU6dOnSTjS877Wp48eWjYsCFz5sxJMIJg9uzZ+Pv706FDB8C85zZs2JAjR47w9NNPU6hQIVatWsWQIUM4duwY48aNi3f/KVOmcPXqVXr37k1wcDA5cuS4ZXtGR0cn+p6eMWNGMmTI4LoeGxtL8+bNueeeexg9ejRLlixh2LBhxMTE8PrrrwNgWRYPP/wwP//8Mz179qRKlSr8+OOPDB48mCNHjvDee++5zjdixAiGDx9O3bp1ef311wkKCmLt2rUsX76cpk2buo7bu3cv7du3p2fPnnTr1o3JkyfTvXt3qlevTvny5YHkfUYTH2WJSKqbMmWKBVjr169P8pjWrVtbQUFB1r59+1z7jh49amXOnNlq0KCBa1/lypWtVq1aJXmes2fPWoD1zjvv3HGcVapUsXLnzm39+++/rn1bt261/Pz8rK5du7r2de7c2cqdO7cVExPj2nfs2DHLz8/Pev311137HnjgAatixYrW1atXXfvi4uKsunXrWiVLlnTtc7ZPvXr14p0zKQcOHLCAJC+rV692HVu4cGELsObPn+/ad/78eStv3rxW1apVXfsGDBhgAdavv/7q2nfhwgWraNGiVpEiRazY2FjLsixr8uTJFmC9++67CeKKi4uLF19YWJh15swZ1+3ffPONBVjffvutZVkp+12JiHi7DRs2WIAVERFhWZb5H1ugQAGrf//+8Y4bOnSoBVhff/11gnM4/y8n53/3zz//bAHWzz//HO925//0KVOmuPZ169bNAqyXXnopwfkuX76cYN+oUaMsh8Nh/f333659DRo0sDJnzhxv343xWJZlDRkyxAoODrbOnTvn2nfy5EkrICDAGjZsWILHuVFy39c+/fRTC7C2b98e7/7lypWz7r//ftf1kSNHWhkzZrR2794d77iXXnrJ8vf3tw4dOmRZ1vX2ypIli3Xy5MlbxujkfK9O7DJq1CjXcc5279evn2tfXFyc1apVKysoKMg6deqUZVmWtXDhQguw3njjjXiP0759e8vhcFh79+61LMuy9uzZY/n5+Vlt2rRxtceN5705vl9++cW17+TJk1ZwcLA1aNAg177bfUYT36Xh5SIeIDY2lqVLl9K6dWuKFSvm2p83b14ee+wxfvvtNyIjIwHTi/vHH3+wZ8+eRM+VIUMGgoKCWLFiBWfPnk12DMeOHWPLli1079493rfRlSpVokmTJixevNi1r2PHjpw8eTLeELx58+YRFxdHx44dAThz5gzLly/n0Ucf5cKFC5w+fZrTp0/z77//0qxZM/bs2cORI0fixdCrVy/8/f2THXPv3r2JiIhIcClXrly84/LlyxevlyBLlix07dqVzZs3c/z4cQAWL15MrVq1qFevnuu4TJky0bt3bw4ePOgaFjd//nxy5sxJv379EsTjcDjiXe/YsSPZs2d3Xa9fvz5ghvvB3f+uRER8wfTp08mTJw/33XcfYP7HduzYkVmzZhEbG+s6bv78+VSuXDlBb7DzPs5jkvu/+04888wzCfbd2Ct76dIlTp8+Td26dbEsi82bNwNw6tQpfvnlF5588kkKFSqUZDxdu3YlKiqKefPmufbNnj2bmJiY285/Tu77Wtu2bQkICGD27Nmu43bs2MHOnTtd7+lgevDr169P9uzZXe/pp0+fpnHjxsTGxvLLL7/Ee/x27dqRK1euW8Z4o9q1ayf6nt65c+cEx/bt29e17XA46Nu3L9euXeOnn35yPXd/f3+ef/75ePcbNGgQlmXxww8/ALBw4ULi4uIYOnQofn7x06KbXxflypVzvY8D5MqVi9KlS7ve0+H2n9HEdynpFvEAp06d4vLly5QuXTrBbWXLliUuLo5//vkHgNdff51z585RqlQpKlasyODBg9m2bZvr+ODgYN5++21++OEH8uTJQ4MGDRg9erQruUzK33//DZBkDKdPn3YN+W7evDlZs2aN9wY9e/ZsqlSpQqlSpQAzDMuyLF577TVy5coV7+Icwnby5Ml4j1O0aNHbttWNSpYsSePGjRNcsmTJEu+4EiVKJHjzdMbpnDv9999/J/ncnbcD7Nu3j9KlS8cbvp6Umz9IORNwZ4J9t78rERFvFxsby6xZs7jvvvs4cOAAe/fuZe/evdSuXZsTJ06wbNky17H79u1zTWtKyp38706ugICARNcfOXTokOsL7EyZMpErVy7XdKzz588D1798vV3cZcqUoWbNmvHmsk+fPp177rnntqu4J/d9LWfOnDzwwAPMmTPHdczs2bMJCAigbdu2rn179uxhyZIlCd7TGzduDKT8PT1nzpyJvqcXLlw43nF+fn7xOigg8ff0fPnykTlz5ls+93379uHn55fgy/rE3PyeDuZ9/cYvzW/3GU18l5JukXSmQYMG7Nu3j8mTJ1OhQgU+//xzqlWrxueff+46ZsCAAezevZtRo0YREhLCa6+9RtmyZV3fsKdUcHAwrVu3ZsGCBcTExHDkyBF+//33eN+IOxdV+c9//pPoN9cREREJPjDc2DvgDZLqtbduWMQltX9XIiLp0fLlyzl27BizZs2iZMmSrsujjz4KkOSCaimRVI/3jb3qNwoODk7QOxobG0uTJk34/vvvefHFF1m4cCERERGuRdhuXHAsubp27crKlSs5fPgw+/btY82aNW5f5btTp07s3r2bLVu2ADBnzhweeOABcubM6TomLi6OJk2aJPme3q5du3jn9MX39OR8RhPfpIXURDxArly5CA0NZdeuXQlu++uvv/Dz86NgwYKufTly5KBHjx706NGDixcv0qBBA4YPH85TTz3lOqZ48eIMGjSIQYMGsWfPHqpUqcLYsWP56quvEo3B+U1yUjHkzJkzXimUjh07MnXqVJYtW8aff/6JZVnxkm7nt9CBgYGub8Ht4ux1v/ED1e7duwFci5UVLlw4yefuvB1Mu65du5bo6GjXYmgpdae/KxERbzd9+nRy587NhAkTEtz29ddfs2DBAj755BMyZMhA8eLF41XNSExy/nc7RyOdO3cu3n5nr2hybN++nd27dzN16lS6du3q2n/z6tXO98jbxQ0mIR44cCAzZ87kypUrBAYGxnu/TUpy39cAWrduzdNPP+0awbZ7926GDBkS737Fixfn4sWLtr+nx8XFsX//flfvNiT+nv7TTz9x4cKFeL3dib2nx8XFsXPnTqpUqeKW+JLzGU18j3q6RTyAv78/TZs25ZtvvolXKurEiRPMmDGDevXquYZM//vvv/HumylTJkqUKEFUVBRgVhe9evVqvGOKFy9O5syZXcckJm/evFSpUoWpU6fG+8CxY8cOli5dSsuWLeMd37hxY3LkyMHs2bOZPXs2tWrVijeULHfu3DRq1IhPP/2UY8eOJXi8U6dO3bpR3Ojo0aPxSq5ERkYybdo0qlSpQnh4OAAtW7Zk3bp18cqvXLp0iYkTJ1KkSBHX0LN27dpx+vTpeCVUnKxEypDcyt3+rkREvNmVK1f4+uuvefDBB2nfvn2CS9++fblw4YKr/GS7du3YunVroqW1nP+Xk/O/u3Dhwvj7+yeYm/zRRx8lO3Znb+iN7weWZfH+++/HOy5Xrlw0aNCAyZMnc+jQoUTjccqZMyctWrTgq6++Yvr06TRv3jxeD3RSkvu+BmYucrNmzZgzZw6zZs0iKCiI1q1bxzvfo48+yurVq/nxxx8TPNa5c+eIiYm5bUzucuPv0bIsxo8fT2BgIA888ABgnntsbGyC3/d7772Hw+GgRYsWgPmywc/Pj9dffz3BKIQ7fU+H239GE9+lnm6RNDR58mSWLFmSYH///v154403iIiIoF69ejz77LMEBATw6aefEhUVxejRo13HlitXjkaNGlG9enVy5MjBhg0bmDdvnmtRkd27d/PAAw/w6KOPUq5cOQICAliwYAEnTpygU6dOt4zvnXfeoUWLFtSpU4eePXu6SoZlzZqV4cOHxzs2MDCQtm3bMmvWLC5dusSYMWMSnG/ChAnUq1ePihUr0qtXL4oVK8aJEydYvXo1hw8fZuvWrXfRitdt2rQp0d7gm8uolCpVip49e7J+/Xry5MnD5MmTOXHiBFOmTHEd89JLLzFz5kxatGjB888/T44cOZg6dSoHDhxg/vz5riGEXbt2Zdq0aQwcOJB169ZRv359Ll26xE8//cSzzz7LI488kuz4U/K7EhHxVosWLeLChQs8/PDDid5+zz33kCtXLqZPn07Hjh0ZPHgw8+bNo0OHDjz55JNUr16dM2fOsGjRIj755BMqV66crP/dWbNmpUOHDnz44Yc4HA6KFy/Od999l2Cu8q2UKVOG4sWL85///IcjR46QJUsW5s+fn+himR988AH16tWjWrVq9O7dm6JFi3Lw4EG+//571zBvp65du9K+fXsARo4cmaxYkvu+5tSxY0eeeOIJPvroI5o1a0a2bNni3T548GAWLVrEgw8+6CqVdenSJbZv3868efM4ePBgsr4MSMqRI0cSfU/PlClTvC8AQkJCWLJkCd26daN27dr88MMPfP/997z88suuhdseeugh7rvvPl555RUOHjxI5cqVWbp0Kd988w0DBgxwlYgrUaIEr7zyCiNHjqR+/fq0bduW4OBg1q9fT758+Rg1atQdPYfbfUYTH2bDiukiPsdZEiupyz///GNZlmVt2rTJatasmZUpUyYrNDTUuu+++6xVq1bFO9cbb7xh1apVy8qWLZuVIUMGq0yZMtb//vc/69q1a5ZlWdbp06et5557zipTpoyVMWNGK2vWrFbt2rWtOXPmJCvWn376ybr33nutDBkyWFmyZLEeeugha+fOnYkeGxERYQGWw+FwPYeb7du3z+ratasVHh5uBQYGWvnz57cefPBBa968eQna51Yl1W50u5Jh3bp1cx1buHBhq1WrVtaPP/5oVapUyQoODrbKlCljzZ07N9FY27dvb2XLls0KCQmxatWqZX333XcJjrt8+bL1yiuvWEWLFrUCAwOt8PBwq3379q5yb874EisFBrjKvKT0dyUi4o0eeughKyQkxLp06VKSx3Tv3t0KDAy0Tp8+bVmWZf37779W3759rfz581tBQUFWgQIFrG7durlut6zb/++2LMs6deqU1a5dOys0NNTKnj279fTTT1s7duxItGRYxowZE41t586dVuPGja1MmTJZOXPmtHr16mVt3bo1wTksy7J27NhhtWnTxvW+U7p0aeu1115LcM6oqCgre/bsVtasWa0rV64kpxkty0r++5plWVZkZKSVIUMGC7C++uqrRI+5cOGCNWTIEKtEiRJWUFCQlTNnTqtu3brWmDFjXJ9DbvUemJRblQwrXLiw6zhnu+/bt89q2rSpFRoaauXJk8caNmxYgpJfFy5csF544QUrX758VmBgoFWyZEnrnXfeiVcKzGny5MlW1apVreDgYCt79uxWw4YNXaXqnPElVgqsYcOGVsOGDV3Xb/cZTXyXw7LuYuyEiEg6UaRIESpUqMB3331ndygiIiJ3JSYmhnz58vHQQw8xadIku8OxTffu3Zk3bx4XL160OxSRO6I53SIiIiIiHmzhwoWcOnUq3uJsIpJ+aE63iIiIiIgHWrt2Ldu2bWPkyJFUrVrVVe9bRNIX9XSLiIiIiHigjz/+mGeeeYbcuXMzbdo0u8MRkbukOd0iIiIiIiIiqUQ93SIiIiIiIiKpREm3iIiIiIiISCrRQmp3KS4ujqNHj5I5c2YcDofd4YiIiA+yLIsLFy6QL18+/Px853t0vQeLiIgnSO77sJLuu3T06FEKFixodxgiIiL8888/FChQwO4w0ozeg0VExJPc7n1YSfddypw5M2AaOEuWLCk6V3R0NEuXLqVp06YEBga6IzyfpzZ1L7Wn+6lN3c8X2zQyMpKCBQu63pN8hd6DPZva1P3Upu6nNnU/X2zT5L4PK+m+S87hbFmyZHHLG35oaChZsmTxmRdoalObupfa0/3Upu7ny23qa0Os9R7s2dSm7qc2dT+1qfv5cpve7n3YdyaAiYiIiIiIiKQxJd0iIiIiIiIiqURJt4iIiIiIiEgq0ZxuERFJM7GxsURHR6fKuaOjowkICODq1avExsamymPYISgoyKfKgblTcl5v3vq6sVNK2zQwMBB/f/9UiExExB5KukVEJNVZlsXx48c5d+5cqj5GeHg4//zzj1ctLObn50fRokUJCgqyO5R0405eb976urGTO9o0W7ZshIeH63ciIl5BSbeIiKQ6ZwKUO3duQkNDU+WDdFxcHBcvXiRTpkxe0zMcFxfH0aNHOXbsGIUKFVICkkx38nrzxteN3VLSppZlcfnyZU6ePAlA3rx5UyNEEZE0paRbRERSVWxsrCsBCgsLS7XHiYuL49q1a4SEhHhV8pQrVy6OHj1KTEyMz5VguRt3+nrz1teNnVLaphkyZADg5MmT5M6dW0PNRSTd07uLiIikKuec2tDQUJsjSZ+cw8o13zh59HrzDs7fX2qtASEikpaUdIuISJrQ0Oi7o3a7O2q39E2/PxHxJkq6RURERERERFKJkm4RERERERGRVKKkW0REJAndu3endevWdochPmb16tX4+/vTqlUru0MRERE3UNItIiIi4kEmTZpEv379+OWXXzh69KhtcVy7ds22xxYR8SZKukVERO7CypUrqVWrFsHBweTNm5eXXnqJmJgY1+3z5s2jYsWKZMiQgbCwMBo3bsylS5cAWLFiBbVq1SJjxoxky5aNe++9l7///tuupyIe5OLFi8yePZtnnnmGVq1a8cUXX8S7/dtvv6VmzZqEhISQM2dO2rRp47otKiqKF198kYIFCxIcHEyJEiWYNGkSAF988QXZsmWLd66FCxfGW7Bs+PDhVKlShc8//5yiRYsSEhICwJIlS6hXrx7ZsmUjLCyMBx98kH379sU71+HDh+ncuTM5cuQgc+bM3Hfffaxdu5aDBw/i5+fHhg0b4h0/btw4ChcuTFxcXEqbTETE46lOt90uXMCxciV5V62Cli3tjkZEJG1YFly+7N5zxsXBpUvg7w+3qg0cGgopXBn5yJEjtGzZku7duzNt2jT++usvevXqRUhICMOHD+fYsWN07tyZ0aNH06ZNGy5cuMCvv/6KZVnExMTQunVrevXqxcyZM7l27Rrr1q3Tas2p6FYvt+S+bO7Wnb7c5syZQ5kyZShdujRPPPEEAwYMYMiQITgcDr7//nvatGnDK6+8wrRp07h27RqLFy923bdr166sXr2aDz74gMqVK3PgwAFOnz59R/Hu3buX+fPn8/XXX7vqY1+6dImBAwdSqVIlLl68yNChQ2nTpg1btmzBz8+Pixcv0rBhQ/Lnz8+iRYvInTs3v//+O3FxcRQpUoTGjRszZcoUatSo4XqcKVOm0L17d9VGF1vExcFvvzk4fz7I7lDERyjpttu6dQQ89BDlc+eGN96wOxoRkbRx+TJkyuTWU/oB2ZJz4MWLkDFjih7ro48+omDBgowfPx6Hw0GZMmU4evQoL774IkOHDuXYsWPExMTQtm1bChcuDEDFihUBOHPmDOfPn+fBBx+kePHiAJQtWzZF8cit3frlluxXzl2505fbpEmTeOKJJwBo3rw558+fZ+XKlTRq1Ij//e9/dOrUiREjRriOr1y5MgC7d+9mzpw5RERE0LhxYwCKFSt2x/Feu3aNadOmkStXLte+du3axTtm8uTJ5MqVi507d1KhQgVmzJjBqVOnWL9+PTly5CAuLo7cuXOTJUsWAJ566in69OnDu+++S3BwMJs2bWL79u188803dxyfSEpYFvz4I7zyCmzaFEDWrPeTP7+DBx6wOzLxdvp60W41a2I5HGQ8eRJOnrQ7GhERSYY///yTOnXqxOudvvfee7l48SKHDx+mcuXKPPDAA1SsWJEOHTrw2WefcfbsWQBy5MhB9+7dadasGQ899BDvv/8+x44ds+upiAfZtWsX69ato3PnzgAEBATQsWNH1xDxLVu28EAS2cGWLVvw9/enYcOGKYqhcOHC8RJugD179tC5c2eKFStGlixZKFKkCACHDh1yPXbVqlXJkSNHouds3bo1/v7+LFiwADBD3e+77z7XeUTSwq+/QsOG0KIFbNpk9p0/H0yzZv68/75JyEVSi5Juu2XJAmXKAOBYt87mYERE0khoqOkCdOMlLjKSc4cPExcZeetjQ0NT/en5+/sTERHBDz/8QLly5fjwww8pXbo0Bw4cAMzQ2tWrV1O3bl1mz55NqVKlWLNmTarH5atu9XKLjIzj8OFzREbGufsleccvt0mTJhETE0O+fPkICAggICCAjz/+mPnz53P+/HkyZMiQ5H1vdRuAn58f1k1ZRXR0dILjMibSLf/QQw9x5swZPvvsM9auXcvatWuB6wut3e6xg4KC6Nq1K1OmTOHatWvMmDGDJ5988pb3EXGXTZvMDM4GDUziHRICgwbB3r3RNGz4D7GxDgYMgK5d3T/rScRJSbcHsGrVAsCxfr3NkYiIpBGHw4y5tePihrnTZcuWZfXq1fGSmN9//53MmTNToECB/3+KDu69915GjBjB5s2bCQoKcvX0AVStWpUhQ4awatUq1xBdSR3p4eUWExPDtGnTGDt2LFu2bHFdtm7dSr58+Zg5cyaVKlVi2bJlid6/YsWKxMXFsXLlykRvz5UrFxcuXHAt5gemh/p2/v33X3bt2sWrr77KAw88QNmyZV2jNpwqVarEli1bOHPmTJLneeqpp/jpp5/46KOPXFMvRFLTX3/Bo49C9erwww8QEAB9+sDevTBmDBQqBAMGbGLMmFj8/eGrr6BePTh40O7IxRsp6fYAVs2agJJuERFPdP78+XhJ0JYtW+jduzf//PMP/fr146+//uKbb75h2LBhDBw4ED8/P9auXcubb77Jhg0bOHToEF9//TWnTp2ibNmyHDhwgCFDhrB69Wr+/vtvli5dyp49ezSv28d99913nD17lp49e1KhQoV4l3bt2jFp0iSGDRvGzJkzGTZsGH/++Sfbt2/n7bffBqBIkSJ069aNJ598koULF3LgwAFWrFjBnDlzAKhduzahoaG8/PLL7Nu3jxkzZiRYGT0x2bNnJywsjIkTJ7J3716WL1/OwIED4x3TuXNnwsPDad26Nb///jv79+9n0aJFrF692nVM2bJlueeee3jxxRfp3LnzbXvHRe7WwYPQoweULw9z55ovvp54wiThH38M+fNfP9bhgOefj+OnnyBnTti8GWrUgCS+2xK5a0q6PUDcjUm3SmeIiHiUFStWULVq1XiXkSNHsnjxYtatW0flypXp06cPPXv25NVXXwUgS5Ys/PLLL7Rs2ZJSpUrx6quvMnbsWFq0aEFoaCh//fUX7dq1o1SpUvTu3ZvnnnuOp59+2uZnKnaaNGkSjRs3JmvWrAlua9euHRs2bCBHjhzMnTuXRYsWUaVKFe6//37W3TA17eOPP6Z9+/Y8++yzlClThl69erl6tnPkyMFXX33F4sWLqVixIjNnzmT48OG3jcvPz49Zs2axceNGKlSowAsvvMA777wT75igoCCWLl1K7ty5admyJZUrV2bcuHGu1c+devbsybVr1zS0XFLF8ePQrx+UKgVffGE+UrduDdu2wZdfwv+vW5moRo1g40bTK/7vv9C0KYwdq3ne4j4O6+YJPpIskZGRZM2alfPnz7tW57xb0Veu4MiWjYBr1+DPP11zvOXuRUdHs3jxYlq2bElgYKDd4aR7ak/386U2vXr1KgcOHIhX9zc1xMXFERkZSZYsWbyqDNGt2s+d70Xpya2e952+3rz1dWOnpNp05MiRzJ07l23btt32HGn1fyO98KX3jDt19iyMHg0ffHB9TnbjxqYoUO3aSd8vsTa9cgWeeQamTjXHdOoEn3+e4oIXPsMXX6fJfR/Wu4snCAjgvPPrt/9fnERERETEG1y8eJEdO3Ywfvx4+vXrZ3c44iUuXoQ334SiReGtt0zCfc89Zmh4RMStE+6kZMgAU6bA+PFmDvisWVCnDuzb5/74xbco6fYQZ0uVMhtKukVERMSL9O3bl+rVq9OoUSMNLZcUi4oyvdrFi5t62+fPQ8WKsGgRrFoF99+fsvM7HPDcc7B8OeTJA9u3Q82apr63JO38efj6aweXLwfYHYpHUtLtIVxJt8qGiYiIiBf54osviIqKYvbs2QnmeYskV0wMTJoEJUtC//5w8iSUKAEzZsCWLfDQQ24pTuFSv76Z5127thnC3qIFjBqled43i4szowNKlYJOnQIYMqQex47ZHZXnUdLtIc6WLGk2tm41E0pERERERHxcXBzMnm1WI3/qKfjnH7MC+cSJsHMndO4MqbUcQ/78sHIl9Oplku2XX4YOHeDChdR5vPRm7Voz/P7JJ82XIAB//52V++4LYP9+e2PzNEq6PcSVXLmw8uQxX+Nt3mx3OCIiIiIitrEs+P57qFbNLGi2e7cp6/Xuu6bWdq9ekBZrdQUHmwT/00/N482fb+aO79mT+o/tqY4fN2XZ7rnHDNLNlAneeQe2bYsmT55L7N/v4N57zdB8MZR0ewqHw1WvW/O6RcQbxakk4l1RkZG7o9db+qbfn29buRLq1YMHHzSDQLNkgddfh/374YUXwI4F7Xv3NnHlzWt62GvWNF8K+JJr10wpNWdZNoBu3cwXIv/5jynANGrUr1SoYHH8ODRoYObZC2imuwexatWC775T0i0iXiUoKAg/Pz+OHj1Krly5CAoKwuHOiXf/Ly4ujmvXrnH16lWvKf1kWRanTp3C4XD4TPmVlLrT15s3vm7slpI2tSyLa9eucerUKfz8/AgKCkqlKMUTbdhgFkdbutRcz5DB1N7+738hLMze2MAMpd640Qwx//13M498+HB49dXUG+LuKZYsgQEDYNcuc71GDfjwQ9PbfaMcOaJYtiyGNm0CWbXKlG/7+mto3jzNQ/YoSro9iFWrltnQYmoi4kX8/PwoWrQox44d4+jRo6n2OJZlceXKFTJkyJAqSb1dHA4HBQoU0AJUyXSnrzdvfd3YyR1tGhoaSqFChfRFiI/YuRNee80kZ2DKdfXubRLwfPnsje1mefOalc1feAE++giGDYNNm2DaNNMj72327YOBA83q8AC5cpkSbd27J/1FQ/bs5ouT9u1Nsv7ww/Dll9CxY5qF7XGUdHsQq3p1s+zigQNw6pR5VYuIeIGgoCAKFSpETEwMsbGxqfIY0dHR/PLLLzRo0MCreoUDAwOVcN+hO3m9eevrxk4pbVN/f38CAgL0JYgPOHDA9BR/+aWZw+1wQJcuJpEtVszu6JIWFAQTJpje3meegW++gVq1YOFCM8TaGzjroI8da4aVBwSYUQdDh0K2bLe/f8aMpl26dTP1zjt3hjNnTHv5IiXdniRrVvOX+uefZoj5gw/aHZGIiNs4h0inVmLj7+9PTEwMISEhSp4k2a83vW7cT20qt3PsGLzxBnz2GURHm31t25p52+XL2xvbnejRAypUMLHv2mUS72nToHVruyO7e5YFM2fC4MHgHCzUpAm8/z6ULXtn5woKgq++Mj3fH38Mzz5rEu+XX3Zvebf0QGN2PE3t2uan5nWLiIiIiBf591948UUoXtwMzY6OhmbNYP16syp4ekq4nWrWNPO8GzY0pcTatDG9welxLcDNm0198scfNwl30aKm9/7HH+884Xby9zejAl591Vx/9VUYNCh9tk9KKOn2NEq6RURERMSLXLgAI0eaIeOjR8OVK1C3LqxYYeb81qhhd4Qpkzs3RERA//7m+siRZpG1c+dsDSvZTp+GPn2genWzQFxoqBmJsHMnPPJIynulHQ7TJu+9Z66/956p7R0Tk/LY0wsl3Z7GmXSvW+d7XwGJiIiIiNe4etUkWMWKmd7fyEioXNmU2vrtN9M77C0CA2HcODM/PSQEFi82veA7dtgdWdJiYmD8eChZ0tQhtyxTE/2vv8widu4uzTZgAEydanq/p06Fdu3Ma8QXKOn2NBUqmFf4+fOwZ4/d0YiIiIiI3JHoaJg4EUqUMCtfnz5tajvPnm1W+m7Z0nvn9D7xhOktLlwY9u41JbXmzbM7qoR+/hmqVjWLo507Z74MWbnSzOcuWDD1HrdrV7NKfXCwWRG9eXPzZYy3U9LtaQIDzdgO0BBzEREREUk34uJM0lauHDz9NBw5YhK4SZPgjz/g0Ue9v541QLVqpub4Aw/ApUumrveQIZBKxTvuyN9/m3juv9/0wufIYRY527gRGjRImxgeftjME8+c2ST6990HJ0+mzWPbxQde9umQ5nWLiIiISDoSGWl6sB97zPTw5s5tVrzes8fM3w3wsZpJOXOa+er/+Y+5/tZbpn3OnLEnnitXYMQIUyhp3jzz5cdzz5nfT58+Zsh3WmrY0Mzpz5XLjH6oXx8OHUrbGNKSkm5PpKRbRERERNKJf/6BevVM72VoKPzvf7BvHzz/vBlG7KsCAuCdd0zvf4YMsHSpWTRu69a0i8GyzMrwZcuamuhXr5qEd/NmM587R460i+Vm1arBr79CoUKwezfce6+pnOyNlHR7ImfSvXWr+VpKRERERMQDbdxoPrpu3w7h4fDLL6YOc6ZMdkfmOTp1gjVrzIJyBw5AnTomEU9tO3ZA48bQvr0ZVl6woJlX//PPUKlS6j9+cpQubRbVK1MGDh82Pd7r19sdlfsp6fZEhQqZMTkxMbBli93RiIiIiIgk8O23Zh7wsWNmLeC1a68vTSTxVapkkslmzUyf2mOPmaHnqVE26+xZU76sShVYvtyMNnjtNdOL/OijnreIXcGCpse7Rg1Ty/3++03c3kRJtydyODTEXEREREQ81gcfQOvWcPkyNGlieisLFbI7Ks+WI4cplzZkiLk+dqxJwk+fds/5Y2PNqvGlSpnfT2wstGljku3XX4eMGd3zOKkhZ06TaN9/P1y8CC1awIIFdkflPkq6PZWSbhERERHxMLGxZq52//5mtfJevUwimTWr3ZGlD/7+8OabMHeuSYKXLzejAzZtStl5f//d1AV/+mmTxJctCxERpjxX0aLuiT21Zc5sXktt2sC1a2ZY/OTJdkflHkq6PZWSbhERERHxIBcvmoToww/N9bffhk8/NRVv5c60b28+5pcsaVbtvvde+PLLOz/PkSOmNni9emZxtKxZYdw4szRU48ZuDzvVhYTAnDlmxfu4OOjZE8aMsTuqlFPS7alq1jTDzA8cgFOn7I5GRERERHzY0aNm1etvvzVzhOfMgf/+1/PmB6cn5cvDunXQqpVZVbxrVzOCIDr69veNijJlyEqXhunTze/hqafMKuD9+6fvL0ICAuDzz6+XWxs82AzJtyx740oJJd2eKmtWs4wfmL9GEREREREbbN8O99xjhkDnymVWv+7Qwe6ovEO2bLBoEQwdaq5/8IHpoT5xIvHjLQu++84k7EOGwKVLZjX0devgs8/MWszewOEw5dbeestcf+stM3Q+NtbeuO6Wkm5PVquW+akh5iIiIiJigx9/NEOf//nH9KquWWOSPHEfPz8YMQIWLjTzmn/5xczzvrnfbdcu0yv+0EOmDnp4OEybZhaxq1HDltBT3YsvmikMDof5UqFzZ9PLn94o6fZkmtctIiIiIjaZONEkeRcumKHlq1aZWtOSOh55xCTaZcqYudr165uFxCIjzRDrChXghx/M0PH//tcMJe/SxSTt3qx3bzOdITDQLED30ENmfYH0xMt/RemcM+let86sJCAiIiIiksri4kwPo3M4b5cusHSpKXklqatMGdPf1rq1WcG7Z09Tx3rMGFPTu2VL2LHDLGKXObPd0aad9u3NyuYZM5pV2Rs3hjNn7I4q+ZR0e7KKFc0SfufOwZ49dkcjIiIiIl7uyhXo2BFGjzbXhw+HqVMhKMjWsHxKliwwfz6MHGmGVUdGmlXOv/vOJJ6lStkdoT2aNIGffoLs2c0XEw0amBEB6YGSbk8WGGgmdICGmIuIiIhIqjp5Eu6/H+bNMx9Dv/wShg3TCuV28PODV1+FlSth0iSzmF2rVnZHZb977oFff4V8+eCPP0yptPTQN6mk29M5F1PTCuYiIiIikkr+/NMkNGvWmJ7EiAhT/1nsVb++qVkdHGx3JJ6jfHn4/XcoUQIOHjSJ95Ytdkd1a0q6PZ0WUxMRERGRVPTzz1C3Lhw4YBZKW73aLJwm4qmKFDGrtleubEZoNGpkesA9lZJuT+dMurduhatX7Y1FRERERLzK1KnQrJlZQqhuXdPTXbq03VGJ3F6ePLBihenpPn8emjY1c949kZJuT1e4sKlyHx0NmzfbHY2IiIiIeAHLMvO1u3c3HzMffRSWLYNcueyOTCT5smUzteRbtTL9k61bw/TpdkeVkJJuT+dwaIi5iIiIiLhNVJQpA/b66+b6kCEwc6YpmiOS3oSGwoIF8PjjpqzaE0/A+PF2RxWfku70QIupiYiIiIgbnDljhuFOnw7+/vDZZ/Dmm2a1bJH0KjAQpk2Dfv3M9X79YMQIM6LDE+jPKz1QT7eIiIiIpNDevVCnDvzyi6kF/cMP8NRTdkcl4h5+fvD++6a2PJif/ftDXJydURlKutODmjXNz/374dQpe2MRERERkXRn1SqTcO/eDYUKmZJLTZrYHZWIezkcZq2CDz4w1z/8ELp2NesW2ElJd3qQLRuUKWO2NcRcRERERO7A7Nlw//1w+jRUr25WKK9Qwe6oRFJPv37w1VdmCsX06dCmDVy+bF88SrrTCw0xFxEREZE7YFlmvnanTmbxtEcegZUrIW9euyMTSX2PPw7ffGMWCPz+++ul8eygpDu9cC6mpqRbREQ83JEjR3jiiScICwsjQ4YMVKxYkQ0bNrhutyyLoUOHkjdvXjJkyEDjxo3Zs2ePjRGLeJ/oaOjVC155xVwfMADmz4eMGW0NSyRNtWoFS5eaNQx++w0aNYITJ9I+DiXd6YWzp3vdOs9Zhk9EROQmZ8+e5d577yUwMJAffviBnTt3MnbsWLJnz+46ZvTo0XzwwQd88sknrF27lowZM9KsWTOuXr1qY+Qi3uPcOWjRAiZNMotLffghvPeeGWor4mvq1zcjPHLnhq1boV49OHgwbWMISNuHk7tWqZIZG3HuHOzZA6VK2R2RiIhIAm+//TYFCxZkypQprn1FixZ1bVuWxbhx43j11Vd55JFHAJg2bRp58uRh4cKFdOrUKc1jFvEmf/8NLVvCzp2mV3v2bNPbJ+LLqlQxPd1NmphV/O+9F379FYoVS5vHV9KdXgQGQrVqZunJtWuVdIuIiEdatGgRzZo1o0OHDqxcuZL8+fPz7LPP0qtXLwAOHDjA8ePHady4ses+WbNmpXbt2qxevTrRpDsqKoqoqCjX9cjISACio6OJTuGStM77p/Q8cp3a1P2S26YbNjho08afEycc5MtnsWBBDFWr2r9ysyfS69T9PL1NixSBFSugZcsA8ua1yJ07NsV/G8l9rkq605Pata8n3V262B2NiIhIAvv37+fjjz9m4MCBvPzyy6xfv57nn3+eoKAgunXrxvHjxwHIkydPvPvlyZPHddvNRo0axYgRIxLsX7p0KaGhoW6JOyIiwi3nkevUpu53qzZdsyacd9+tzrVrDooUOc+rr67h2LGrHDuWhgGmQ3qdup+nt+mQIYEEBlosWxaT4nNdTuaS6Eq60xOtYC4iIh4uLi6OGjVq8OabbwJQtWpVduzYwSeffEK3bt3u6pxDhgxh4MCBruuRkZEULFiQpk2bkiVLlhTFGx0dTUREBE2aNCEwMDBF5xJDbep+t2pTy4IPPvDj7bf9sCwHzZrFMWNGKJkz329TtOmDXqfu54tt6hx5dTtKutMT5wrmW7fC1atmjreIiIgHyZs3L+XKlYu3r2zZssyfPx+A8PBwAE6cOEHeG+oWnThxgipVqiR6zuDgYIKDgxPsDwwMdNsHO3eeSwy1qfvd3KYxMfDCCzBhgrnepw98+KEfAQFaKzm59Dp1P19q0+Q+T9v/IidMmECRIkUICQmhdu3arFu37pbHz507lzJlyhASEkLFihVZvHhxksf26dMHh8PBuHHj4u3ftGkTTZo0IVu2bISFhdG7d28uXrzojqeTuooUgVy5zMScLVvsjkZERCSBe++9l127dsXbt3v3bgoXLgyYRdXCw8NZtmyZ6/bIyEjWrl1LnTp10jRWkfTswgVTd3vCBHA4YMwY+OgjCFCXmojHsTXpnj17NgMHDmTYsGFs2rSJypUr06xZM06ePJno8atWraJz58707NmTzZs307p1a1q3bs2OHTsSHLtgwQLWrFlDvnz54u0/evQojRs3pkSJEqxdu5YlS5bwxx9/0L1799R4iu7lcGiIuYiIeLQXXniBNWvW8Oabb7J3715mzJjBxIkTee655wBwOBwMGDCAN954g0WLFrF9+3a6du1Kvnz5aN26tb3Bi6QTR45AgwaweLEZ+Dh3LgwaZD4qiojnsTXpfvfdd+nVqxc9evSgXLlyfPLJJ4SGhjJ58uREj3///fdp3rw5gwcPpmzZsowcOZJq1aoxfvz4eMcdOXKEfv36MX369ARd/t999x2BgYFMmDCB0qVLU7NmTT755BPmz5/P3r17U+25uo2SbhER8WA1a9ZkwYIFzJw5kwoVKjBy5EjGjRvH448/7jrmv//9L/369aN3797UrFmTixcvsmTJEkI0bUrktrZuNR8Ht2wxdYdXrIB27eyOSkRuxbak+9q1a2zcuDFeyRA/Pz8aN27M6tWrE73P6tWr4x0P0KxZs3jHx8XF0aVLFwYPHkz58uUTnCMqKoqgoCD8/K4/9QwZMgDw22+/peg5pQkl3SIi4uEefPBBtm/fztWrV/nzzz9d5cKcHA4Hr7/+OsePH+fq1av89NNPlFIpTJHb+uEHB/XqmZ7usmVhzZrrHw1FxHPZNuvj9OnTxMbGJloy5K+//kr0PsePH79tiZG3336bgIAAnn/++UTPcf/99zNw4EDeeecd+vfvz6VLl3jppZcAOHaLmgoeUyO0ShUCAfbvJ/rYMciZM0WP7a08vU5geqP2dD+1qfv5Ypv60nMV8XU//FCEzz7zJy4O7r8f5s+HbNnsjkpEksOrllrYuHEj77//Pps2bcKRxKSW8uXLM3XqVAYOHMiQIUPw9/fn+eefJ0+ePPF6v2/mSTVC78+fn8xHjrDho484WaOGWx7bW3l6ncD0Ru3pfmpT9/OlNk1ufVARSb8uX4YXX/Tj008rA9C9O3z6KQQF2RuXiCSfbUl3zpw58ff358SJE/H2nzhxwlVO5Gbh4eG3PP7XX3/l5MmTFCpUyHV7bGwsgwYNYty4cRw8eBCAxx57jMcee4wTJ06QMWNGHA4H7777LsWKFUsyXk+qEep/333w1VfUsiziWrZM0WN7K1+sE5ia1J7upzZ1P19s0+TWBxWR9MeyYN48s0DaP//4AzBiRCyvveavBdNE0hnbku6goCCqV6/OsmXLXKuVxsXFsWzZMvr27ZvoferUqcOyZcsYMGCAa19ERISrxEiXLl0SnfPdpUsXevTokeB8zqHqkydPJiQkhCZNmiQZr0fVCK1TB776Cv8NG/D3kQ+Wd8uX6gSmBbWn+6lN3c+X2tRXnqeIr9mxA55/Hn7+2VwvXNji8cfXMWRINRwOf3uDE5E7Zuvw8oEDB9KtWzdq1KhBrVq1GDduHJcuXXIlyF27diV//vyMGjUKgP79+9OwYUPGjh1Lq1atmDVrFhs2bGDixIkAhIWFERYWFu8xAgMDCQ8Pp3Tp0q5948ePp27dumTKlImIiAgGDx7MW2+9Rbb0MjHGuWLGunXma1B93SkiIiKS7p07B8OGmdrbsbGmHNiLL8ILL8SwYsXx295fRDyTrUl3x44dOXXqFEOHDuX48eNUqVKFJUuWuHqgDx06FG+edd26dZkxYwavvvoqL7/8MiVLlmThwoVUqFDhjh533bp1DBs2jIsXL1KmTBk+/fRTunTp4tbnlqoqVoTgYDh7FvbsAa34KiIiIpJuxcXBlCkwZAicOmX2tWkD774LRYqA1kwUSd9sX0itb9++SQ4nX7FiRYJ9HTp0oEOHDsk+v3Me942mTZuW7Pt7pKAgqFYNVq82vd1KukVERETSpXXroG9fWL/eXC9TBj74AG4x61FE0hnb6nRLCqlet4iIiEi6deIEPPmk+Ui3fj1kzgxjxsDWrUq4RbyN7T3dcpeUdIuIiIikO9HRZs72sGHgLEDQrRu89RYkUcBHRNI5Jd3plTPp3rIFrl41K22IiIiIiMdavhz69YOdO8316tXhww9NYRoR8V4aXp5eFSkCOXOar0u3bLE7GhERERFJwqFD0KEDPPCASbhz5oSJE82ARSXcIt5PSXd65XDELx0mIiIiIh7l6lUYOdIsjjZvHvj5mUXTdu+GXr3AXyW3RXyCku70TPO6RURERDyOZcHChVCuHAwdCleuQIMGsHmzGU6ePbvdEYpIWlLSnZ4p6RYRERHxKH/9Bc2bmzrbBw5A/vwwcyasWAGVKtkdnYjYQUl3elarlvm5bx+cPm1vLCIiIiI+LDISBg+GihVh6VIICoKXXzZJeKdOZmagiPgmJd3pWbZsULq02da8bhEREZE0Z1nw5ZfmI9mYMRATAw8+CH/8Af/7H2TKZHeEImI3Jd3pnbO3W0PMRURERNLUpk1Qrx507QrHj0OJEvD99/Dtt2ZbRASUdKd/WsFcREREJE2dPg1PPw01asCqVZAxI4waBTt2QMuWdkcnIp4mwO4AJIVuTLotSxOGRERERFJJTAx8+im89hqcPWv2de4Mo0dDgQL2xiYinks93eldpUoQHAxnzsDevXZHIyIiIuKVfv3V9Gz37WsS7kqVYOVKmDFDCbeI3JqS7vQuKAiqVTPbmtctIiIi4lZHjsBjj5k621u3mhrb48fDxo1mn4jI7Sjp9gZaTE1ERETEraKi4K23zKrkM2eaGXxPPw27d8Nzz0GAJmmKSDLp34U30GJqIiIiIm6zeDEMGAB79pjrdevChx9eH1woInIn1NPtDZxJ95Yt5mtZEREREblje/fCQw9Bq1Ym4Q4Ph2nT4LfflHCLyN1T0u0NihaFnDnh2jWTeIuIiIhIsl26BK+8AuXLw3ffmaHj//kP7NoFXbqoOIyIpIySbm/gcFzv7da8bhEREZFksSyYPRvKlIE33zT9F02bwvbt8M47kCWL3RGKiDdQ0u0ttJiaiIiISLKdOAH33w+dOsHhw1CkCCxYAEuWmCRcRMRdtJCat1BPt4iIiEiy9ekDK1ZASAgMGQKDB0OGDHZHJSLeSEm3t3D2dO/bB//+C2Fh9sYjIiIi4qEWL4aFC83c7TVroHJluyMSEW+m4eXeInt2KFXKbKt0mIiIiEiirl6Ffv3M9gsvKOEWkdSnpNubaIi5iIiIyC29/Tbs3w/588PQoXZHIyK+QEm3N1HSLSIiIpKk/fth1Ciz/d57kCmTvfGIiG9Q0u1NnPO6160zNTBERERExKV/f4iKgsaNoX17u6MREV+hpNubVK4MwcFw5oxZUE1ERES8hmXpO/WUWLQIvvsOAgNh/HhwOOyOSER8hZJubxIUBFWrmm0NMRcREfEap06ZOcidOinxvhuXL8Pzz5vt//wHSpe2Nx4R8S1Kur2N5nWLiIh4nZUr4dgxmDMHPvnE7mjSn1Gj4O+/oVAheOUVu6MREV+jpNvbKOkWERHxOn/9dX178GCzIJgkz549MHq02R43DjJmtDUcEfFBSrq9jXMxtS1bzEohIiIiku7t2mV+BgTApUvQowfExdkbU3pgWaYm97Vr0Lw5tG5td0Qi4ouUdHubYsUgZ07z7rJ1q93RiIiIiBs4k+633jJlrn75BT74wN6Y0oMFC+DHH82yNx9+qMXTRMQeSrq9jcNxvbdbQ8xFRETSPcu6nnQ3bw5jxpjtIUOu75eELl2CAQPM9osvQokStoYjIj5MSbc30rxuERERr3H8OERGgp+fSRx794amTeHqVejeHWJj7Y7QM73xBvzzDxQpAi+9ZHc0IuLLlHR7IyXdIiIiXsPZm12kCAQHm0Ftn38OWbLAmjXXe77lur/+grFjzfYHH0BoqL3xiIhvU9LtjWrWND/37oV//7U3FhEREUkRZ9Jdpsz1fQULwvvvm+2hQ2HHjrSPy1NZFvTtC9HR8OCD8NBDdkckIr5OSbc3ypEDSpY02+vW2RuLiIiIpIgz6S5dOv7+bt1MUnntGnTtapJMgblzYdkyCAnRYnMi4hmUdHsr5xBzJd0iIiLpmrNG981Jt8MBEydC9uyweTO8+Wbax+ZpLlyAF14w20OGQNGi9sYjIgJKur2X5nWLiIh4hcSGlzvlzQsTJpjtN96ATZvSLi5P9PrrcPQoFC8O//2v3dGIiBhKur3VjT3dlmVvLCIiInJXoqLg4EGzfXNPt1OnTtCuHcTEmCHnUVFpFp5H+eMPGDfObH/4oRleLiLiCZR0e6tKlSAoyCyktm+f3dGIiIjIXdi7F+LizErlefIkfozDAR9/DLlymQXVRoxI2xg9gWXBc8+ZLx5at4YWLeyOSETkOiXd3io4GKpWNdsaYi4iIpIu3Tif2+FI+rhcueCTT8z222/73lv/zJmwciVkyHC9t1tExFMo6fZmWkxNREQkXbvVfO6btW0Ljz9uesa7dYMrV1I3Nk9x/jwMGmS2X30VChe2Nx4RkZsp6fZmWkxNREQkXUuqXFhSPvzQLK62a5dJQH3B8OFw/DiUKnU9+RYR8SRKur2ZM+nevNl3V1URERFJx5IqF5aU7Nnh88/N9nvvwa+/pk5cnmLbNvNFA8D48WZ2nYiIp1HS7c2KFYOwMLh2DbZutTsaERERuQOWdec93QAtW8KTT5r79+gBly6lTnx2syx49lmIjYUOHaBJE7sjEhFJnJJub+ZwQK1aZltDzEVERNKVkyfNfGWHA0qWvLP7vvsuFCxoCpi8+GLqxGe3adPg998hY0bzfEVEPJWSbm+ned0iIiLpkrOXu0iRO685nTUrTJ5stidMgGXL3Bqa7c6dg8GDzfbQoVCggK3hiIjckpJub6cVzEVERNKlO53PfbPGjeGZZ8z2k09CZKR74vIEr74Kp05B2bIwYIDd0YiI3JqSbm/nHF6+Zw+cOWNvLCIiIpJsdzOf+2ajR5slXg4dgoED3ROX3TZtgo8/NtsTJkBQkL3xiIjcjpJub5cjx/WJYOrtFhERSTfupEZ3UjJlgilTzLzwSZNg8WL3xGaXuDh47jnzs3NnuO8+uyMSEbk9Jd2+QIupiYiIpDvu6OkGaNAA+vc32716wdmzKTufnaZMgTVrIHNmGDPG7mhERJJHSbcv0GJqIiIi6UpUFOzfb7ZTmnQDvPkmlCoFR4/C88+n/Hx2OHPm+krsI0ZAvnz2xiMiklxKun3BjYupWZa9sYiIiMht7dtnhlBnzgx586b8fBkywNSp4OcHX30FCxem/Jxp7eWX4d9/oUIF6NvX7mhERJJPSbcvqFzZrDLy77/XvzYXERERj3Xj0HKHwz3nvOee62W2nn4aTp92z3nTwvr1MHGi2Z4wAQID7Y1HROROKOn2BcHBULWq2dYQcxEREY+X0nJhSRkxAsqXh5Mn4dln3Xvu1BIba2K1LOjSxcxRFxFJT5R0+wotpiYiIpJuuGsRtZsFB5th5gEBMHcuzJ7t3vOnhs8/hw0bIEsWUwJNRCS9UdLtK7SYmoiISLrhjnJhSaleHV55xWw/+ywcP+7+x3CX06dhyBCz/cYbEB5ubzwiIndDSbevcCbdmzfDtWv2xiIiIiJJsqzU6+l2euUVM/PszBkzv9tT11l96SVT4qxKFXjmGbujERG5O0q6fUXx4hAWZhLurVvtjkZERESScOqUSTQdDihZMnUeIzDQDDMPDIRFi+DLL1PncVJizRqYNMlsT5hghsSLiKRHSrp9hcOhed0iIiLpgLOXu1AhU+ortVSsaBZWA1O7+/Dh1HusO+VcPA2gRw+oW9feeEREUkJJty9R0i0iIuLxUnM+980GDzYfD86fh549PWeY+SefmBlx2bLBW2/ZHY2ISMoo6fYlWkxNRETE46X2fO4bBQSYYeYhIbB0KXz2Weo/5u2cOHF9obc334Tcue2NR0QkpZR0+xJnT/eePWblFBEREfE4qVWjOyllysD//me2Bw2CgwfT5nGT8uKLpue9enXo3dveWERE3EFJty8JC4MSJcz2+vX2xiIiIl5p+PDhOByOeJcyN4yTbtSoUYLb+/TpY2PEnicth5c79e8P9erBxYtmDnVcXNo99o1++830vDsc8NFH4O9vTxwiIu6kpNvXaIi5iIiksvLly3Ps2DHX5bfffot3e69eveLdPnr0aJsi9TzXrsH+/WY7rXq6wSS3X3wBoaGwYoVZLTytxcRcXzztqaeuD9ATEUnvlHT7GiXdIiKSygICAggPD3ddcubMGe/20NDQeLdnyZLFpkg9z759ZuXuTJkgX760fezixcH5/ceLL5rZaGlpwgTYvh1y5DBzuUVEvIUqHvqaG1cwtywzfktERMSN9uzZQ758+QgJCaFOnTqMGjWKQoUKuW6fPn06X331FeHh4Tz00EO89tprhIaGJnm+qKgooqKiXNcjIyMBiI6OJjo6OkWxOu+f0vO4yx9/OIAASpa0iImJSfPHf+opmD/fn59/9qNbtziWL4+94yHed9Omx47Ba68FAA7+978Ysma18JBfiUfwtNepN1Cbup8vtmlyn6uSbl9TpQoEBcG//5rxa8WL2x2RiIh4kdq1a/PFF19QunRpjh07xogRI6hfvz47duwgc+bMPPbYYxQuXJh8+fKxbds2XnzxRXbt2sXXX3+d5DlHjRrFCGdB6RssXbr0lsn6nYiIiHDLeVLq229LAOXJnPkwixdvsiWGzp0zsGbNfaxeHcgzz+ykdet9d3WeO2nT996rxoULBSlZ8ix58vzC4sV39ZBez1Nep95Ebep+vtSmly9fTtZxSrp9TXCwSbzXrTMXJd0iIuJGLVq0cG1XqlSJ2rVrU7hwYebMmUPPnj3pfcNy1BUrViRv3rw88MAD7Nu3j+JJvCcNGTKEgQMHuq5HRkZSsGBBmjZtmuKh6dHR0URERNCkSRMCAwNTdC53WLDAdCs3apSPli3DbYsjLs5Bnz4wc2Z5BgwoTblyyb/vnbbpypUOVq4MwOGw+PLLTFSr1jIFkXsnT3udegO1qfv5Yps6R17djpJuX1S7tkm4166Fzp3tjkZERLxYtmzZKFWqFHv37k309tr/v9bI3r17k0y6g4ODCQ4OTrA/MDDQbR/s3HmulNi92/wsV86fwED7lu7u3Ru++QZ++MFBr16BrFplanrfieS0aXS0WTkdoE8fB7Vr2/878GSe8jr1JmpT9/OlNk3u89RCar5Ii6mJiEgauXjxIvv27SNv3ryJ3r5lyxaAJG/3Nc5yYWm5cnliHA747DPIls1UGX377dR5nPffh507IWfO67XCRUS8jZJuX+RcTG3zZlObRERExE3+85//sHLlSg4ePMiqVato06YN/v7+dO7cmX379jFy5Eg2btzIwYMHWbRoEV27dqVBgwZUqlTJ7tBtd/o0nDljtkuVsjcWgPz54cMPzfaIEbB1q3vPf/gwDB9utkePhuzZ3Xt+ERFPoaTbF5UoYepxREW5/x1URER82uHDh+ncuTOlS5fm0UcfJSwsjDVr1pArVy6CgoL46aefaNq0KWXKlGHQoEG0a9eOb7/91u6wPYKzl7tQIVMv2xM8/ji0bm2GgXft6t7v6gcNgkuXoE4d6NbNfecVEfE0mtPtixwO09u9ZIkZYl6zpt0RiYiIl5g1a1aStxUsWJCVK1emYTTpy19/mZ92Dy2/kcMBn3wCv/4K27bByJHmklI//QRz5oCfH3z0kfkpIuKt9C/OVznnda9bZ28cIiIiAnjOfO6b5ckDH39stkeNMnO8U+LaNejb12w/95wpqiIi4s3U0+2rtJiaiIgAcXFxrFy5kl9//ZW///6by5cvkytXLqpWrUrjxo0pWLCg3SH6DGfSXaaMvXEkpkMH6NgRZs82Q8E3bYKQkLs717vvmueaJw+8/rp74xQR8UTq6fZVziHlu3fD2bP2xiIiImnuypUrvPHGGxQsWJCWLVvyww8/cO7cOfz9/dm7dy/Dhg2jaNGitGzZkjVr1tgdrk/wxOHlN5owwSTKf/4JQ4fe3TkOHbo+PP2dd8zq6CIi3k5Jt6/KmROc9VA1xFxExOeUKlWKbdu28dlnnxEZGcnq1auZP38+X331FYsXL+bQoUPs27eP+vXr06lTJz777DO7Q/Zq0dGwf7/Z9tSkOywMJk4022PGwKpVd36OF16Ay5ehfn144gn3xici4qlsT7onTJhAkSJFCAkJoXbt2qy7TQI4d+5cypQpQ0hICBUrVmTx4sVJHtunTx8cDgfjxo2Lt3/37t088sgj5MyZkyxZslCvXj1+/vlndzyd9EVDzEVEfNbSpUuZM2cOLVu2JDAwMNFjChcuzJAhQ9izZw/3339/GkfoW/bvh5gYyJjRlOryVA8/bFYxtyzo3t0k0Mm1ZAl8/TX4+5tec4cj1cIUEfEotibds2fPZuDAgQwbNoxNmzZRuXJlmjVrxsmTJxM9ftWqVXTu3JmePXuyefNmWrduTevWrdmxY0eCYxcsWMCaNWvIly9fgtsefPBBYmJiWL58ORs3bqRy5co8+OCDHD9+3O3P0aNpMTUREZ9VtmzZZB8bGBhIcefoKEkVzvncpUp5/kre779vvhjYsweGDEnefa5ehX79zPbzz0PFiqkXn4iIp7H13/q7775Lr1696NGjB+XKleOTTz4hNDSUyZMnJ3r8+++/T/PmzRk8eDBly5Zl5MiRVKtWjfHjx8c77siRI/Tr14/p06cn+Pb+9OnT7Nmzh5deeolKlSpRsmRJ3nrrLS5fvpxo8u7Vbuzptix7YxEREdvFxMQwYcIEOnToQNu2bRk7dixXr161Oyyf4OnzuW+ULRt8/rnZ/uADWLHi9vcZMwb27oW8eWH48FQMTkTEA9m2evm1a9fYuHEjQ274itTPz4/GjRuzevXqRO+zevVqBg4cGG9fs2bNWLhwoet6XFwcXbp0YfDgwZQvXz7BOcLCwihdujTTpk2jWrVqBAcH8+mnn5I7d26qV6+eZLxRUVFERUW5rkdGRgIQHR1NdHR0sp5zUpz3T+l57li5cgQEBuI4fZro3buhWLG0ffxUZFubeim1p/upTd3PF9vU3c/1+eefZ/fu3bRt25bo6GimTZvGhg0bmDlzplsfRxLy1HJhSWneHHr1gs8+gx49TA3vzJkTP/bgQfjf/8z22LGQJUuahSki4hFsS7pPnz5NbGwsefLkibc/T548/OX8uvcmx48fT/T4G4eFv/322wQEBPD8888neg6Hw8FPP/1E69atyZw5M35+fuTOnZslS5aQPXv2JOMdNWoUI0aMSLB/6dKlhIaGJnm/OxEREeGW89yJBkWKkH3PHrZOnMiRBg3S/PFTmx1t6s3Unu6nNnU/X2rTy3cyoTYRCxYsoE2bNq7rS5cuZdeuXfj7+wPmi+177rknRY8hyePJ5cKSMnYsLF1qkurBg+GTTxI/rn9/M7z8vvugU6c0DVFExCN4VZ3ujRs38v7777Np0yYcSazOYVkWzz33HLlz5+bXX38lQ4YMfP755zz00EOsX7+evHnzJnq/IUOGxOtlj4yMpGDBgjRt2pQsKfzKNjo6moiICJo0aZLkYjapxW/pUtizh6rR0VRu2TJNHzs12dmm3kjt6X5qU/fzxTZ1jrq6W5MnT2bq1Kl89NFH5MuXj2rVqtGnTx/atWtHdHQ0n332GTWdJSYlVaW3nm4wPdtTpsD998Onn0LbttC0afxjvvsOFi2CgAAYP16Lp4mIb7It6c6ZMyf+/v6cOHEi3v4TJ04QHh6e6H3Cw8Nvefyvv/7KyZMnKVSokOv22NhYBg0axLhx4zh48CDLly/nu+++4+zZs65k+aOPPiIiIoKpU6fy0ksvJfrYwcHBBAcHJ9gfGBjotg937jxXstWpAx99hP/69fh74YdUW9rUi6k93U9t6n6+1KYpfZ7ffvsts2fPplGjRvTr14+JEycycuRIXnnlFWJjY7n33nsZrgm4qe7ff+H0abNdqpS9sdyp++4zC6R9+CH07Anbt5sV2AGuXDGLpgEMHAjlytkXp4iInWxbSC0oKIjq1auzbNky1764uDiWLVtGnTp1Er1PnTp14h0PZhih8/guXbqwbds2tmzZ4rrky5ePwYMH8+OPPwLXh+L53bQ0qJ+fH3FxcW57fumGczG1zZvh2jV7YxERkTTXsWNH1q1bx/bt22nWrBlPPPEEGzduZMuWLUyYMIFcuXLZHaLXc/ZyFyhwPWFNT0aNghIl4PBhGDDg+v533vHjwAGz0vlrr9kWnoiI7WwdXj5w4EC6detGjRo1qFWrFuPGjePSpUv06NEDgK5du5I/f35GjRoFQP/+/WnYsCFjx46lVatWzJo1iw0bNjBx4kTALJIWFhYW7zECAwMJDw+n9P+P16pTpw7Zs2enW7duDB06lAwZMvDZZ59x4MABWrVqlYbP3kOUKAHZs8PZs2YVlBo17I5IRETSWLZs2Zg4cSK//PILXbt2pXnz5owcOZKQkBC7Q/MJ6XE+940yZoQvvoD69WHqVHj4YQcnT4byzjumg+O99yBTJntjFBGxk60lwzp27MiYMWMYOnQoVapUYcuWLSxZssS1WNqhQ4c4duyY6/i6desyY8YMJk6cSOXKlZk3bx4LFy6kQoUKyX7MnDlzsmTJEi5evMj9999PjRo1+O233/jmm2+oXLmy25+jx3M4oFYts712rb2xiIhImjp06BCPPvooFStW5PHHH6dkyZJs3LiR0NBQKleuzA8//GB3iD4hPc7nvtm998KgQWb72Wf9+eijKkRFOWjSBNq3tzc2ERG72b6QWt++fenbt2+it61IpPBjhw4d6NChQ7LPf/DgwQT7atSo4RpuLpgh5j/+aJLu556zOxoREUkjXbt2JTw8nHfeeYcff/yRp59+mkWLFjFixAg6derE008/zZQpU5gzZ47doXq19FSj+1ZGjoTvv4c//3Rw4kQuAgMtPvzQocXTRMTn2Z50iwdwzutWT7eIiE/ZsGEDW7dupXjx4jRr1oyiRYu6bitbtiy//PKLawqXpB5v6OkGCAkxw8vr1LGIjXUwcGAcpUv72x2WiIjtbB1eLh7CObx8924zt1tERHxC9erVGTp0KEuXLuXFF1+kYsWKCY7p3bu3DZH5juho2LfPbKfXOd03qlkTJk6MpXnzAwwZ4oML1IqIJEJJt0DOnFC8uNlev97eWEREJM1MmzaNqKgoXnjhBY4cOcKnn35qd0g+58ABk3hnyGBWL/cGXbpY9OmzjdBQuyMREfEMGl4uRq1a5qv2tWuhaVO7oxERkTRQuHBh5s2bZ3cYPs05tLxUKfBTV4iIiFfSv3cxNK9bRMSnXLp0KVWPl+RJ7+XCRETk9pR0i3Fj0m1Z9sYiIiKprkSJErz11lvxSnPezLIsIiIiaNGiBR988EEaRuc7vGURNRERSZqGl4tRpQoEBsLp03DwINywgq2IiHifFStW8PLLLzN8+HAqV65MjRo1yJcvHyEhIZw9e5adO3eyevVqAgICGDJkCE8//bTdIXslbykXJiIiSVPSLUZIiEm81683vd1KukVEvFrp0qWZP38+hw4dYu7cufz666+sWrWKK1eukDNnTqpWrcpnn31GixYt8PdX2afUop5uERHvp6RbrqtV63rS3amT3dGIiEgaKFSoEIMGDWLQoEF2h+Jzzp6FU6fMtpJuERHvpTndcp0WUxMREUkzzl7u/PkhUyZ7YxERkdSjpFuucybdmzbBtWv2xiIiIuLlNJ9bRMQ3KOmW60qWhOzZISoKtm2zOxoRERGvpvncIiK+QUm3XOdwmHndAOvW2RuLiIiIl1ONbhER36CkW+LTvG4REZE0oeHlIiK+QUm3xOfs6VbSLSLiM4oUKcLrr7/OoUOH7A7FZ8TEwN69ZltJt4iId1PSLfE5k+5du0wtExER8XoDBgzg66+/plixYjRp0oRZs2YRFRVld1he7eBBiI6GkBAoVMjuaEREJDUp6Zb4cuWCYsXM9vr19sYiIiJpYsCAAWzZsoV169ZRtmxZ+vXrR968eenbty+bNm2yOzyv5JzPXaoU+OnTmIiIV9O/eUnIOa9bi6mJiPiUatWq8cEHH3D06FGGDRvG559/Ts2aNalSpQqTJ0/Gsiy7Q/Qams8tIuI7lHRLQlpMTUTEJ0VHRzNnzhwefvhhBg0aRI0aNfj8889p164dL7/8Mo8//rjdIXoNlQsTEfEdAXdzp3/++QeHw0GBAgUAWLduHTNmzKBcuXL07t3brQGKDW5cTM2yTCkxERHxWps2bWLKlCnMnDkTPz8/unbtynvvvUeZG2pZtWnThpo1a9oYpXdRuTAREd9xVz3djz32GD///DMAx48fp0mTJqxbt45XXnmF119/3a0Big2qVoXAQDh1yqz0IiIiXq1mzZrs2bOHjz/+mCNHjjBmzJh4CTdA0aJF6dSpk00Reh/1dIuI+I67Srp37NhBrf/vDZ0zZw4VKlRg1apVTJ8+nS+++MKd8YkdQkKgcmWzrSHmIiJeb//+/SxZsoQOHToQGBiY6DEZM2ZkypQpaRyZdzp3Dk6cMNulStkaioiIpIG7Srqjo6MJDg4G4KeffuLhhx8GoEyZMhw7dsx90Yl9NK9bRMRnnDx5krWJ/L9fu3YtGzZssCEi7+bs5c6bF7JksTcWERFJfXeVdJcvX55PPvmEX3/9lYiICJo3bw7A0aNHCQsLc2uAYhOtYC4i4jOee+45/vnnnwT7jxw5wnPPPWdDRN5N87lFRHzLXSXdb7/9Np9++imNGjWic+fOVP7/ociLFi1yDTuXdM75e9y0CaKj7Y1FRERS1c6dO6lWrVqC/VWrVmXnzp02ROTdVC5MRMS33NXq5Y0aNeL06dNERkaSPXt21/7evXsTGhrqtuDERiVLQrZsZuLZtm1QvbrdEYmISCoJDg7mxIkTFCtWLN7+Y8eOERBwVx8V5Ba0iJqIiG+5q57uK1euEBUV5Uq4//77b8aNG8euXbvInTu3WwMUm/j5xS8dJiIiXqtp06YMGTKE8+fPu/adO3eOl19+mSZNmtgYmXdS0i0i4lvuKul+5JFHmDZtGmDelGvXrs3YsWNp3bo1H3/8sVsDFBtpMTUREZ8wZswY/vnnHwoXLsx9993HfffdR9GiRTl+/Dhjx461OzyvEhsLe/aYbc3pFhHxDXeVdG/atIn69esDMG/ePPLkycPff//NtGnT+OCDD9waoNhIi6mJiPiE/Pnzs23bNkaPHk25cuWoXr0677//Ptu3b6dgwYJ2h+dVDh6Ea9cgOBgKFbI7GhERSQt3NVHr8uXLZM6cGYClS5fStm1b/Pz8uOeee/j777/dGqDYyDm8/K+/zNzubNnsjEZERFJRxowZ6d27t91heD3n0PKSJcHf395YREQkbdxV0l2iRAkWLlxImzZt+PHHH3nhhRcAU+cziwpOeo9cuaBoUThwANavB83rExHxajt37uTQoUNcu3Yt3v6HH37Ypoi8j8qFiYj4nrtKuocOHcpjjz3GCy+8wP3330+dOnUA0+tdtWpVtwYoNqtd2yTda9cq6RYR8VL79++nTZs2bN++HYfDgWVZADgcDgBiY2PtDM+raBE1ERHfc1dzutu3b8+hQ4fYsGEDP/74o2v/Aw88wHvvvee24MQDaDE1ERGv179/f4oWLcrJkycJDQ3ljz/+4JdffqFGjRqsWLHC7vC8imp0i4j4nrsuvhkeHk54eDiHDx8GoECBAtRyzgEW73Fj0m1Z8P+9HiIi4j1Wr17N8uXLyZkzJ35+fvj5+VGvXj1GjRrF888/z+bNm+0O0Wuop1tExPfcVU93XFwcr7/+OlmzZqVw4cIULlyYbNmyMXLkSOLi4twdo9ipalUICIBTp0CL5ImIeKXY2FjXAqk5c+bk6NGjABQuXJhdzixRUuz8eTh+3Gwr6RYR8R131dP9yiuvMGnSJN566y3uvfdeAH777TeGDx/O1atX+d///ufWIMVGISFQuTJs3Gh6u4sUsTsiERFxswoVKrB161aKFi1K7dq1GT16NEFBQUycOJFixYrZHZ7XcH5/ER4OWbPaG4uIiKSdu0q6p06dyueffx5vNdNKlSqRP39+nn32WSXd3qZ27etJd8eOdkcjIiJu9uqrr3Lp0iUAXn/9dR588EHq169PWFgYs2fPtjk676Gh5SIivumuku4zZ85QJpFaF2XKlOHMmTMpDko8TO3a8NFHWkxNRMRLNWvWzLVdokQJ/vrrL86cOUP27NldK5hLyinpFhHxTXc1p7ty5cqMHz8+wf7x48dTqVKlFAclHsa5mNqmTRAdbW8sIiLiVtHR0QQEBLBjx454+3PkyKGE281Uo1tExDfdVU/36NGjadWqFT/99JOrRvfq1av5559/WLx4sVsDFA9QsiRkywbnzsH27VCtmt0RiYiImwQGBlKoUCHV4k4DKhcmIuKb7qqnu2HDhuzevZs2bdpw7tw5zp07R9u2bfnjjz/48ssv3R2j2M3PD2rWNNsaYi4i4nVeeeUVXn75ZU0RS0WxsbBnj9lW0i0i4lvuuk53vnz5EiyYtnXrViZNmsTEiRNTHJh4mNq1ISLCJN3PPGN3NCIi4kbjx49n79695MuXj8KFC5MxY8Z4t2/atMmmyLzHoUMQFQVBQSoEIiLia+466RYf45zXrZ5uERGv07p1a7tD8HrO+dwlS4K/v72xiIhI2lLSLcnjTLr/+svM7c6Wzc5oRETEjYYNG2Z3CF5P87lFRHzXXc3pFh+UKxcULWq2f/vN3lhERETSGZULExHxXXfU0922bdtb3n7u3LmUxCKe7qGH4IMP4Msv4cEH7Y5GRETcxM/P75blwbSyecqpXJiIiO+6o6Q7a9ast729a9euKQpIPFj37ibpXrgQzp6F7NntjkhERNxgwYIF8a5HR0ezefNmpk6dyogRI2yKyruop1tExHfdUdI9ZcqU1IpD0oMqVaBSJdi2DWbN0irmIiJe4pFHHkmwr3379pQvX57Zs2fTs2dPG6LyHpGRcPSo2VbSLSLiezSnW5LP4YAePcy2voAREfF699xzD8uWLbM7jHRv927zM3durUMqIuKLlHTLnXnsMQgIgPXr4Y8/7I5GRERSyZUrV/jggw/Inz+/3aGke5rPLSLi21QyTO5M7tzQqhV88w1MnQqjR9sdkYiIpFD27NnjLaRmWRYXLlwgNDSUr776ysbIvIPKhYmI+DYl3XLnevQwSfeXX8Kbb5qebxERSbfee++9eEm3n58fuXLlonbt2mTXopkppkXURER8m7IluXMtW5q63cePw48/mp5vERFJt7p37253CF5NSbeIiG/TnG65c4GB8PjjZvuLL2wNRUREUm7KlCnMnTs3wf65c+cydepUGyLyHnFx1xdS05xuERHfpKRb7o6zV2TRIvj3X1tDERGRlBk1ahQ5c+ZMsD937ty8+eabNkTkPQ4dgqtXzffVRYrYHY2IiNhBSbfcncqVoWpVuHYNZs60OxoREUmBQ4cOUbRo0QT7CxcuzKFDh2yIyHs4h5aXKKElUEREfJWSbrl7zt5uDTEXEUnXcufOzbZt2xLs37p1K2FhYXd0ruHDh+NwOOJdytwwrvrq1as899xzhIWFkSlTJtq1a8eJEydS/Bw8leZzi4iIkm65e489ZsbLbdwI27fbHY2IiNylzp078/zzz/Pzzz8TGxtLbGwsy5cvp3///nTq1OmOz1e+fHmOHTvmuvz222+u21544QW+/fZb5s6dy8qVKzl69Cht27Z159PxKKrRLSIiGugkdy9nTnjwQViwwNTsHjPG7ohEROQujBw5koMHD/LAAw8Q8P9joOPi4ujatetdzekOCAggPDw8wf7z588zadIkZsyYwf333w+YRdzKli3LmjVruOeee1L2RDyQanSLiIiSbkmZHj1M0v3llzBqlOn5FhGRdCUoKIjZs2fzxhtvsGXLFjJkyEDFihUpXLjwXZ1vz5495MuXj5CQEOrUqcOoUaMoVKgQGzduJDo6msaNG7uOLVOmDIUKFWL16tVJJt1RUVFERUW5rkdGRgIQHR1NdHT0XcXo5Lx/Ss+TlF27AgAHxYvHEB1tpcpjeJrUblNfpDZ1P7Wp+/limyb3uSrplpRp3hxy54aTJ2HJEnjoIbsjEhGRu1SyZElKliyZonPUrl2bL774gtKlS3Ps2DFGjBhB/fr12bFjB8ePHycoKIhs2bLFu0+ePHk4fvx4kuccNWoUI0aMSLB/6dKlhIaGpihep4iICLec50ZXrgRw5EgrAP7+eylnzvjOB1FInTb1dWpT91Obup8vtenly5eTdZySbkmZwEB44gl4912zoJqSbhGRdKddu3bUqlWLF198Md7+0aNHs379+kRreCelRYsWru1KlSpRu3ZtChcuzJw5c8iQIcNdxTdkyBAGDhzouh4ZGUnBggVp2rQpWbJkuatzOkVHRxMREUGTJk0IdPNorU2bzM9cuSw6dmzi1nN7stRsU1+lNnU/tan7+WKbOkde3Y6Sbkm57t1N0v3tt3D6tJnrLSIi6cYvv/zC8OHDE+xv0aIFY8eOTdG5s2XLRqlSpdi7dy9NmjTh2rVrnDt3Ll5v94kTJxKdA+4UHBxMcHBwgv2BgYFu+2DnznM57dtnfpYu7fCZD6A3So029XVqU/dTm7qfL7Vpcp+nVi+XlKtYEapXh+homDHD7mhEROQOXbx4kaCgoAT7AwMDk/0t/q3OvW/fPvLmzUv16tUJDAxk2bJlrtt37drFoUOHqFOnTooexxOpXJiIiICSbnEX1ewWEUm3KlasyOzZsxPsnzVrFuXKlbujc/3nP/9h5cqVHDx4kFWrVtGmTRv8/f3p3LkzWbNmpWfPngwcOJCff/6ZjRs30qNHD+rUqeOVK5erXJiIiICGl4u7dO4MgwbB5s2wdStUrmx3RCIikkyvvfYabdu2Zd++fa5SXsuWLWPmzJl3NJ8b4PDhw3Tu3Jl///2XXLlyUa9ePdasWUOuXLkAeO+99/Dz86Ndu3ZERUXRrFkzPvroI7c/J0+gcmEiIgJKusVdwsLg4Ydh3jzT2/3ee3ZHJCIiyfTQQw+xcOFC3nzzTebNm0eGDBmoVKkSP/30Ew0bNryjc82aNeuWt4eEhDBhwgQmTJiQkpA9Xlwc7N5ttpV0i4j4Ng0vF/dxDjH/6iu4ds3WUERE5M60atWK33//nUuXLnH69GmWL19Ow4YN2bFjh92hpUuHD8OVKxAQAEWL2h2NiIjYSUm3uE+zZpAnj1nB/Icf7I5GRETu0oULF5g4cSK1atWisqYL3RXnfO4SJUx1TRER8V1KusV9AgKgSxezrQXVRETSnV9++YWuXbuSN29exowZw/3338+aNWvsDitd0nxuERFx0pxuca/u3WHMGPjuOzh5EnLntjsiERG5hePHj/PFF18wadIkIiMjefTRR4mKimLhwoV3vHK5XKdyYSIi4qSebnGv8uWhZk2IiVHNbhERD/fQQw9RunRptm3bxrhx4zh69Cgffvih3WF5BSXdIiLipKRb3E81u0VE0oUffviBnj17MmLECFq1aoW/v7/dIXkN5/By1egWEREl3eJ+nTtDUJCp1715s93RiIhIEn777TcuXLhA9erVqV27NuPHj+f06dN2h5XuXbpkVi8H9XSLiIiSbkkN2bND69ZmW73dIiIe65577uGzzz7j2LFjPP3008yaNYt8+fIRFxdHREQEFy5csDvEdMlZnzsszFxERMS3KemW1OEcYj59ump2i4h4uIwZM/Lkk0/y22+/sX37dgYNGsRbb71F7ty5efjhh+0OL91xzufW0HIREQEl3ZJamjSBvHnh33/h++/tjkZERJKpdOnSjB49msOHDzNz5ky7w0mXVC5MRERupKRbUkdAAHTtaranTLE3FhERuWP+/v60bt2aRYsW2R1KuqOVy0VE5EZKuiX1dOtmfi5eDCdO2BuLiIhIGlHSLSIiN/KIpHvChAkUKVKEkJAQateuzbp16255/Ny5cylTpgwhISFUrFiRxYsXJ3lsnz59cDgcjBs3zrVvxYoVOByORC/r169319OSsmWhdm2IjTVzu0VERLxcXJzmdIuISHy2J92zZ89m4MCBDBs2jE2bNlG5cmWaNWvGyZMnEz1+1apVdO7cmZ49e7J582Zat25N69at2bFjR4JjFyxYwJo1a8iXL1+8/XXr1uXYsWPxLk899RRFixalRo0aqfI8fVaPHubnlClgWfbGIiIiksqOHIHLl80sq2LF7I5GREQ8ge1J97vvvkuvXr3o0aMH5cqV45NPPiE0NJTJkycnevz7779P8+bNGTx4MGXLlmXkyJFUq1aN8ePHxzvuyJEj9OvXj+nTpxMYGBjvtqCgIMLDw12XsLAwvvnmG3r06IHD4Ui15+qTOnaE4GDYsQM2bbI7GhERkVTl7OUuVgxu+vghIiI+KsDOB7927RobN25kyJAhrn1+fn40btyY1atXJ3qf1atXM3DgwHj7mjVrxsKFC13X4+Li6NKlC4MHD6Z8+fK3jWPRokX8+++/9HD2yiYiKiqKqKgo1/XIyEgAoqOjiY6Ovu1j3Irz/ik9j0fKmBH/Rx7Bb84cYidPJq5SpTR5WK9uUxuoPd1Pbep+vtimvvRc0wvN5xYRkZvZmnSfPn2a2NhY8uTJE29/njx5+MtZb+Mmx48fT/T448ePu66//fbbBAQE8PzzzycrjkmTJtGsWTMKFCiQ5DGjRo1ixIgRCfYvXbqU0NDQZD3O7URERLjlPJ4mV9my1AViv/ySH++7j7g0/OrfW9vULmpP91Obup8vtenly5ftDkFuovncIiJyM1uT7tSwceNG3n//fTZt2pSsoeKHDx/mxx9/ZM6cObc8bsiQIfF62CMjIylYsCBNmzYlS5YsKYo5OjqaiIgImjRpkmAovFdo1gzr888JOnKEFjExWI88kuoP6fVtmsbUnu6nNnU/X2xT56gr8Ryq0S0iIjezNenOmTMn/v7+nLipnNSJEycIDw9P9D7h4eG3PP7XX3/l5MmTFCpUyHV7bGwsgwYNYty4cRw8eDDefadMmUJYWBgPP/zwLWMNDg4mODg4wf7AwEC3fbhz57k8SmCgqdk9ahQBX30FnTql4UN7aZvaRO3pfmpT9/OlNvWV55meaHi5iIjczNaF1IKCgqhevTrLli1z7YuLi2PZsmXUqVMn0fvUqVMn3vFghhI6j+/SpQvbtm1jy5Ytrku+fPkYPHgwP/74Y7z7WZbFlClT6Nq1qz64pDZnze4lS+DYMXtjERERSQWXL8OhQ2Zbw8tFRMTJ9uHlAwcOpFu3btSoUYNatWoxbtw4Ll265FrUrGvXruTPn59Ro0YB0L9/fxo2bMjYsWNp1aoVs2bNYsOGDUycOBGAsLAwwsLC4j1GYGAg4eHhlL7pa+fly5dz4MABnnrqqTR4pj6udGmoUwdWrzY1u//zH7sjEhERcavdu83PHDkgZ057YxEREc9he8mwjh07MmbMGIYOHUqVKlXYsmULS5YscS2WdujQIY7d0DNat25dZsyYwcSJE6lcuTLz5s1j4cKFVKhQ4Y4fe9KkSdStW5cy+jo6bahmt4iIeDENLRcRkcTY3tMN0LdvX/r27ZvobStWrEiwr0OHDnTo0CHZ5795HrfTjBkzkn0OcYNHH4Xnn4edO2HDBqhZ0+6IRERE3EZJt4iIJMb2nm7xIVmzQtu2ZvuLL2wNRURExN1ULkxERBKjpFvSlnOI+YwZcPWqvbGIiIi4kcqFiYhIYpR0S9q67z4oWBDOnYNFi+yORkRExC0s6/pCakq6RUTkRkq6JW35+5ua3aAh5iIi4jWOHoWLF83bXPHidkcjIiKeREm3pD1nze4ffzSfUkRERNI553zuYsUgKMjeWERExLMo6Za0V7Ik1KsHcXHw5Zd2RyMiIpJims8tIiJJUdIt9uje3fz84gvV7BYRkXRP5cJERCQpSrrFHh06QIYMpmtg3Tq7oxEREUkRJd0iIpIUJd1ijyxZoF07sz1lir2xiIiIpJBzeLlqdIuIyM2UdIt9nDW7Z82CK1fsjUVEROQuXbkChw6ZbfV0i4jIzZR0i30aNYJCheD8efjmG7ujERERuSt79pjlSbJlg1y57I5GREQ8jZJusY+f3/XyYarZLSIi6ZRzPneZMuBw2BuLiIh4HiXdYi9n0r10KRw+bG8sIiIid0HlwkRE5FaUdIu9iheHBg3MuDzV7BYRkXRIK5eLiMitKOkW+6lmt4iIpGNKukVE5FaUdIv92reH0FDYvRtWr7Y7GhERkWSzrPhzukVERG6mpFvslzkzdOhgtrWgmoiIpCPHjsGFC2Zt0OLF7Y5GREQ8kZJu8QzOIeazZ8Ply7aGIiIiklzOXu6iRSE42N5YRETEMynpFs/QoAEUKQKRkbBwod3RiIiIJIvmc4uIyO0o6RbP4Od3vbd7yhRbQxEREUkuZ7kwzecWEZGkKOkWz9G1q/m5bBkcOmRvLCIiIsmgnm4REbkdJd3iOYoWhUaNVLNbRETSDSXdIiJyO0q6xbOoZreIiKQTV6/CwYNmW0m3iIgkRUm3eJb27SFTJti7F37/3e5oREREkrRnj/l+OGtWyJPH7mhERMRTKekWz5Ixo2p2i4hIunDj0HKHw95YRETEcynpFs/jHGI+Zw5cumRrKCIiIknRfG4REUkOJd3ieerXh2LF4MIF+Ppru6MRERFJlDPpVrkwERG5FSXd4nkcjvgLqomIiHggZ41u9XSLiMitKOkWz+Ss2b18Ofz9t72xiIiI3MSyNLxcRESSR0m3eKbCheH++832tGn2xiIiInKTEycgMtIMzipRwu5oRETEkynpFs/Vo4f5+cUXEBdnaygiIiI3cg4tL1oUQkLsjUVERDybkm7xXG3aQObMsH8//Pab3dGIiIi4aGi5iIgkl5Ju8VwZM8Kjj5ptLagmIiIeREm3iIgkl5Ju8Ww31uy+eNHWUERERJyUdIuISHIp6RbPdu+9ZoWaS5dg/ny7oxEREQGuz+lWjW4REbkdJd3i2VSzW0REPExUFBw8aLbV0y0iIrejpFs8X9euJvlesQIOHLA7GhER8XF795qiGpkzQ3i43dGIiIinU9Itnq9gQWjc2GxPnWpvLCIi4vNunM/tcNgbi4iIeD4l3ZI+OIeYT52qmt0iImIrzecWEZE7oaRb0ofWrSFLFjOJ7pdf7I5GRER8mFYuFxGRO6GkW9KH0FDo2NFsT5libywiIuLTlHSLiMidUNIt6UePHubnvHlw4YK9sYiIiE+yrOtJt4aXi4hIcijplvTjnnugVCm4fNkk3iIiImns5Ek4d84soFaihN3RiIhIeqCkW9IP1ewWERGbOXu5CxeGDBnsjUVERNIHJd2SvnTpAn5+ZjG1ffvsjkZERHyM5nOLiMidUtIt6UuBAtCkidlWzW4REUljKhcmIiJ3Skm3pD+q2S0iIjZRT7eIiNwpJd2S/jzyCGTNCocOwc8/2x2NiIj4ECXdIiJyp5R0S/qTIQN07my2taCaiIhHe+utt3A4HAwYMMC1r1GjRjgcjniXPn362BdkMkVFwYEDZltJt4iIJJeSbkmfnEPM58+HyEhbQxERkcStX7+eTz/9lEqVKiW4rVevXhw7dsx1GT16tA0R3pl9+yA2FjJlgnz57I5GRETSCyXdkj7VqmVWsblyBebOtTsaERG5ycWLF3n88cf57LPPyJ49e4LbQ0NDCQ8Pd12yZMliQ5R35sah5Q6HvbGIiEj6oaRb0qcba3ZPmWJrKCIiktBzzz1Hq1ataNy4caK3T58+nZw5c1KhQgWGDBnC5cuX0zjCO6f53CIicjcC7A5A5K516QIvvwy//w579kDJknZHJCIiwKxZs9i0aRPr169P9PbHHnuMwoULky9fPrZt28aLL77Irl27+PrrrxM9PioqiqioKNf1yP+fVhQdHU10dHSKYnXePznn+fNPf8CPkiVjiY5W9Yyk3EmbSvKoTd1Pbep+vtimyX2uSrol/cqXD5o1gx9+MOXD3njD7ohERHzeP//8Q//+/YmIiCAkJCTRY3r37u3arlixInnz5uWBBx5g3759FC9ePMHxo0aNYsSIEQn2L126lNDQULfEHRERcdtj1q6tD+Tg0qVNLF581C2P682S06ZyZ9Sm7qc2dT9fatPkjtJS0i3pW/fu15PuESPA39/uiEREfNrGjRs5efIk1apVc+2LjY3ll19+Yfz48URFReF/0//q2rVrA7B3795Ek+4hQ4YwcOBA1/XIyEgKFixI06ZNUzwXPDo6moiICJo0aUJgYGCSx1kW9OhhPjZ17FiFypWrpOhxvVly21SST23qfmpT9/PFNo1M5oLOSrolfXv4YciWDQ4fhuXLoUkTuyMSEfFpDzzwANu3b4+3r0ePHpQpU4YXX3wxQcINsGXLFgDy5s2b6DmDg4MJDg5OsD8wMNBtH+xud65Tp+DsWbNdrlwgPvJ5MkXc+fsRQ23qfmpT9/OlNk3u81TSLelbSAg89hh89JGp2a2kW0TEVpkzZ6ZChQrx9mXMmJGwsDAqVKjAvn37mDFjBi1btiQsLIxt27bxwgsv0KBBg0RLi3mKv/4yPwsXBjeNaBcRER+h1csl/XOuYv7113D+vK2hiIjIrQUFBfHTTz/RtGlTypQpw6BBg2jXrh3ffvut3aHdklYuFxGRu6Webkn/atSAcuVg506YPRtuWKBHRETst2LFCtd2wYIFWblypX3B3CUl3SIicrfU0y3pn8MBPXqY7S++sDUUERHxTkq6RUTkbinpFu/w+ONm5fLVq69/MhIREXET55zuMmXsjUNERNIfJd3iHfLmhebNzfbUqfbGIiIiXuXaNdi/32yrp1tERO6Ukm7xHs4F1aZNg9hYW0MRERHvsX+/eVvJmBHy57c7GhERSW+UdIv3eOghyJEDjhzBsWyZ3dGIiIiXcM5aKlXKLCMiIiJyJ5R0i/cIDjY1uwG/adNsDkZERLyF5nOLiEhKKOkW7/L/Q8wd33xD4MWL9sYiIiJeQSuXi4hISijpFu9SrRpUrIgjKooiP/5odzQiIuIFlHSLiEhKKOkW7+JwQM+eAJT78kv827eHQ4dsDkpERNIzDS8XEZGUUNIt3ue554gdOJA4f3/8Fi2CsmXh7bdNzRcREZE7cPo0nDljtkuWtDcWERFJn5R0i/cJCCDurbdY8e67xNWrB5cvw0svQdWqsHKl3dGJiEg64hxaXrCgKRkmIiJyp5R0i9e6ULgwscuWwRdfQK5csHMnNGoEXbvCiRN2hyciIumA5nOLiEhKKekW7+ZwQLduZkJenz7m+pdfmol5H38MsbF2RygiIh5M87lFRCSllHSLb8iRwyTZa9aYFc7PnYNnn4V77oENG+yOTkREPJR6ukVEJKWUdItvqVUL1q2DDz+ELFlMwl2rFjz3nEnERUREbqCkW0REUkpJt/gef3/o29d8knr8cbAs+Ogj84nqq6/MdRER8XnR0bBvn9lW0i0iIndLSbf4rvBwk2QvX24m6508CV26wH33mUXXRETEp+3fDzExEBoKBQrYHY2IiKRXSrpF7rsPtm6FN9+EDBlMWbHKlU2ZsUuX7I5ORERs4hxaXqoU+OkTk4iI3CW9hYgABAXBkCGmh/vhh03XxttvQ7ly8M03GnIuIuKDNJ9bRETcwfake8KECRQpUoSQkBBq167NunXrbnn83LlzKVOmDCEhIVSsWJHFixcneWyfPn1wOByMGzcuwW3ff/89tWvXJkOGDGTPnp3WrVun8JmIVyhSxCTZ33wDhQvDoUPQurVJxA8csDs6ERFJQ0q6RUTEHWxNumfPns3AgQMZNmwYmzZtonLlyjRr1oyTJ08mevyqVavo3LkzPXv2ZPPmzbRu3ZrWrVuzY8eOBMcuWLCANWvWkC9fvgS3zZ8/ny5dutCjRw+2bt3K77//zmOPPeb25yfp2MMPwx9/mN7vwED47jsoX94MQY+Ksjs6ERFJA6rRLSIi7mBr0v3uu+/Sq1cvevToQbly5fjkk08IDQ1l8uTJiR7//vvv07x5cwYPHkzZsmUZOXIk1apVY/z48fGOO3LkCP369WP69OkEBgbGuy0mJob+/fvzzjvv0KdPH0qVKkW5cuV49NFHU+15SjqVMaNJsrduNfO+r1yBV14x872XLbM7OhERSWXq6RYREXewLem+du0aGzdupHHjxteD8fOjcePGrF69OtH7rF69Ot7xAM2aNYt3fFxcHF26dGHw4MGUL18+wTk2bdrEkSNH8PPzo2rVquTNm5cWLVok2lsuAkDZsibJnj4d8uQxn8IaN4bHHoNjx+yOTkREUsG//8Lp02a7VCl7YxERkfQtwK4HPn36NLGxseTJkyfe/jx58vCXczzXTY4fP57o8cePH3ddf/vttwkICOD5559P9Bz79+8HYPjw4bz77rsUKVKEsWPH0qhRI3bv3k2OHDkSvV9UVBRRNwwrjoyMBCA6Opro6OjbPNtbc94/peeR61KlTTt0gCZN8Bs+HL9PPsExcybW998TN2IEcU8/DQG2/TmlOr1G3U9t6n6+2Ka+9FzTmrOXu0AByJTJ3lhERCR986osYePGjbz//vts2rQJh8OR6DFxcXEAvPLKK7Rr1w6AKVOmUKBAAebOncvTTz+d6P1GjRrFiBEjEuxfunQpoaGhbok/IiLCLeeR61KlTZs2JWuxYlT+9FOy79mD/wsvcOHDD9nWpw9nvbw7RK9R91Obup8vtenly5ftDsFraWi5iIi4i21Jd86cOfH39+fEiRPx9p84cYLw8PBE7xMeHn7L43/99VdOnjxJoUKFXLfHxsYyaNAgxo0bx8GDB8mbNy8A5cqVcx0THBxMsWLFOHToUJLxDhkyhIEDB7quR0ZGUrBgQZo2bUqWLFmS+awTFx0dTUREBE2aNEkwB13uTpq06XPPETtpEn6vvkq2/fup/+KLxD31FHEjR0ISIybSK71G3U9t6n6+2KbOUVfifkq6RUTEXWxLuoOCgqhevTrLli1zleuKi4tj2bJl9O3bN9H71KlTh2XLljFgwADXvoiICOrUqQNAly5dEp3z7VypHKB69eoEBweza9cu6tWrB5gPagcPHqRw4cJJxhscHExwcHCC/YGBgW77cOfOc4mRqm0aGAjPPWeGnf/3vzimTsX/s8/wX7AA3nkHunWDJEZcpFd6jbqf2tT9fKlNfeV52kFJt4iIuIutw8sHDhxIt27dqFGjBrVq1WLcuHFcunTJlSB37dqV/PnzM2rUKAD69+9Pw4YNGTt2LK1atWLWrFls2LCBiRMnAhAWFkZYWFi8xwgMDCQ8PJzS//+umSVLFvr06cOwYcMoWLAghQsX5p133gGgQ4cOafXUxZvkzg1ffAFPPgnPPmtKjfXoAZMmwUcfQcWKdkcoIiJ3SOXCRETEXWxNujt27MipU6cYOnQox48fp0qVKixZssS1WNqhQ4fw87u+wHrdunWZMWMGr776Ki+//DIlS5Zk4cKFVKhQ4Y4e95133iEgIIAuXbpw5coVateuzfLly8mePbtbn5/4mAYNYPNmGDcOhg+H336DqlXhhRdg2DCtxCMikk7ExMC+fWZbPd0iIpJSti+k1rdv3ySHk69YsSLBvg4dOtxRj/TBgwcT7AsMDGTMmDGMGTMm2ecRSZbAQBg8GDp1ggED4OuvYcwYmDXLJONt23rdkHMREW9z4ABER0OGDFCwoN3RiIhIemdbnW4Rr1awIMyfD99/D8WKweHD0L49tGx5vftEREQ8knM+d8mS4KdPSiIikkJ6KxFJTS1bwo4d8NprEBQES5ZA+fLw+utw9ard0YmISCI0n1tERNxJSbdIasuQwSTZ27dD48YQFWXmeFesCEuX2h2diIjcRCuXi4iIO9k+p1vEZ5QqZZLsuXPNfO+9e6FZM7j3XihRAgoUgPz54//MmVNjG0VE0piSbhERcScl3SJpyeGARx+F5s1Nb/cHH8Dvv5tLYgIDTQJ+czJ+4898+cxxIiLiFs7h5Uq6RUTEHZR0i9ghSxZ47z145hlYu9YstHbkSPyfJ06Y5XMPHjSXpDgcplZ4Ukm586e3liyzLLh8GS5eNJfLl6FoUe99viKSqs6ehVOnzLaSbhERcQcl3SJ2KlXKXBITHQ3HjiVMxm/8eeQIXLtmEvQTJ2DjxqQfK2vWWyflBQpAWFjqljSLjr6eHF+6dH07OZekjr90ySTeNwoNNeXZunWD++4Df//Ue04i4lWcQ8vz5YPMme2NRUREvIOSbhFPFRgIhQqZS1Li4uDff00SnlRifvgwXLgA58+by86dSZ8vODjR4eyOPHkI278fR2CgWXX9ThNj5+XaNfe3040yZYKAADh3Dr76ylwKFIAuXUwCrm4rEbkNzecWERF3U9Itkp75+UGuXOZStWrSx0VGXu8ZTyo5P3nSrKy+f7+53CAAqOfOuIOCIGNGkyTfyeVW98mQwbSHZZkh+1OnwqxZ5vmNGmUutWub5LtjR8iRw53PSES8hMqFiYiIuynpFvEFWbKYS9mySR8TFQVHjyaakMf98w+XDx0iY86cODJnTn4inFTiHBSUes/V4YB77jGX996Db781CfiSJSYZX7vWrB7/8MMmAW/WTAvRiYiLerpFRMTdlHSLiBEcbBYgK1o0wU2x0dEsW7yYli1bEpieEtSQEOjQwVxOnIDp000Cvm0bzJtnLrlzw+OPmwS8cmW7IxYRmynpFhERd1MBYBHxDXnywMCBsHUrbNkCL7xgEu6TJ02PeJUq5vLeeyZBFxGfExMDe/eabSXdIiLiLkq6RcT3VK4M775rhs9/+y20b2+GvG/dahLz/PnhoYdMT3hUlN3RikgaOXjQrPcYEnLrNSxFRETuhJJuEfFdgYHw4IMwd64pz/bRR2axtdhY+O47Myw9b15TT33NmoSlyUTEqziHlpcsqUqDIiLiPkq6RUTArGbuTK7//BOGDDHlxs6ehU8+gTp1zEJ0b74J//xjd7Qikgo0n1tERFKDkm4RkZuVKWOS64MHISICnnjClCTbtQteeQUKF4bGjeHLL01tchHxCs5yYUq6RUTEnZR0i4gkxd//enJ94gRMngwNG5ph5suWQdeuEB4OPXrAihUQF2d3xCKSAs6ebtXoFhERd1LSLSKSHJkzX0+u9++HESOgWDG4eBG++ALuuw+KF4ehQ68vfywi6YqGl4uISGpQ0i0icqeKFr2eXP/6Kzz1FGTJYoajjxxpVmGqVw8++wzOn7c7WhFJhnPnrlcLVNItIiLupKRbRORuORzXk+vjx2HGDGjWDPz84PffoXdvM/y8c2dYssSsii4iHmn3bgdgChZkyWJzMCIi4lUC7A5ARMQrZMhgkuvOneHoUfjqK5g6FXbuhFmzzCVvXrMoW7duUKpU6sViWRAdbWqMX7uWvMutjo2Lg7p1zcVP39WKd9LQchERSS1KukVE3C1fPvjvf2HwYNi40STfM2eaWuDvvAPvvIN/tWqULlkSvw0bICbGPcmx8xIdnTrPKzwc2rSBdu3MgnIBegsR7+Hs6VbSLSIi7qZPTCIiqcXhgBo1zGXsWPj+e5OAf/89fps2UWbTprSLIzgYgoKSd7n52CtXTOm048fh44/NJSwMWrc2CfgDD5jjRNIxJd0iIpJalHSLiKSFoCDTS9ymDZw6Rez06fyzeDEFixXDPyQk8WT3ThLjW138/U3inRLXrpkyafPmwTffwL//wqRJ5pI1Kzz8sEnAmzY1Q+1F0pldu8zfiMqFiYiIuynpFhFJa7lyEffcc2wtWpT8LVviHxhod0S3FxQELVqYy6efwsqVMH8+LFhgesC//NJcMmaEVq1MAt6yJWTKZHfkIrcVGwv79plt9XSLiIi7aUUcERG5MwEBZkj5Rx/B4cOmbFr//lCwIFy6BHPmQMeOkCuX6dn/6iuVThOPdupUKFFRDoKDoXBhu6MRERFvo6RbRETunr+/KZs2bhz8/TesXWsWkSteHK5ehYULoUsXk4C3bAmTJ5uh6SIe5PBhMyKjRAnzkhYREXEnJd0iIuIeDgfUqgVvvw179sDmzfDqq1C2rFlR/YcfoGdPyJMHGjc2C7IdP2531CIcPWqSbs3nFhGR1KCkW0RE3M/hgCpVYORIU6t8506zXbmymUC7bBk8+6wpr9agAbz/Pvzzj91Ri486csQk3ZrPLSIiqUFJt4iIpL6yZU2v95YtsHev6Q2vVQssy8wJHzAAChWC2rVh9Ojrq1qJpAEl3SIikpq0ermIiKSt4sXNvO///tf0bn/9tSlF9vvvsG6dubz4oukpb9fOXMqWtTvqOxMTAydOmOHzx44l/vO118xq8GK7I0cyA0q6RUQkdSjpFhER+xQsaFY+79/fJKILFphSZCtWmF7xLVtMclq2rEm+27eHSpVSXnf8blgWXLyYdBJ948/Tp83xt7Jnj5JuDxAZCWfPhgBKukVEJHUo6RYREc8QHg7PPGMup0/DokWmB/ynn+DPP+GNN8ylePHrPeA1a6Y8AY+N/b/27j+myrr/4/jrgHA4ECg/bn44IbUfovgjFTWku5a/ydptUc6NGlmbs5BAZosyp6781Za6iZI48580y5qFdZsR3WmaTMIwXIp/tG+5HKCr5Nct8uVc9x9Mus9tP456XV5cnOdjuwbnc+Difb3H9uLNda5zSU1Nfz5IX/m8vd3//QYHd79pXGKilJR09cdx426sbpjizJnu35+EBEMDBtjwzxwAQJ/H0A0A6H3i4qSnnurefv1V+uij7jPgn3zSfb33a691bykp0iOP/DaA/7e2Nv/OSp8/L3m9/tcWGfnHg/R/f4yN5f5TDlBf3/3xzjsNSQzdAADzMXQDAHq3AQOkxx/v3lpbpX/+s3sA//hj6ccfu+8RvnGj+iUmKjMmRv2WLOkeqFtb/f8ZQUFSfLx/w3REhFVHChvU13cP2sOG/cXlAAAAXCeGbgCAc9xyizR3bvf2739Ln37aPYCXl8vV0KC4/73vd3h497D8V4P03/7GWekAdeXl5XfeaXMhAIA+i6EbAOBMHo/0j390b5cv6/8/+0zf/OtfGpuVpX7Jyd3DdGSk3VWil1uyxKv4+DpNnz5CEv94AQCYj6EbAOB8oaEypk/Xuc5O3fX3v0shIXZXBIdITzfU1PR/SksbYXcpAIA+KsjuAgAAAAAA6KsYugEAAAAAsAhDNwAAAAAAFmHoBgAAllm7dq1cLpcKCwt71i5duqS8vDzFxsbqlltuUXZ2thobG+0rEgAACzF0AwAAS1RXV2vr1q0aPXq0z/rixYu1b98+7dmzRwcPHtS5c+f0yCOP2FQlAADWYugGAACma21tVU5OjrZt26bo6Oie9YsXL2r79u1av369pkyZovHjx2vHjh366quvVFVVZWPFAABYg6EbAACYLi8vT7Nnz9a0adN81mtqatTZ2emznpqaqpSUFB09evRmlwkAgOW4TzcAADDV7t27dfz4cVVXV1/1XENDg0JDQzVgwACf9YSEBDU0NPzu/jo6OtTR0dHzuLm5WZLU2dmpzs7OG6r1yvff6H7wG3pqPnpqPnpqvkDsqb/HytANAABMc/bsWRUUFKiiokJhYWGm7HPNmjVauXLlVeuffvqpwsPDTfkZFRUVpuwHv6Gn5qOn5qOn5guknra3t/v1dQzdAADANDU1NWpqatK4ceN61rq6unTo0CGVlJTowIEDunz5sn799Vefs92NjY1KTEz83X2++OKLKioq6nnc3Nys5ORkzZgxQ1FRUTdUb2dnpyoqKjR9+nSFhITc0L7QjZ6aj56aj56aLxB7euWVV3+FoRsAAJhm6tSpqqur81mbP3++UlNT9cILLyg5OVkhISGqrKxUdna2JKm+vl4//vijMjIyfnefbrdbbrf7qvWQkBDT/rAzc1/oRk/NR0/NR0/NF0g99fc4GboBAIBpIiMjNXLkSJ+1iIgIxcbG9qw//fTTKioqUkxMjKKiopSfn6+MjAzdfffddpQMAIClGLoBAMBNtWHDBgUFBSk7O1sdHR2aOXOmtmzZYndZAABYgqEbAABY6osvvvB5HBYWps2bN2vz5s32FAQAwE3EfboBAAAAALAIZ7qvk2EYkvx/x7o/09nZqfb2djU3NwfMmw5YjZ6ai36aj56aLxB7eiWDrmRSoCCDezd6aj56aj56ar5A7Km/OczQfZ1aWlokScnJyTZXAgAIdC0tLerfv7/dZdw0ZDAAoDf5qxx2GYH273GTeL1enTt3TpGRkXK5XDe0ryv3Gz179uwN328U3eipuein+eip+QKxp4ZhqKWlRQMHDlRQUOBcMUYG92701Hz01Hz01HyB2FN/c5gz3dcpKChIgwYNMnWfUVFRAfMLerPQU3PRT/PRU/MFWk8D6Qz3FWSwM9BT89FT89FT8wVaT/3J4cD5tzgAAAAAADcZQzcAAAAAABZh6O4F3G63li9fLrfbbXcpfQY9NRf9NB89NR89xfXg98Z89NR89NR89NR89PSP8UZqAAAAAABYhDPdAAAAAABYhKEbAAAAAACLMHQDAAAAAGARhm6bbd68WYMHD1ZYWJgmTZqkY8eO2V2SY61Zs0YTJkxQZGSk4uPjNWfOHNXX19tdVp+ydu1auVwuFRYW2l2Ko/300096/PHHFRsbK4/Ho1GjRunrr7+2uyzH6urq0rJlyzRkyBB5PB7ddttteuWVV8RblsAf5LB5yGFrkcHmIIPNRQb7h6HbRu+8846Kioq0fPlyHT9+XGPGjNHMmTPV1NRkd2mOdPDgQeXl5amqqkoVFRXq7OzUjBkz1NbWZndpfUJ1dbW2bt2q0aNH212Ko/3yyy/KzMxUSEiI9u/fr++++06vv/66oqOj7S7NsdatW6fS0lKVlJTo1KlTWrdunV577TVt2rTJ7tLQy5HD5iKHrUMGm4MMNh8Z7B/evdxGkyZN0oQJE1RSUiJJ8nq9Sk5OVn5+voqLi22uzvnOnz+v+Ph4HTx4UPfee6/d5Thaa2urxo0bpy1btujVV1/VXXfdpY0bN9pdliMVFxfryJEj+vLLL+0upc948MEHlZCQoO3bt/esZWdny+Px6K233rKxMvR25LC1yGFzkMHmIYPNRwb7hzPdNrl8+bJqamo0bdq0nrWgoCBNmzZNR48etbGyvuPixYuSpJiYGJsrcb68vDzNnj3b5/cV16e8vFzp6el67LHHFB8fr7Fjx2rbtm12l+VokydPVmVlpc6cOSNJOnHihA4fPqysrCybK0NvRg5bjxw2BxlsHjLYfGSwf/rZXUCgunDhgrq6upSQkOCznpCQoNOnT9tUVd/h9XpVWFiozMxMjRw50u5yHG337t06fvy4qqur7S6lT/j+++9VWlqqoqIivfTSS6qurtZzzz2n0NBQ5ebm2l2eIxUXF6u5uVmpqakKDg5WV1eXVq1apZycHLtLQy9GDluLHDYHGWwuMth8ZLB/GLrRJ+Xl5enkyZM6fPiw3aU42tmzZ1VQUKCKigqFhYXZXU6f4PV6lZ6ertWrV0uSxo4dq5MnT+qNN94g8K/Tu+++q507d2rXrl1KS0tTbW2tCgsLNXDgQHoK2IQcvnFksPnIYPORwf5h6LZJXFycgoOD1djY6LPe2NioxMREm6rqGxYtWqSPPvpIhw4d0qBBg+wux9FqamrU1NSkcePG9ax1dXXp0KFDKikpUUdHh4KDg22s0HmSkpI0YsQIn7Xhw4fr/ffft6ki53v++edVXFysefPmSZJGjRqlH374QWvWrCHw8YfIYeuQw+Ygg81HBpuPDPYP13TbJDQ0VOPHj1dlZWXPmtfrVWVlpTIyMmyszLkMw9CiRYu0d+9eff755xoyZIjdJTne1KlTVVdXp9ra2p4tPT1dOTk5qq2tJeyvQ2Zm5lW30Dlz5oxuvfVWmypyvvb2dgUF+cZZcHCwvF6vTRXBCchh85HD5iKDzUcGm48M9g9num1UVFSk3Nxcpaena+LEidq4caPa2to0f/58u0tzpLy8PO3atUsffvihIiMj1dDQIEnq37+/PB6PzdU5U2Rk5FXX4kVERCg2NpZr9K7T4sWLNXnyZK1evVpz587VsWPHVFZWprKyMrtLc6yHHnpIq1atUkpKitLS0vTNN99o/fr1euqpp+wuDb0cOWwucthcZLD5yGDzkcF+MmCrTZs2GSkpKUZoaKgxceJEo6qqyu6SHEvS7247duywu7Q+5b777jMKCgrsLsPR9u3bZ4wcOdJwu91GamqqUVZWZndJjtbc3GwUFBQYKSkpRlhYmDF06FBj6dKlRkdHh92lwQHIYfOQw9Yjg28cGWwuMtg/3KcbAAAAAACLcE03AAAAAAAWYegGAAAAAMAiDN0AAAAAAFiEoRsAAAAAAIswdAMAAAAAYBGGbgAAAAAALMLQDQAAAACARRi6AQAAAACwCEM3AMdxuVz64IMP7C4DAICAQwYD146hG8A1efLJJ+Vyua7aZs2aZXdpAAD0aWQw4Ez97C4AgPPMmjVLO3bs8Flzu902VQMAQOAggwHn4Uw3gGvmdruVmJjos0VHR0vqftlZaWmpsrKy5PF4NHToUL333ns+319XV6cpU6bI4/EoNjZWCxYsUGtrq8/XvPnmm0pLS5Pb7VZSUpIWLVrk8/yFCxf08MMPKzw8XHfccYfKy8utPWgAAHoBMhhwHoZuAKZbtmyZsrOzdeLECeXk5GjevHk6deqUJKmtrU0zZ85UdHS0qqurtWfPHn322Wc+gV5aWqq8vDwtWLBAdXV1Ki8v1+233+7zM1auXKm5c+fq22+/1QMPPKCcnBz9/PPPN/U4AQDobchgoBcyAOAa5ObmGsHBwUZERITPtmrVKsMwDEOSsXDhQp/vmTRpkvHMM88YhmEYZWVlRnR0tNHa2trz/Mcff2wEBQUZDQ0NhmEYxsCBA42lS5f+YQ2SjJdffrnncWtrqyHJ2L9/v2nHCQBAb0MGA87ENd0Artn999+v0tJSn7WYmJiezzMyMnyey8jIUG1trSTp1KlTGjNmjCIiInqez8zMlNfrVX19vVwul86dO6epU6f+aQ2jR4/u+TwiIkJRUVFqamq63kMCAMARyGDAeRi6AVyziIiIq15qZhaPx+PX14WEhPg8drlc8nq9VpQEAECvQQYDzsM13QBMV1VVddXj4cOHS5KGDx+uEydOqK2tref5I0eOKCgoSMOGDVNkZKQGDx6sysrKm1ozAAB9ARkM9D6c6QZwzTo6OtTQ0OCz1q9fP8XFxUmS9uzZo/T0dN1zzz3auXOnjh07pu3bt0uScnJytHz5cuXm5mrFihU6f/688vPz9cQTTyghIUGStGLFCi1cuFDx8fHKyspSS0uLjhw5ovz8/Jt7oAAA9DJkMOA8DN0Artknn3yipKQkn7Vhw4bp9OnTkrrf1XT37t169tlnlZSUpLffflsjRoyQJIWHh+vAgQMqKCjQhAkTFB4eruzsbK1fv75nX7m5ubp06ZI2bNigJUuWKC4uTo8++ujNO0AAAHopMhhwHpdhGIbdRQDoO1wul/bu3as5c+bYXQoAAAGFDAZ6J67pBgAAAADAIgzdAAAAAABYhJeXAwAAAABgEc50AwAAAABgEYZuAAAAAAAswtANAAAAAIBFGLoBAAAAALAIQzcAAAAAABZh6AYAAAAAwCIM3QAAAAAAWIShGwAAAAAAizB0AwAAAABgkf8ARGoAHLcgMWgAAAAASUVORK5CYII=","text/plain":["<Figure size 1000x500 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","# Separate lists for loss and accuracy\n","losses = []\n","accuracies = []\n","\n","# Parse the data\n","for entry in Losses:\n","    loss_str, acc_str = entry.split(',')\n","\n","    # Convert loss to float and remove % from accuracy, converting to float\n","    losses.append(float(loss_str))\n","    accuracies.append(float(acc_str.strip('%')))\n","\n","def divide_list(lst, n):\n","    # Use list comprehension to divide the list into n parts\n","    return [lst[i * len(lst) // n : (i + 1) * len(lst) // n] for i in range(n)]\n","\n","# Divide the list into 10 parts\n","divided_lists_losses = divide_list(losses, 10)\n","divided_lists_acc = divide_list(accuracies, 10)\n","\n","means_l = [np.mean(sublist) for sublist in divided_lists_losses]\n","means_cc = [np.mean(sublist) for sublist in divided_lists_acc]\n","\n","# Plotting\n","plt.figure(figsize=(10,5))\n","\n","# Plot losses\n","plt.subplot(1, 2, 1)\n","plt.plot(means_l, label='Loss', color='red')\n","plt.title('Loss over Epochs')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.grid(True)\n","plt.legend()\n","\n","# Plot accuracies\n","plt.subplot(1, 2, 2)\n","plt.plot(means_cc, label='Accuracy', color='blue')\n","plt.title('Accuracy over Epochs')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy (%)')\n","plt.grid(True)\n","plt.legend()\n","\n","# Show the plots\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"ozgDKq4jTyhD"},"source":["### Prédiction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EMFDl5bcTyhE","outputId":"21bce933-2ffa-49ca-99e5-1dc08353a9f9"},"outputs":[{"data":{"text/plain":["array([5], dtype=int64)"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["import trimesh\n","import numpy as np\n","def one_file_preprocessing(file_path, resolution=30):\n","\n","    mesh = trimesh.load(file_path)  # Load the 3D mesh from the file using trimesh.\n","    points, _ = trimesh.sample.sample_surface(mesh, count=10000)  # Sample 10,000 points from the surface of the mesh.\n","\n","    # Avoid division by zero by checking if max == min\n","    diff = points.max(axis=0) - points.min(axis=0)\n","    diff[diff == 0] = 1  # Avoid division by zero\n","\n","    points_normalized = (points - points.min(axis=0)) / diff # Normalize the points to fit within a unit cube [0, 1].\n","    points_voxels = (points_normalized * (resolution - 1)).astype(int)  # Scale the normalized points to the voxel grid resolution and convert them to integers.\n","\n","    grid = np.zeros((resolution, resolution, resolution), dtype=bool)  # Initialize a 3D voxel grid with the given resolution.\n","    grid[points_voxels[:, 0], points_voxels[:, 1], points_voxels[:, 2]] = True  # Mark the grid cells corresponding to the sampled points as occupied (True).\n","\n","    voxel_grid = grid.astype(np.float32)  # Convert the voxel grid to float32 for further processing.\n","\n","    voxel_grid_tensor = tf.convert_to_tensor(voxel_grid, dtype=tf.float32)  # Convert the voxel grid to a TensorFlow tensor.\n","    voxel_grid_tensor = tf.expand_dims(voxel_grid_tensor, axis=-1)  # Add a channel dimension to the tensor (for Conv3D compatibility).\n","\n","    return voxel_grid_tensor  # Return the processed voxel grid tensor.\n","capsule_net.predict(one_file_preprocessing(r\"ModelNet40\\bathtub\\train\\bathtub_0018.off\"))\n"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}